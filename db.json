{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/kd_1.png","path":"images/kd_1.png","modified":1,"renderable":0},{"_id":"source/images/kd_2.png","path":"images/kd_2.png","modified":1,"renderable":0},{"_id":"source/images/kd_3.png","path":"images/kd_3.png","modified":1,"renderable":0},{"_id":"source/images/water3.0.png","path":"images/water3.0.png","modified":1,"renderable":0},{"_id":"source/images/xishi1.png","path":"images/xishi1.png","modified":1,"renderable":0},{"_id":"source/images/xishi3.png","path":"images/xishi3.png","modified":1,"renderable":0},{"_id":"source/images/kd_example.png","path":"images/kd_example.png","modified":1,"renderable":0},{"_id":"source/images/tongshi1.png","path":"images/tongshi1.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex1.png","path":"images/paper/alex1.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex10.png","path":"images/paper/alex10.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex12.png","path":"images/paper/alex12.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex15.jpg","path":"images/paper/alex15.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex13.jpg","path":"images/paper/alex13.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex14.jpg","path":"images/paper/alex14.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex16.jpg","path":"images/paper/alex16.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex19.png","path":"images/paper/alex19.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex18.jpg","path":"images/paper/alex18.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex17.png","path":"images/paper/alex17.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex21.jpg","path":"images/paper/alex21.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex4.png","path":"images/paper/alex4.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex3.png","path":"images/paper/alex3.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex5.jpg","path":"images/paper/alex5.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex7.png","path":"images/paper/alex7.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex9.jpg","path":"images/paper/alex9.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/arc1.png","path":"images/paper/arc1.png","modified":1,"renderable":0},{"_id":"source/images/paper/arc4.png","path":"images/paper/arc4.png","modified":1,"renderable":0},{"_id":"source/images/paper/arc3.png","path":"images/paper/arc3.png","modified":1,"renderable":0},{"_id":"source/images/paper/arc5.png","path":"images/paper/arc5.png","modified":1,"renderable":0},{"_id":"source/images/paper/arc2.png","path":"images/paper/arc2.png","modified":1,"renderable":0},{"_id":"source/images/paper/arc6.png","path":"images/paper/arc6.png","modified":1,"renderable":0},{"_id":"source/images/paper/arc7.png","path":"images/paper/arc7.png","modified":1,"renderable":0},{"_id":"source/images/paper/lgb01.png","path":"images/paper/lgb01.png","modified":1,"renderable":0},{"_id":"source/images/paper/lgb04.png","path":"images/paper/lgb04.png","modified":1,"renderable":0},{"_id":"source/images/paper/lgb06.png","path":"images/paper/lgb06.png","modified":1,"renderable":0},{"_id":"source/images/tongshi2.png","path":"images/tongshi2.png","modified":1,"renderable":0},{"_id":"source/images/paper/lgb05.png","path":"images/paper/lgb05.png","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"source/images/bayes2.png","path":"images/bayes2.png","modified":1,"renderable":0},{"_id":"source/images/p54.jpg","path":"images/p54.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex11.png","path":"images/paper/alex11.png","modified":1,"renderable":0},{"_id":"source/images/paper/alex6.jpg","path":"images/paper/alex6.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/alex8.jpg","path":"images/paper/alex8.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/lgb03.png","path":"images/paper/lgb03.png","modified":1,"renderable":0},{"_id":"source/images/p59.jpg","path":"images/p59.jpg","modified":1,"renderable":0},{"_id":"source/images/paper/lgb02.png","path":"images/paper/lgb02.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","path":"lib/needsharebutton/font-embedded.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","path":"lib/pace/pace-theme-big-counter.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","path":"lib/needsharebutton/needsharebutton.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","path":"lib/needsharebutton/needsharebutton.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","path":"lib/pace/pace-theme-barber-shop.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","path":"lib/pace/pace-theme-bounce.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","path":"lib/pace/pace-theme-center-atom.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","path":"lib/pace/pace-theme-center-circle.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","path":"lib/pace/pace-theme-center-radar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","path":"lib/pace/pace-theme-center-simple.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","path":"lib/pace/pace-theme-fill-left.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","path":"lib/pace/pace-theme-corner-indicator.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","path":"lib/pace/pace-theme-minimal.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","path":"lib/pace/pace-theme-flash.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","path":"lib/pace/pace-theme-loading-bar.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","path":"lib/pace/pace-theme-mac-osx.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/pace/pace.min.js","path":"lib/pace/pace.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"source/images/xishi2.png","path":"images/xishi2.png","modified":1,"renderable":0},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"source/images/paper/alex20.jpg","path":"images/paper/alex20.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","path":"lib/Han/dist/font/han.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"source/images/paper/alex2.png","path":"images/paper/alex2.png","modified":1,"renderable":0},{"_id":"source/images/bayes1.jpg","path":"images/bayes1.jpg","modified":1,"renderable":0}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"334da94ca6f024d60d012cc26ea655681e724ad8","modified":1539440001419},{"_id":"themes/next/.editorconfig","hash":"211d2c92bfdddb3e81ea946f4ca7a539f150f4da","modified":1539440001420},{"_id":"themes/next/.gitattributes","hash":"8454b9313cb1a97b63fb87e2d29daee497ce6249","modified":1539440001420},{"_id":"themes/next/.gitignore","hash":"ee0b13c268cc8695d3883a5da84930af02d4ed08","modified":1539440001423},{"_id":"themes/next/.hound.yml","hash":"289dcf5bfe92dbd680d54d6e0668f41c9c9c0c78","modified":1539440001424},{"_id":"themes/next/.jshintrc","hash":"b7d23f2ce8d99fa073f22f9960605f318acd7710","modified":1539440001425},{"_id":"themes/next/.javascript_ignore","hash":"cd250ad74ca22bd2c054476456a73d9687f05f87","modified":1539440001424},{"_id":"themes/next/.travis.yml","hash":"6674fbdfe0d0c03b8a04527ffb8ab66a94253acd","modified":1539440001426},{"_id":"themes/next/LICENSE","hash":"ec44503d7e617144909e54533754f0147845f0c5","modified":1539440001426},{"_id":"themes/next/.stylintrc","hash":"3b7f9785e9ad0dab764e1c535b40df02f4ff5fd6","modified":1539440001425},{"_id":"themes/next/README.cn.md","hash":"23e92a2599725db2f8dbd524fbef2087c6d11c7b","modified":1539440001427},{"_id":"themes/next/README.md","hash":"50abff86ffe4113051a409c1ed9261195d2aead0","modified":1539440001428},{"_id":"themes/next/gulpfile.coffee","hash":"412defab3d93d404b7c26aaa0279e2e586e97454","modified":1539440001430},{"_id":"themes/next/bower.json","hash":"486ebd72068848c97def75f36b71cbec9bb359c5","modified":1539440001429},{"_id":"themes/next/_config.yml","hash":"51f0c056bd939ac70d65bd801742e6cec0b79c0c","modified":1539962520498},{"_id":"themes/next/package.json","hash":"3963ad558a24c78a3fd4ef23cf5f73f421854627","modified":1539440001487},{"_id":"source/_discarded/hello-world.md","hash":"dc8aa4eb9c394dd0dacbe54dc955c018d5c174ef","modified":1539874959573},{"_id":"source/_discarded/xindeddfdfafadf.md","hash":"cb6e367b3dab6a171bd3aab24063a1ec603a8655","modified":1539874959574},{"_id":"source/_discarded/测试.md","hash":"768c0fa745359b291bd02fadb865dd673a05bb62","modified":1539874959574},{"_id":"source/_posts/ArcFace-Additive-Angular-Margain-Loss-for-Deep-Face-Recogniton（CVPR-2018）.md","hash":"0f58d279f71b0a64439dae52f9b8c87c95edf5d1","modified":1542207528352},{"_id":"source/_posts/K近邻法.md","hash":"e027c812667f7615cc8e2b13d71d02f0c63d0257","modified":1542207528352},{"_id":"source/_posts/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks（Alexnet）.md","hash":"88c2452686f03c1bc33623f19129d29e8f0cd60f","modified":1542207528352},{"_id":"source/_posts/LightGBM-A-Highly-Efficient-Gradient-Boosting-Decision-Tree（NIPS-2017）.md","hash":"bc1c6857e959f4f7c8d213b3fa357587d003b6e4","modified":1542207528352},{"_id":"source/_posts/vijosP1006晴天小猪历险记.md","hash":"c109b38b936c47efccfc0d32eedd4ea4f56e6ba0","modified":1541087089177},{"_id":"source/_posts/Leetcode53最大子序列和.md","hash":"7855e1d9a792c9b10324d489cb9a1880d9465f61","modified":1539874959575},{"_id":"source/_posts/决策树.md","hash":"e855fc7e31dc2510519480305dae1b79ab88a4d2","modified":1542207528352},{"_id":"source/_posts/提升方法.md","hash":"6d30e82fa2ff334e23e42b8aca4e2a0d1d522146","modified":1541087089178},{"_id":"source/_posts/支持向量机.md","hash":"14ca5b5a16b6a42d4a3048f9f58e08c7ab14834f","modified":1540654890795},{"_id":"source/_posts/神经网络.md","hash":"4fed98c26f52cc953f248e4c4ef60e908d2d2ad8","modified":1542207528352},{"_id":"source/_posts/模型评估与选择.md","hash":"b2b612533f7abc1ae08c40e7e05bb24944bf9295","modified":1542207528352},{"_id":"source/_posts/贝叶斯分类器.md","hash":"be6134a89655779e50c7e12975673b59ca256da9","modified":1542207528352},{"_id":"source/_posts/线性回归.md","hash":"ca2801482029b2ea6333b3fd2ad37bdb968aeabd","modified":1540183010920},{"_id":"source/categories/index.md","hash":"93be0af74d42e9b53e93c09ce79d0d2178720385","modified":1539440001412},{"_id":"source/tags/index.md","hash":"76b2abe27e18fc83272517811944598c1ab0428d","modified":1539440001413},{"_id":"source/images/kd_1.png","hash":"e4c7c25114b5179ce654035fb377f730cff343cf","modified":1542207528368},{"_id":"source/images/kd_2.png","hash":"8f56db99775f012b7cb7665958dae8b3f6ff77b8","modified":1542207528383},{"_id":"source/images/kd_3.png","hash":"ee53130e45b2698b98f38d5b67356180b36805be","modified":1542207528383},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5adfad3ef1b870063e621bc0838268eb2c7c697a","modified":1539440001421},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"a0a82dbfabdef9a9d7c17a08ceebfb4052d98d81","modified":1539440001422},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"1228506a940114288d61812bfe60c045a0abeac1","modified":1539440001422},{"_id":"themes/next/.github/browserstack_logo.png","hash":"a6c43887f64a7f48a2814e3714eaa1215e542037","modified":1539440001423},{"_id":"themes/next/layout/_layout.swig","hash":"2164570bb05db11ee4bcfbbb5d183a759afe9d07","modified":1539440001442},{"_id":"themes/next/layout/archive.swig","hash":"9a2c14874a75c7085d2bada5e39201d3fc4fd2b4","modified":1539440001483},{"_id":"themes/next/layout/category.swig","hash":"3cbb3f72429647411f9e85f2544bdf0e3ad2e6b2","modified":1539440001483},{"_id":"themes/next/layout/index.swig","hash":"555a357ecf17128db4e29346c92bb6298e66547a","modified":1539440001484},{"_id":"themes/next/layout/schedule.swig","hash":"87ad6055df01fa2e63e51887d34a2d8f0fbd2f5a","modified":1539440001486},{"_id":"themes/next/languages/de.yml","hash":"fd02d9c2035798d5dc7c1a96b4c3e24b05b31a47","modified":1539440001431},{"_id":"themes/next/layout/post.swig","hash":"7a6ce102ca82c3a80f776e555dddae1a9981e1ed","modified":1539440001485},{"_id":"themes/next/layout/page.swig","hash":"e8fcaa641d46930237675d2ad4b56964d9e262e9","modified":1539440001485},{"_id":"themes/next/languages/default.yml","hash":"b3bcd8934327448a43d9bfada5dd11b1b8c1402e","modified":1539440001432},{"_id":"themes/next/languages/en.yml","hash":"2f4b4776ca1a08cc266a19afb0d1350a3926f42c","modified":1539440001432},{"_id":"themes/next/layout/tag.swig","hash":"34e1c016cbdf94a31f9c5d494854ff46b2a182e9","modified":1539440001486},{"_id":"themes/next/languages/id.yml","hash":"dccae33e2a5b3c9f11c0e05ec4a7201af1b25745","modified":1539440001433},{"_id":"themes/next/languages/fr-FR.yml","hash":"efeeb55d5c4add54ad59a612fc0630ee1300388c","modified":1539440001433},{"_id":"themes/next/languages/it.yml","hash":"a215d016146b1bd92cef046042081cbe0c7f976f","modified":1539440001434},{"_id":"themes/next/languages/ja.yml","hash":"37f954e47a3bc669620ca559e3edb3b0072a4be5","modified":1539440001434},{"_id":"themes/next/languages/nl-NL.yml","hash":"213e7a002b82fb265f69dabafbbc382cfd460030","modified":1539440001436},{"_id":"themes/next/languages/ko.yml","hash":"dc8f3e8c64eb7c4bb2385025b3006b8efec8b31d","modified":1539440001435},{"_id":"themes/next/languages/pt-BR.yml","hash":"568d494a1f37726a5375b11452a45c71c3e2852d","modified":1539440001436},{"_id":"themes/next/languages/pt.yml","hash":"2efcd240c66ab1a122f061505ca0fb1e8819877b","modified":1539440001437},{"_id":"themes/next/languages/ru.yml","hash":"e33ee44e80f82e329900fc41eb0bb6823397a4d6","modified":1539440001437},{"_id":"themes/next/languages/vi.yml","hash":"a9b89ebd3e5933033d1386c7c56b66c44aca299a","modified":1539440001438},{"_id":"themes/next/languages/zh-Hans.yml","hash":"66b9b42f143c3cb2f782a94abd4c4cbd5fd7f55f","modified":1539440001439},{"_id":"themes/next/scripts/merge-configs.js","hash":"38d86aab4fc12fb741ae52099be475196b9db972","modified":1539440001487},{"_id":"themes/next/scripts/merge.js","hash":"39b84b937b2a9608b94e5872349a47200e1800ff","modified":1539440001489},{"_id":"themes/next/languages/zh-hk.yml","hash":"fe0d45807d015082049f05b54714988c244888da","modified":1539440001439},{"_id":"themes/next/languages/zh-tw.yml","hash":"432463b481e105073accda16c3e590e54c8e7b74","modified":1539440001440},{"_id":"themes/next/test/.jshintrc","hash":"c9fca43ae0d99718e45a6f5ce736a18ba5fc8fb6","modified":1539440001673},{"_id":"themes/next/test/helpers.js","hash":"f25e7f3265eb5a6e1ccbb5e5012fa9bebf134105","modified":1539440001673},{"_id":"themes/next/test/intern.js","hash":"db90b1063356727d72be0d77054fdc32fa882a66","modified":1539440001674},{"_id":"source/images/water3.0.png","hash":"e1fc6345abdc0d72151421e8859f692d3e21e36c","modified":1542207528446},{"_id":"source/images/xishi1.png","hash":"b07f13503390133e46b745e1c061bda24ae88610","modified":1542207528446},{"_id":"source/images/xishi3.png","hash":"f1cc2322d954e711a1e8df757c9b63dcf7afe4e2","modified":1542207528446},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001561},{"_id":"source/images/kd_example.png","hash":"cbeaba1e8d01757f1d1fad404c8ac40862f06309","modified":1542207528383},{"_id":"source/images/tongshi1.png","hash":"7a10a1cbde5d3f0f2a426d8c7d68012eaee067db","modified":1542207528430},{"_id":"source/images/paper/alex1.png","hash":"08db8b04c7c8c0c8f51691e3c79b98aa4c8820ca","modified":1542207528383},{"_id":"source/images/paper/alex10.png","hash":"9157f2781e98dc075b4fde444e9d691acb1cb722","modified":1542207528383},{"_id":"source/images/paper/alex12.png","hash":"77e8e6550ad85aae441369af45d28823a1085940","modified":1542207528383},{"_id":"source/images/paper/alex15.jpg","hash":"c5e0d7968cff08d4b97e889aad94f816de0ae991","modified":1542207528383},{"_id":"source/images/paper/alex13.jpg","hash":"9d737faf7482000fa798737b70fae19b4ff39099","modified":1542207528383},{"_id":"source/images/paper/alex14.jpg","hash":"8241421aae8cd68619ea1f269846ff0e033dedab","modified":1542207528383},{"_id":"source/images/paper/alex16.jpg","hash":"d81be72d1ef27cba9da8ba3a8bed646ea8d8d369","modified":1542207528383},{"_id":"source/images/paper/alex19.png","hash":"c84ec56af134859237e5da1c0708a6faaa3922c0","modified":1542207528399},{"_id":"source/images/paper/alex18.jpg","hash":"7f2b7e3052f14fdd98d938a65ce18e7b489ae40a","modified":1542207528399},{"_id":"source/images/paper/alex17.png","hash":"991d77abb0618153d01fbef1ec968f9255fa545d","modified":1542207528383},{"_id":"source/images/paper/alex21.jpg","hash":"0af70c13feb75f3ff59f4e39b423e1b919384572","modified":1542207528415},{"_id":"source/images/paper/alex4.png","hash":"6502b2998b3ba8077beb8d28780eaa4e91edb682","modified":1542207528415},{"_id":"source/images/paper/alex3.png","hash":"36ffb335b7f5273731829a6d26bb49c20ecf367e","modified":1542207528415},{"_id":"source/images/paper/alex5.jpg","hash":"63e665feda76c16488308614032120eda9a8291e","modified":1542207528415},{"_id":"source/images/paper/alex7.png","hash":"8b7186f7dd8a47f863bfee335dbcc7afd5f3cfdb","modified":1542207528415},{"_id":"source/images/paper/alex9.jpg","hash":"4d408ea7c07d37ec35b36c14807852f002c395c1","modified":1542207528415},{"_id":"source/images/paper/arc1.png","hash":"6219b87e36d021bede7e83863858028e21494a74","modified":1542207528415},{"_id":"source/images/paper/arc4.png","hash":"3bf6bcdcc30aaac8c7361edf79570f9b7257036a","modified":1542207528415},{"_id":"source/images/paper/arc3.png","hash":"efd5a056106d68385c6e744c7aec1d10c0dd7ec2","modified":1542207528415},{"_id":"source/images/paper/arc5.png","hash":"4472549f0d59e6706a7c38357cd9a07d5d7992bb","modified":1542207528430},{"_id":"source/images/paper/arc2.png","hash":"318fbc11309f1e693fc95adb300bfc92141282b4","modified":1542207528415},{"_id":"source/images/paper/arc6.png","hash":"796c3d4155043f94b0a5a7b46b7212338aa0b7fa","modified":1542207528430},{"_id":"source/images/paper/arc7.png","hash":"db2d509859bb8466c9c06fb53cfbfbd72352bbe1","modified":1542207528430},{"_id":"source/images/paper/lgb01.png","hash":"42c73ad2816e9ad2b8d5b91b539cd93c74b6d083","modified":1542207528430},{"_id":"source/images/paper/lgb04.png","hash":"68d1bb6ee332579b55bd40d582147a11ed62643f","modified":1542207528430},{"_id":"source/images/paper/lgb06.png","hash":"8464c256a12192f77535884661181945fb30c52d","modified":1542207528430},{"_id":"source/images/tongshi2.png","hash":"402c8563482f6f7a1202e55f18174561721491b3","modified":1542207528446},{"_id":"source/images/paper/lgb05.png","hash":"2e2612d7fd48cf7ed56c196a1caeeb99535e34c2","modified":1542207528430},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"f83befdc740beb8dc88805efd7fbb0fef9ed19be","modified":1539440001443},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"8c56dd26157cbc580ae41d97ac34b90ab48ced3f","modified":1539440001443},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"e2e4eae391476da994045ed4c7faf5e05aca2cd7","modified":1539440001446},{"_id":"themes/next/layout/_custom/header.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1539440001441},{"_id":"themes/next/layout/_macro/post.swig","hash":"4ba938822d56c597490f0731893eaa2443942e0f","modified":1539440001444},{"_id":"themes/next/layout/_macro/reward.swig","hash":"357d86ec9586705bfbb2c40a8c7d247a407db21a","modified":1539440001445},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"9c7343fd470e0943ebd75f227a083a980816290b","modified":1539440001445},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"ba8ab5a0280b953aa97435ff8946cbcbb2755a27","modified":1539440001441},{"_id":"themes/next/layout/_partials/footer.swig","hash":"0e377e83d297a3589690ed4fb918cbfb1abaf9db","modified":1539962469296},{"_id":"themes/next/layout/_partials/comments.swig","hash":"4adc65a602d1276615da3b887dcbf2ac68e7382b","modified":1539440001447},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"77c61e0baea3544df361b7338c3cd13dc84dde22","modified":1539440001450},{"_id":"themes/next/layout/_partials/head.swig","hash":"f14a39dad1ddd98e6d3ceb25dda092ba80d391b5","modified":1539440001448},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"1634fb887842698e01ff6e632597fe03c75d2d01","modified":1539440001451},{"_id":"themes/next/layout/_partials/header.swig","hash":"c54b32263bc8d75918688fb21f795103b3f57f03","modified":1539440001450},{"_id":"themes/next/layout/_partials/search.swig","hash":"b4ebe4a52a3b51efe549dd1cdee846103664f5eb","modified":1539440001452},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"ba75672183d94f1de7c8bd0eeee497a58c70e889","modified":1539440001475},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"8301c9600bb3e47f7fb98b0e0332ef3c51bb1688","modified":1539440001475},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"a0bd3388587fd943baae0d84ca779a707fbcad89","modified":1539440001475},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"554ec568e9d2c71e4a624a8de3cb5929050811d6","modified":1539440001477},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"9a188938d46931d5f3882a140aa1c48b3a893f0c","modified":1539440001478},{"_id":"themes/next/layout/_third-party/needsharebutton.swig","hash":"fa882641da3bd83d9a58a8a97f9d4c62a9ee7b5c","modified":1539440001476},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"db15d7e1552aa2d2386a6b8a33b3b3a40bf9e43d","modified":1539440001477},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"c0f5a0955f69ca4ed9ee64a2d5f8aa75064935ad","modified":1539440001457},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"931808ad9b8d8390c0dcf9bdeb0954eeb9185d68","modified":1539440001457},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9be624634703be496a5d2535228bc568a8373af9","modified":1539440001461},{"_id":"themes/next/scripts/tags/button.js","hash":"eddbb612c15ac27faf11c59c019ce188f33dec2c","modified":1539440001490},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"99b66949f18398689b904907af23c013be1b978f","modified":1539440001490},{"_id":"themes/next/scripts/tags/exturl.js","hash":"5022c0ba9f1d13192677cf1fd66005c57c3d0f53","modified":1539440001491},{"_id":"themes/next/scripts/tags/full-image.js","hash":"c9f833158c66bd72f627a0559cf96550e867aa72","modified":1539440001491},{"_id":"themes/next/scripts/tags/label.js","hash":"6f00952d70aadece844ce7fd27adc52816cc7374","modified":1539440001493},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"ac681b0d0d8d39ba3817336c0270c6787c2b6b70","modified":1539440001492},{"_id":"themes/next/scripts/tags/note.js","hash":"f7eae135f35cdab23728e9d0d88b76e00715faa0","modified":1539440001494},{"_id":"themes/next/scripts/tags/lazy-image.js","hash":"bcba2ff25cd7850ce6da322d8bd85a8dd00b5ceb","modified":1539440001493},{"_id":"themes/next/scripts/tags/tabs.js","hash":"aa7fc94a5ec27737458d9fe1a75c0db7593352fd","modified":1539440001494},{"_id":"themes/next/source/css/main.styl","hash":"a91dbb7ef799f0a171b5e726c801139efe545176","modified":1539440001560},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1539440001562},{"_id":"themes/next/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1539440001563},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1539440001563},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1539440001564},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1539440001564},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1539440001565},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1539440001566},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1539440001567},{"_id":"themes/next/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1539440001568},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1539440001566},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1539440001569},{"_id":"themes/next/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1539440001569},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1539440001568},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1539440001570},{"_id":"themes/next/source/images/logo.svg","hash":"169f56fd82941591dad3abd734a50ec7259be950","modified":1539440001570},{"_id":"themes/next/source/images/quote-l.svg","hash":"cd108d6f44351cadf8e6742565217f88818a0458","modified":1539440001571},{"_id":"themes/next/source/images/quote-r.svg","hash":"2a2a250b32a87c69dcc1b1976c74b747bedbfb41","modified":1539440001571},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1539440001572},{"_id":"source/images/bayes2.png","hash":"08923f8f93387b83c2b3d360dfc0b132a4222377","modified":1542207528368},{"_id":"source/images/p54.jpg","hash":"f9b9671cfe921cfb196a6b7bb4dd8a85cd1d8b82","modified":1539874959581},{"_id":"source/images/paper/alex11.png","hash":"9449e345998a9644e5c8e0aba325dfc9b6eb0a85","modified":1542207528383},{"_id":"source/images/paper/alex6.jpg","hash":"a3005b380dda442b9a5d69241a1883832fa44594","modified":1542207528415},{"_id":"source/images/paper/alex8.jpg","hash":"0eccf0149faa987551f6f5e3590c385d179c2bb6","modified":1542207528415},{"_id":"source/images/paper/lgb03.png","hash":"b30056fb284e9e51883736fa94ecf056c04eccb5","modified":1542207528430},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001459},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001460},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001540},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001540},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001542},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001558},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1539440001560},{"_id":"source/images/p59.jpg","hash":"760780f746d8e3bae307ed28e9c2e3bed4625e9f","modified":1539874959587},{"_id":"source/images/paper/lgb02.png","hash":"27bd36c224870d594dc61e77f635aa251cc0c642","modified":1542207528430},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"a223919d2e1bf17ca4d6abb2c86f2efca9883dc1","modified":1539440001449},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"f5e487b0d213ca0bd94aa30bc23b240d65081627","modified":1539440001449},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"b2f0d247b213e4cf8de47af6a304d98070cc7256","modified":1539440001452},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"a8c7f9ca7c605d039a1f3bf4e4d3183700a3dd62","modified":1539440001453},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"b25002a83cbd2ca0c4a5df87ad5bff26477c0457","modified":1539440001453},{"_id":"themes/next/layout/_third-party/analytics/analytics-with-widget.swig","hash":"ff947f3561b229bc528cb1837d4ca19612219411","modified":1539440001462},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"753d262911c27baf663fcaf199267133528656af","modified":1539440001463},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"71397a5823e8ec8aad3b68aace13150623b3e19d","modified":1539440001463},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"a10b7f19d7b5725527514622899df413a34a89db","modified":1539440001464},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"7d94845f96197d9d84a405fa5d4ede75fb81b225","modified":1539440001465},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"7b11eac3a0685fa1ab2ab6ecff60afc4f15f0d16","modified":1539440001463},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b1e13df83fb2b1d5d513b30b7aa6158b0837daab","modified":1539440001466},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"45f3f629c2aacc381095750e1c8649041a71a84b","modified":1539440001466},{"_id":"themes/next/layout/_third-party/analytics/firestore.swig","hash":"ccc443b22bd4f8c7ac4145664686c756395b90e0","modified":1539440001465},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"8a399df90dadba5ad4e781445b58f4765aeb701e","modified":1539440001467},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"9e3d133ac5bcc6cb51702c83b2611a49811abad1","modified":1539440001454},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"e6d10ee4fb70b3ae1cd37e9e36e000306734aa2e","modified":1539440001467},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"5a8027328f060f965b3014060bebec1d7cf149c1","modified":1539440001468},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"f9a1647a8f1866deeb94052d1f87a5df99cb1e70","modified":1539440001468},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"d9e2d9282f9be6e04eae105964abb81e512bffed","modified":1539440001455},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"d4fbffd7fa8f2090eb32a871872665d90a885fac","modified":1539440001455},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"0a9cdd6958395fcdffc80ab60f0c6301b63664a5","modified":1539440001456},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"4c501ea0b9c494181eb3c607c5526a5754e7fbd8","modified":1539440001470},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"b83a51bbe0f1e2ded9819070840b0ea145f003a6","modified":1539440001470},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"9246162d4bc7e949ce1d12d135cbbaf5dc3024ec","modified":1539440001473},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"1600f340e0225361580c44890568dc07dbcf2c89","modified":1539440001471},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"493bd5999a1061b981922be92d8277a0f9152447","modified":1539440001472},{"_id":"themes/next/layout/_third-party/comments/gitment.swig","hash":"4dcc3213c033994d342d02b800b6229295433d30","modified":1539440001471},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"af7f3e43cbdc4f88c13f101f0f341af96ace3383","modified":1539440001472},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"4050553d44ba1396174161c9a6bb0f89fa779eca","modified":1539440001474},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"7e65ff8fe586cd655b0e9d1ad2912663ff9bd36c","modified":1539440001474},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"34599633658f3b0ffb487728b7766e1c7b551f5a","modified":1539440001480},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"93479642fd076a1257fecc25fcf5d20ccdefe509","modified":1539440001481},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"d8c98938719284fa06492c114d99a1904652a555","modified":1539440001482},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"fe95dd3d166634c466e19aa756e65ad6e8254d3e","modified":1539440001481},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1539440001459},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"9b84ab576982b2c3bb0291da49143bc77fba3cc6","modified":1539440001458},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a9a3995b9615adfb8d6b127c78c6771627bee19a","modified":1539440001460},{"_id":"themes/next/source/css/_mixins/Gemini.styl","hash":"07f7da320689f828f6e36a6123807964a45157a0","modified":1539440001540},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"7896c3ee107e1a8b9108b6019f1c070600a1e8cc","modified":1539440001541},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"3403fdd8efde1a0afd11ae8a5a97673f5903087f","modified":1539440001539},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"0e55cbd93852dc3f8ccb44df74d35d9918f847e0","modified":1539440001542},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"a25408534f8fe6e321db4bbf9dd03335d648fe17","modified":1539440001558},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"58e7dd5947817d9fc30770712fc39b2f52230d1e","modified":1539440001557},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4069f918ccc312da86db6c51205fc6c6eaabb116","modified":1539440001559},{"_id":"themes/next/source/css/_variables/base.styl","hash":"b1f6ea881a4938a54603d68282b0f8efb4d7915d","modified":1539440001559},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"cb431b54ba9c692165a1f5a12e4c564a560f8058","modified":1539440001574},{"_id":"themes/next/source/js/src/affix.js","hash":"1b509c3b5b290a6f4607f0f06461a0c33acb69b1","modified":1539440001573},{"_id":"themes/next/source/js/src/exturl.js","hash":"a2a0f0de07e46211f74942a468f42ee270aa555c","modified":1539440001575},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"0289031200c3d4c2bdd801ee10fff13bb2c353e4","modified":1539440001574},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"b35a7dc47b634197b93487cea8671a40a9fdffce","modified":1539440001575},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"1512c751d219577d338ac0780fb2bbd9075d5298","modified":1539440001576},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"02cf91514e41200bc9df5d8bdbeb58575ec06074","modified":1539440001579},{"_id":"themes/next/source/js/src/motion.js","hash":"885176ed51d468f662fbf0fc09611f45c7e5a3b1","modified":1539440001577},{"_id":"themes/next/source/js/src/post-details.js","hash":"93a18271b4123dd8f94f09d1439b47c3c19a8712","modified":1539440001577},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"b7657be25fc52ec67c75ab5481bdcb483573338b","modified":1539440001579},{"_id":"themes/next/source/js/src/utils.js","hash":"b3e9eca64aba59403334f3fa821f100d98d40337","modified":1539440001580},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1539440001591},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"9be892a4e14e0da18ff9cb962c9ef71f163b1b22","modified":1539440001599},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1539440001597},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"672d3b5767e0eacd83bb41b188c913f2cf754793","modified":1539440001600},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"b02737510e9b89aeed6b54f89f602a9c24b06ff2","modified":1539440001598},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"bf3eef9d647cd7c9b62feda3bc708c6cdd7c0877","modified":1539440001611},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"a9b3ee1e4db71a0e4ea6d5bed292d176dd68b261","modified":1539440001613},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"6f474ea75c42442da7bbcf2e9143ce98258efd8d","modified":1539440001612},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"68a9b9d53126405b0fa5f3324f1fb96dbcc547aa","modified":1539440001613},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"03ddbf76c1dd1afb93eed0b670d2eee747472ef1","modified":1539440001617},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"ee33b2798b1e714b904d663436c6b3521011d1fa","modified":1539440001618},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"b4aefc910578d76b267e86dfffdd5121c8db9aec","modified":1539440001616},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"c31ff06a740955e44edd4403902e653ccabfd4db","modified":1539440001617},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"71e7183634dc1b9449f590f15ebd7201add22ca7","modified":1539440001618},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"865d6c1328ab209a4376b9d2b7a7824369565f28","modified":1539440001636},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"90fa628f156d8045357ff11eaf32e61abacf10e8","modified":1539440001639},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4ded6fee668544778e97e38c2b211fc56c848e77","modified":1539440001640},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"b930297cb98b8e1dbd5abe9bc1ed9d5935d18ce8","modified":1539440001640},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"e0acf1db27b0cc16128a59c46db1db406b5c4c58","modified":1539440001641},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"f4a570908f6c89c6edfb1c74959e733eaadea4f2","modified":1539440001641},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"bf773ad48a0b9aa77681a89d7569eefc0f7b7b18","modified":1539440001642},{"_id":"themes/next/source/lib/needsharebutton/font-embedded.css","hash":"14264a210bf94232d58d7599ea2ba93bfa4fb458","modified":1539440001644},{"_id":"themes/next/source/lib/pace/pace-theme-big-counter.min.css","hash":"5b561dc328af4c4d512e20a76fe964d113a32ba8","modified":1539440001647},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.css","hash":"e33aa8fa48b6639d8d8b937d13261597dd473b3a","modified":1539440001644},{"_id":"themes/next/source/lib/needsharebutton/needsharebutton.js","hash":"2ce5f3bf15c523b9bfc97720d8884bb22602a454","modified":1539440001645},{"_id":"themes/next/source/lib/pace/pace-theme-barber-shop.min.css","hash":"ee0d51446cb4ffe1bb96bd7bc8c8e046dddfcf46","modified":1539440001646},{"_id":"themes/next/source/lib/pace/pace-theme-bounce.min.css","hash":"f6bdb9a785b7979dd8ec5c60e278af955ef1e585","modified":1539440001647},{"_id":"themes/next/source/lib/pace/pace-theme-center-atom.min.css","hash":"dcf79c24fe5350fb73d8038573a104e73639e9d3","modified":1539440001648},{"_id":"themes/next/source/lib/pace/pace-theme-center-circle.min.css","hash":"a4066769c78affbfbc5e30a600e2c7862cd532e0","modified":1539440001648},{"_id":"themes/next/source/lib/pace/pace-theme-center-radar.min.css","hash":"ab7cba998bf4c03b13df342bf43647fa4f419783","modified":1539440001649},{"_id":"themes/next/source/lib/pace/pace-theme-center-simple.min.css","hash":"67f44c947548bd4d77e7590d3f59e236cbf9e98a","modified":1539440001649},{"_id":"themes/next/source/lib/pace/pace-theme-fill-left.min.css","hash":"0bec1e235a4a2cccda3f993b205424e1441a44ae","modified":1539440001650},{"_id":"themes/next/source/lib/pace/pace-theme-corner-indicator.min.css","hash":"b3c64c973f31884e3d8145989476707333406b9a","modified":1539440001650},{"_id":"themes/next/source/lib/pace/pace-theme-minimal.min.css","hash":"9cd783cceb8a191f3c8b5d81f7a430ecc3e489d3","modified":1539440001653},{"_id":"themes/next/source/lib/pace/pace-theme-flash.min.css","hash":"13ace22c40312d7bbd8d9c1e50eff897a7a497d8","modified":1539440001651},{"_id":"themes/next/source/lib/pace/pace-theme-loading-bar.min.css","hash":"7ee28875dfc1230d76c537f6605766e8d4011e9f","modified":1539440001651},{"_id":"themes/next/source/lib/pace/pace-theme-mac-osx.min.css","hash":"9f2e7b51b084da407863826b25265b31150b3821","modified":1539440001652},{"_id":"themes/next/source/lib/pace/pace.min.js","hash":"8aaa675f577d5501f5f22d5ccb07c2b76310b690","modified":1539440001653},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"2d9a9f38c493fdf7c0b833bb9184b6a1645c11b2","modified":1539440001655},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"46a50b91c98b639c9a2b9265c5a1e66a5c656881","modified":1539440001655},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"92d92860418c4216aa59eb4cb4a556290a7ad9c3","modified":1539440001666},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"8148492dd49aa876d32bb7d5b728d3f5bf6f5074","modified":1539440001656},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"63da5e80ebb61bb66a2794d5936315ca44231f0c","modified":1539440001665},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1539440001670},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"dbbfb50f6502f6b81dcc9fee7b31f1e812da3464","modified":1539440001671},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1539440001672},{"_id":"themes/next/source/lib/jquery/index.js","hash":"17a740d68a1c330876c198b6a4d9319f379f3af2","modified":1539440001638},{"_id":"source/images/xishi2.png","hash":"e4d2ed80de33f84d55354daffbf576825bef6bd9","modified":1542207528446},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"2530de0f3125a912756f6c0e9090cd012134a4c5","modified":1539440001480},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"218cc936ba3518a3591b2c9eda46bc701edf7710","modified":1539440001479},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"8f86f694c0749a18ab3ad6f6df75466ca137a4bc","modified":1539440001496},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"7ad4081466b397e2a6204141bb7768b7c01bd93c","modified":1539440001498},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"ff4489cd582f518bba6909a301ac1292a38b4e96","modified":1539440001498},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"8b32928686c327151e13d3ab100157f9a03cd59f","modified":1539440001497},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"237d185ac62ec9877e300947fa0109c44fb8db19","modified":1539440001497},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"4f2801fc4cf3f31bf2069f41db8c6ce0e3da9e39","modified":1539440001508},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"6eb4bcc3056bd279d000607e8b4dad50d368ca69","modified":1539440001523},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"12662536c7a07fff548abe94171f34b768dd610f","modified":1539440001534},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"24ee4b356ff55fc6e58f26a929fa07750002cf29","modified":1539440001535},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"1da5c800d025345f212a3bf1be035060f4e5e6ed","modified":1539440001536},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"a17e2b871a335f290afb392a08f94fd35f59c715","modified":1539440001538},{"_id":"themes/next/source/css/_common/scaffolding/mobile.styl","hash":"91ca75492cd51f2553f4d294ed2f48239fcd55eb","modified":1539440001536},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"3f40e8a9fe8e7bd5cfc4cf4cbbbcb9539462e973","modified":1539440001537},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"31127dcbf4c7b4ada53ffbf1638b5fe325b7cbc0","modified":1539440001550},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"e695e58f714129ca292c2e54cd62c251aca7f7fe","modified":1539440001551},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"ea9069645696f86c5df64208490876fe150c8cae","modified":1539440001538},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"748dbfbf9c08e719ddc775958003c64b00d39dab","modified":1539440001551},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1539440001552},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"60fa84aa7731760f05f52dd7d8f79b5f74ac478d","modified":1539440001544},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"25d5e45a355ee2093f3b8b8eeac125ebf3905026","modified":1539440001544},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"5dbc0d0c897e46760e5dbee416530d485c747bba","modified":1539440001552},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"26666c1f472bf5f3fb9bc62081cca22b4de15ccb","modified":1539440001546},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"b1025c421406d2c24cc92a02ae28c1915b01e240","modified":1539440001545},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"d0bfd1bef988c76f7d7dd72d88af6f0908a8b0db","modified":1539440001545},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"09c965022c13b84ed8a661fee8ac2a6d550495ae","modified":1539440001547},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"bce344d3a665b4c55230d2a91eac2ad16d6f32fd","modified":1539440001554},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"9c99034f8e00d47e978b3959f51eb4a9ded0fcc8","modified":1539440001546},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9b913b73d31d21f057f97115ffab93cfa578b884","modified":1539440001548},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"1f6e2ce674735269599acc6d77b3ea18d31967fc","modified":1539440001555},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"416988dca389e6e2fdfa51fa7f4ee07eb53f82fb","modified":1539440001554},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"86197902dfd3bededba10ba62b8f9f22e0420bde","modified":1539440001556},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"ad2dcedf393ed1f3f5afd2508d24969c916d02fc","modified":1539440001556},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"4642e30010af8b2b037f5b43146b10a934941958","modified":1539440001555},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"f1d0b5d7af32c423eaa8bb93ab6a0b45655645dc","modified":1539440001578},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"6c26cdb36687d4f0a11dabf5290a909c3506be5c","modified":1539440001586},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"6d586bfcfb7ae48f1b12f76eec82d3ad31947501","modified":1539440001589},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"16b03db23a52623348f37c04544f2792032c1fb6","modified":1539440001590},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1539440001601},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1539440001603},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1539440001602},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1539440001602},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1539440001601},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1539440001604},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1539440001608},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"d71602cbca33b9ecdb7ab291b7f86a49530f3601","modified":1539440001609},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"1d6aeda0480d0e4cb6198edf7719d601d4ae2ccc","modified":1539440001615},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1539440001610},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"3655f1fdf1e584c4d8e8d39026093ca306a5a341","modified":1539440001620},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1539440001615},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"1573904b82807abbb32c97a3632c6c6808eaac50","modified":1539440001621},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"88af80502c44cd52ca81ffe7dc7276b7eccb06cf","modified":1539440001621},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"41ea797c68dbcff2f6fb3aba1d1043a22e7cc0f6","modified":1539440001664},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"a817b6c158cbc5bab3582713de9fe18a18a80552","modified":1539440001664},{"_id":"source/images/paper/alex20.jpg","hash":"dd683df00a4b08059a9ef866aaa7b4eeb0f50dbd","modified":1542207528415},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"4ac683b2bc8531c84d98f51b86957be0e6f830f3","modified":1539440001588},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1539440001634},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1539440001635},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"4237c6e9d59da349639de20e559e87c2c0218cfd","modified":1539440001669},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"9f73c4696f0907aa451a855444f88fc0698fa472","modified":1539440001500},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"167986d0f649516671ddf7193eebba7b421cd115","modified":1539440001503},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"53cde051e0337f4bf42fb8d6d7a79fa3fa6d4ef2","modified":1539440001501},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"7fe4d4d656e86276c17cb4e48a560cb6a4def703","modified":1539440001505},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d63e0cacc53dd375fcc113465a4328c59ff5f2c1","modified":1539440001501},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"1a0d059799a298fe17c49a44298d32cebde93785","modified":1539440001502},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"50450d9fdc8a2b2be8cfca51e3e1a01ffd636c0b","modified":1539440001504},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"1727702eac5d326b5c81a667944a245016668231","modified":1539440001503},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"0656e753f182c9f47fef7304c847b7587a85ef0d","modified":1539440001502},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"b6f3a06a94a6ee5470c956663164d58eda818a64","modified":1539440001505},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"f9760ecf186954cee3ba4a149be334e9ba296b89","modified":1539440001506},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"7fb593f90d74a99c21840679933b9ef6fdc16a61","modified":1539440001506},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"4e3838d7ac81d9ad133960f0f7ed58a44a015285","modified":1539440001507},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"39f04c4c7237a4e10acd3002331992b79945d241","modified":1539440001517},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"761eba9811b050b25d548cc0854de4824b41eb08","modified":1539440001518},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"8cf318644acc8b4978537c263290363e21c7f5af","modified":1539440001508},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"8dd9a1c6f4f6baa00c2cf01837e7617120cf9660","modified":1539440001518},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-dimmer.styl","hash":"11c22f0fb3f6beb13e5a425ec064a4ff974c13b7","modified":1539440001519},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"61f8cea3c01acd600e90e1bc2a07def405503748","modified":1539440001519},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"1153bb71edf253765145559674390e16dd67c633","modified":1539440001520},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"e71652d3216e289c8548b1ea2357822c1476a425","modified":1539440001522},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"c8fe49a4bc014c24dead05b782a7082411a4abc5","modified":1539440001520},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a1521d48bb06d8d703753f52a198baa197af7da2","modified":1539440001521},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"62fbbd32cf5a99ae550c45c763a2c4813a138d01","modified":1539440001509},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"5ef6343835f484a2c0770bd1eb9cc443609e4c39","modified":1539440001522},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"a200c0a1c5a895ac9dc41e0641a5dfcd766be99b","modified":1539440001511},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"caf263d1928496688c0e1419801eafd7e6919ce5","modified":1539440001510},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"cd9e214e502697f2f2db84eb721bac57a49b0fce","modified":1539440001512},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"a6c6eb8adba0a090ad1f4b9124e866887f20d10d","modified":1539440001511},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"875cbe88d5c7f6248990e2beb97c9828920e7e24","modified":1539440001510},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"27deb3d3a243d30022055dac7dad851024099a8b","modified":1539440001513},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"b2495ae5e04dcca610aacadc47881d9e716cd440","modified":1539440001514},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5a982d8ef3b3623ea5f59e63728990f5623c1b57","modified":1539440001514},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"d0d7a5c90d62b685520d2b47fea8ba6019ff5402","modified":1539440001512},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"ccb34c52be8adba5996c6b94f9e723bd07d34c16","modified":1539440001515},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"ca88ea6999a61fb905eb6e72eba5f92d4ee31e6e","modified":1539440001513},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"01567edaea6978628aa5521a122a85434c418bfd","modified":1539440001515},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"7968343e41f8b94b318c36289dff1196c3eb1791","modified":1539440001516},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"89d6c3b697efc63de42afd2e89194b1be14152af","modified":1539440001516},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"a3bdd71237afc112b2aa255f278cab6baeb25351","modified":1539440001524},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"2fe76476432b31993338cb45cdb3b29a518b6379","modified":1539440001524},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"2ad1a2a9bbf6742d1b0762c4c623b68113d1e0fe","modified":1539440001525},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"f825da191816eef69ea8efb498a7f756d5ebb498","modified":1539440001525},{"_id":"themes/next/source/css/_common/components/tags/label.styl","hash":"2ab1322fe52ab5aafd49e68f5bd890e8380ee927","modified":1539440001526},{"_id":"themes/next/source/css/_common/components/tags/note-modern.styl","hash":"b7076e58d647265ee0ad2b461fe8ce72c9373bc5","modified":1539440001526},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"9a409b798decdefdaf7a23f0b11004a8c27e82f3","modified":1539440001527},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"b80604868e4f5cf20fccafd7ee415c20c804f700","modified":1539440001528},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"5dbeed535d63a50265d96b396a5440f9bb31e4ba","modified":1539440001529},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"a6e7d698702c2e383dde3fde2abde27951679084","modified":1539440001530},{"_id":"themes/next/source/css/_common/components/tags/tabs.styl","hash":"154a87a32d2fead480d5e909c37f6c476671c5e6","modified":1539440001527},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"bba4f3bdb7517cd85376df3e1209b570c0548c69","modified":1539440001529},{"_id":"themes/next/source/css/_common/components/third-party/gitment.styl","hash":"874278147115601d2abf15987f5f7a84ada1ac6b","modified":1539440001531},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"10599e16414a8b7a76c4e79e6617b5fe3d4d1adf","modified":1539440001531},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"717cc7f82be9cc151e23a7678601ff2fd3a7fa1d","modified":1539440001530},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"15975ba7456b96916b1dbac448a1a0d2c38b8f3d","modified":1539440001532},{"_id":"themes/next/source/css/_common/components/third-party/needsharebutton.styl","hash":"28825ae15fa20ae3942cdaa7bcc1f3523ce59acc","modified":1539440001533},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"16087276945fa038f199692e3eabb1c52b8ea633","modified":1539440001532},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"9c8196394a89dfa40b87bf0019e80144365a9c93","modified":1539440001533},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1539440001553},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"a07aa12cc36ac5c819670c2a3c17d07ed7a08986","modified":1539440001549},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"1f09be9bb38411f0629b58c3b23873589a6dbcaa","modified":1539440001549},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1539440001583},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1539440001582},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1539440001584},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1539440001585},{"_id":"themes/next/source/lib/Han/dist/font/han.woff2","hash":"623af3ed5423371ac136a4fe0e8cc7bb7396037a","modified":1539440001585},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1539440001605},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"6394c48092085788a8c0ef72670b0652006231a1","modified":1539440001605},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"ee948b4489aedeb548a77c9e45d8c7c5732fd62d","modified":1539440001606},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"51139a4c79573d372a347ef01a493222a1eaf10a","modified":1539440001606},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"b88b589f5f1aa1b3d87cc7eef34c281ff749b1ae","modified":1539440001607},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"d22b1629cb23a6181bebb70d0cf653ffe4b835c8","modified":1539440001608},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1539440001624},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1539440001628},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1539440001633},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"90a1b22129efc172e2dfcceeeb76bff58bc3192f","modified":1539440001596},{"_id":"themes/next/source/lib/three/three.min.js","hash":"26273b1cb4914850a89529b48091dc584f2c57b8","modified":1539440001662},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"b5483b11f8ba213e733b5b8af9927a04fec996f6","modified":1539440001631},{"_id":"source/images/paper/alex2.png","hash":"00eb87eadfcc165554c17364226ec41f931830e3","modified":1542207528399},{"_id":"source/images/bayes1.jpg","hash":"9937a581d958b1f2352b2df32a262f7dbf081738","modified":1542207528368}],"Category":[{"name":"论文阅读","_id":"cjohajwn50002m4v52rzhyu55"},{"name":"机器学习","_id":"cjohajwne0006m4v5n4scmyao"},{"name":"算法","_id":"cjohajwnp000cm4v5nchh7zpl"}],"Data":[],"Page":[{"title":"categories","date":"2018-10-11T10:52:50.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-10-11 18:52:50\ntype: \"categories\"\n---\n","updated":"2018-10-13T14:13:21.412Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjohajwwi000km4v5mdlr7fck","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2018-10-11T10:56:52.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-10-11 18:56:52\ntype: \"tags\"\n---\n","updated":"2018-10-13T14:13:21.413Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjohajwwm000mm4v5zx862rz7","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"ArcFace:Additive Angular Margain Loss for Deep Face Recogniton（CVPR 2018）","date":"2018-11-09T12:46:10.000Z","_content":"主要是模式识别的seminar的看的\n# 简介 #\n目前深度卷积神经网络在人脸识别任务上取得了很好的效果，不同的神经网络主要在训练数据集、网络设置和损失函数三种属性有所区别，\n\n文章据此做了相关改进，并主要有以下四个贡献\n\n1.清洗了最大人脸公共训练数据集（MS1M）和测试数据集（MegaFace）\n\n2.探索不同网络设置，并分析精度与速度之间的关系\n\n**3.提出了一种几何可解释的损失函数ArcFace，并优于softmax，SphereFace和CosineFace**\n\n4.在MegaFace人脸数据集上取得了最先进的表现\n# 从softmax到arcFace #\nsoftmax仅能做到分类，但是我们人脸识别做的不仅仅是分类，而是相同的人尽量聚在一起，不同的人尽量分开\n\n最原始的softmax函数\n\n<img src='/images/paper/arc1.png' width=\"640\"/>\n\n<img src=\"/images/paper/arc2.png\" width='640'/>\n\n令偏置b为0，然后权重和输入的内积用上面式子表示，用L2正则化处理Wj使得||Wj||=1，L2正则化就是将Wj向量中的每个值都分别除以Wj的模，从而得到新的Wj，新的Wj的模就是1\n\n<img src=\"/images/paper/arc3.png\" width='640'/>\n\n然后一方面对输入xi也用L2正则化处理，同时再乘以一个scale参数s；另一方面将cos(θyi)用cos(θyi+m)替代\n\n<img src=\"/images/paper/arc4.png\" width='640'/>\n\n# 几何解释 #\n使用二分类来证明，决策边界就是在这个边界上属于正类和属于负类的概率一样的，所以可以得到以下这个表格和图像\n<img src=\"/images/paper/arc5.png\" width='640'/>\n\n<img src=\"/images/paper/arc6.png\" width='640'/>\n\n一些实验\n\n<img src=\"/images/paper/arc7.png\" width='640'/>\n\n\n\n\n\n\n\n\n","source":"_posts/ArcFace-Additive-Angular-Margain-Loss-for-Deep-Face-Recogniton（CVPR-2018）.md","raw":"---\ntitle: 'ArcFace:Additive Angular Margain Loss for Deep Face Recogniton（CVPR 2018）'\ncategories:\n  - 论文阅读\ndate: 2018-11-09 20:46:10\ntags:\n---\n主要是模式识别的seminar的看的\n# 简介 #\n目前深度卷积神经网络在人脸识别任务上取得了很好的效果，不同的神经网络主要在训练数据集、网络设置和损失函数三种属性有所区别，\n\n文章据此做了相关改进，并主要有以下四个贡献\n\n1.清洗了最大人脸公共训练数据集（MS1M）和测试数据集（MegaFace）\n\n2.探索不同网络设置，并分析精度与速度之间的关系\n\n**3.提出了一种几何可解释的损失函数ArcFace，并优于softmax，SphereFace和CosineFace**\n\n4.在MegaFace人脸数据集上取得了最先进的表现\n# 从softmax到arcFace #\nsoftmax仅能做到分类，但是我们人脸识别做的不仅仅是分类，而是相同的人尽量聚在一起，不同的人尽量分开\n\n最原始的softmax函数\n\n<img src='/images/paper/arc1.png' width=\"640\"/>\n\n<img src=\"/images/paper/arc2.png\" width='640'/>\n\n令偏置b为0，然后权重和输入的内积用上面式子表示，用L2正则化处理Wj使得||Wj||=1，L2正则化就是将Wj向量中的每个值都分别除以Wj的模，从而得到新的Wj，新的Wj的模就是1\n\n<img src=\"/images/paper/arc3.png\" width='640'/>\n\n然后一方面对输入xi也用L2正则化处理，同时再乘以一个scale参数s；另一方面将cos(θyi)用cos(θyi+m)替代\n\n<img src=\"/images/paper/arc4.png\" width='640'/>\n\n# 几何解释 #\n使用二分类来证明，决策边界就是在这个边界上属于正类和属于负类的概率一样的，所以可以得到以下这个表格和图像\n<img src=\"/images/paper/arc5.png\" width='640'/>\n\n<img src=\"/images/paper/arc6.png\" width='640'/>\n\n一些实验\n\n<img src=\"/images/paper/arc7.png\" width='640'/>\n\n\n\n\n\n\n\n\n","slug":"ArcFace-Additive-Angular-Margain-Loss-for-Deep-Face-Recogniton（CVPR-2018）","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwmt0000m4v5xy55t9yu","content":"<p>主要是模式识别的seminar的看的</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>目前深度卷积神经网络在人脸识别任务上取得了很好的效果，不同的神经网络主要在训练数据集、网络设置和损失函数三种属性有所区别，</p>\n<p>文章据此做了相关改进，并主要有以下四个贡献</p>\n<p>1.清洗了最大人脸公共训练数据集（MS1M）和测试数据集（MegaFace）</p>\n<p>2.探索不同网络设置，并分析精度与速度之间的关系</p>\n<p><strong>3.提出了一种几何可解释的损失函数ArcFace，并优于softmax，SphereFace和CosineFace</strong></p>\n<p>4.在MegaFace人脸数据集上取得了最先进的表现</p>\n<h1 id=\"从softmax到arcFace\"><a href=\"#从softmax到arcFace\" class=\"headerlink\" title=\"从softmax到arcFace\"></a>从softmax到arcFace</h1><p>softmax仅能做到分类，但是我们人脸识别做的不仅仅是分类，而是相同的人尽量聚在一起，不同的人尽量分开</p>\n<p>最原始的softmax函数</p>\n<p><img src=\"/images/paper/arc1.png\" width=\"640\"></p>\n<p><img src=\"/images/paper/arc2.png\" width=\"640\"></p>\n<p>令偏置b为0，然后权重和输入的内积用上面式子表示，用L2正则化处理Wj使得||Wj||=1，L2正则化就是将Wj向量中的每个值都分别除以Wj的模，从而得到新的Wj，新的Wj的模就是1</p>\n<p><img src=\"/images/paper/arc3.png\" width=\"640\"></p>\n<p>然后一方面对输入xi也用L2正则化处理，同时再乘以一个scale参数s；另一方面将cos(θyi)用cos(θyi+m)替代</p>\n<p><img src=\"/images/paper/arc4.png\" width=\"640\"></p>\n<h1 id=\"几何解释\"><a href=\"#几何解释\" class=\"headerlink\" title=\"几何解释\"></a>几何解释</h1><p>使用二分类来证明，决策边界就是在这个边界上属于正类和属于负类的概率一样的，所以可以得到以下这个表格和图像<br><img src=\"/images/paper/arc5.png\" width=\"640\"></p>\n<p><img src=\"/images/paper/arc6.png\" width=\"640\"></p>\n<p>一些实验</p>\n<p><img src=\"/images/paper/arc7.png\" width=\"640\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>主要是模式识别的seminar的看的</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>目前深度卷积神经网络在人脸识别任务上取得了很好的效果，不同的神经网络主要在训练数据集、网络设置和损失函数三种属性有所区别，</p>\n<p>文章据此做了相关改进，并主要有以下四个贡献</p>\n<p>1.清洗了最大人脸公共训练数据集（MS1M）和测试数据集（MegaFace）</p>\n<p>2.探索不同网络设置，并分析精度与速度之间的关系</p>\n<p><strong>3.提出了一种几何可解释的损失函数ArcFace，并优于softmax，SphereFace和CosineFace</strong></p>\n<p>4.在MegaFace人脸数据集上取得了最先进的表现</p>\n<h1 id=\"从softmax到arcFace\"><a href=\"#从softmax到arcFace\" class=\"headerlink\" title=\"从softmax到arcFace\"></a>从softmax到arcFace</h1><p>softmax仅能做到分类，但是我们人脸识别做的不仅仅是分类，而是相同的人尽量聚在一起，不同的人尽量分开</p>\n<p>最原始的softmax函数</p>\n<p><img src=\"/images/paper/arc1.png\" width=\"640\"></p>\n<p><img src=\"/images/paper/arc2.png\" width=\"640\"></p>\n<p>令偏置b为0，然后权重和输入的内积用上面式子表示，用L2正则化处理Wj使得||Wj||=1，L2正则化就是将Wj向量中的每个值都分别除以Wj的模，从而得到新的Wj，新的Wj的模就是1</p>\n<p><img src=\"/images/paper/arc3.png\" width=\"640\"></p>\n<p>然后一方面对输入xi也用L2正则化处理，同时再乘以一个scale参数s；另一方面将cos(θyi)用cos(θyi+m)替代</p>\n<p><img src=\"/images/paper/arc4.png\" width=\"640\"></p>\n<h1 id=\"几何解释\"><a href=\"#几何解释\" class=\"headerlink\" title=\"几何解释\"></a>几何解释</h1><p>使用二分类来证明，决策边界就是在这个边界上属于正类和属于负类的概率一样的，所以可以得到以下这个表格和图像<br><img src=\"/images/paper/arc5.png\" width=\"640\"></p>\n<p><img src=\"/images/paper/arc6.png\" width=\"640\"></p>\n<p>一些实验</p>\n<p><img src=\"/images/paper/arc7.png\" width=\"640\"></p>\n"},{"title":"K近邻法","date":"2018-11-01T09:04:17.000Z","_content":"# k近邻法 #\n给定一个训练数据集，对新输入的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该输入实例归入到这个类\n## k值的选择 ##\n如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，‘学习’的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是‘学习’的估计误差会变大，预测结果对近邻的实例点非常敏感，如果紧邻点正好是噪声，预测就会出错。换句话说，k的减小意味着整体模型变复杂，容易发生过拟合\n\n如果选择较大的k值，学习估计误差变小，学习近似误差增大，这时与输入实例较远的训练实例也会起预测作用。k值变大意味着模型变得简单\n# k近邻法的实现：kd树 #\n**常规的k-d tree的构建过程为：循环依序取数据点的各维度来作为切分维度，取数据点在该维度的中值作为切分超平面，将中值左侧的数据点挂在其左子树，将中值右侧的数据点挂在其右子树。递归处理其子树，直至所有数据点挂载完毕。**\n<img src=\"/images/kd_example.png\" width=\"640\"/>\nk-d树上的最邻近查找算法\n\n　　在k-d树中进行数据的查找也是特征匹配的重要环节，其目的是检索在k-d树中与查询点距离最近的数据点。这里先以一个简单的实例来描述最邻近查找的基本思路。\n\n　　星号表示要查询的点（2.1,3.1）。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点（2,3）。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行'回溯'操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从（7,2）点开始进行二叉查找，然后到达（5,4），最后到达（2,3），此时搜索路径中的节点为<（7,2），（5,4），（2,3）>，首先以（2,3）作为当前最近邻点，计算其到查询点（2.1,3.1）的距离为0.1414，然后回溯到其父节点（5,4），并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以（2.1,3.1）为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入（5,4）节点右子空间中去搜索。\n<img src=\"/images/kd_1.png\"/>\n回溯到（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。\n\n　　一个复杂点了例子如查找点为（2，4.5）。同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径<（7,2），（5,4），（4,7）>，取（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到（5,4），计算其与查找点之间的距离为3.041。以（2，4.5）为圆心，以3.041为半径作圆，如图5所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找。此时需将（2,3）节点加入搜索路径中得<（7,2），（2,3）>。回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5。回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图6所示。至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。\n<img src=\"/images/kd_2.png\"/>\n\n<img src=\"/images/kd_3.png\"/>\n\n<font color='red'>如果实例点是随机分布的，kd树搜索的平均计算复杂度是O（log N），这里N是训练实数，kd树更适用于训练实例数大于空间维数的k近邻搜索，当空间维数接近训练实例时，它的效率会迅速下降，几乎接近线性扫描</font>\n\n**参考博客**：\n\nhttps://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html\n","source":"_posts/K近邻法.md","raw":"---\ntitle: K近邻法\ncategories:\n  - 机器学习\ndate: 2018-11-01 17:04:17\ntags:\n---\n# k近邻法 #\n给定一个训练数据集，对新输入的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该输入实例归入到这个类\n## k值的选择 ##\n如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，‘学习’的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是‘学习’的估计误差会变大，预测结果对近邻的实例点非常敏感，如果紧邻点正好是噪声，预测就会出错。换句话说，k的减小意味着整体模型变复杂，容易发生过拟合\n\n如果选择较大的k值，学习估计误差变小，学习近似误差增大，这时与输入实例较远的训练实例也会起预测作用。k值变大意味着模型变得简单\n# k近邻法的实现：kd树 #\n**常规的k-d tree的构建过程为：循环依序取数据点的各维度来作为切分维度，取数据点在该维度的中值作为切分超平面，将中值左侧的数据点挂在其左子树，将中值右侧的数据点挂在其右子树。递归处理其子树，直至所有数据点挂载完毕。**\n<img src=\"/images/kd_example.png\" width=\"640\"/>\nk-d树上的最邻近查找算法\n\n　　在k-d树中进行数据的查找也是特征匹配的重要环节，其目的是检索在k-d树中与查询点距离最近的数据点。这里先以一个简单的实例来描述最邻近查找的基本思路。\n\n　　星号表示要查询的点（2.1,3.1）。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点（2,3）。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行'回溯'操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从（7,2）点开始进行二叉查找，然后到达（5,4），最后到达（2,3），此时搜索路径中的节点为<（7,2），（5,4），（2,3）>，首先以（2,3）作为当前最近邻点，计算其到查询点（2.1,3.1）的距离为0.1414，然后回溯到其父节点（5,4），并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以（2.1,3.1）为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入（5,4）节点右子空间中去搜索。\n<img src=\"/images/kd_1.png\"/>\n回溯到（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。\n\n　　一个复杂点了例子如查找点为（2，4.5）。同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径<（7,2），（5,4），（4,7）>，取（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到（5,4），计算其与查找点之间的距离为3.041。以（2，4.5）为圆心，以3.041为半径作圆，如图5所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找。此时需将（2,3）节点加入搜索路径中得<（7,2），（2,3）>。回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5。回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图6所示。至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。\n<img src=\"/images/kd_2.png\"/>\n\n<img src=\"/images/kd_3.png\"/>\n\n<font color='red'>如果实例点是随机分布的，kd树搜索的平均计算复杂度是O（log N），这里N是训练实数，kd树更适用于训练实例数大于空间维数的k近邻搜索，当空间维数接近训练实例时，它的效率会迅速下降，几乎接近线性扫描</font>\n\n**参考博客**：\n\nhttps://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html\n","slug":"K近邻法","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwn10001m4v5w6mtp4zq","content":"<h1 id=\"k近邻法\"><a href=\"#k近邻法\" class=\"headerlink\" title=\"k近邻法\"></a>k近邻法</h1><p>给定一个训练数据集，对新输入的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该输入实例归入到这个类</p>\n<h2 id=\"k值的选择\"><a href=\"#k值的选择\" class=\"headerlink\" title=\"k值的选择\"></a>k值的选择</h2><p>如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，‘学习’的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是‘学习’的估计误差会变大，预测结果对近邻的实例点非常敏感，如果紧邻点正好是噪声，预测就会出错。换句话说，k的减小意味着整体模型变复杂，容易发生过拟合</p>\n<p>如果选择较大的k值，学习估计误差变小，学习近似误差增大，这时与输入实例较远的训练实例也会起预测作用。k值变大意味着模型变得简单</p>\n<h1 id=\"k近邻法的实现：kd树\"><a href=\"#k近邻法的实现：kd树\" class=\"headerlink\" title=\"k近邻法的实现：kd树\"></a>k近邻法的实现：kd树</h1><p><strong>常规的k-d tree的构建过程为：循环依序取数据点的各维度来作为切分维度，取数据点在该维度的中值作为切分超平面，将中值左侧的数据点挂在其左子树，将中值右侧的数据点挂在其右子树。递归处理其子树，直至所有数据点挂载完毕。</strong><br><img src=\"/images/kd_example.png\" width=\"640\"><br>k-d树上的最邻近查找算法</p>\n<p>　　在k-d树中进行数据的查找也是特征匹配的重要环节，其目的是检索在k-d树中与查询点距离最近的数据点。这里先以一个简单的实例来描述最邻近查找的基本思路。</p>\n<p>　　星号表示要查询的点（2.1,3.1）。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点（2,3）。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行’回溯’操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从（7,2）点开始进行二叉查找，然后到达（5,4），最后到达（2,3），此时搜索路径中的节点为&lt;（7,2），（5,4），（2,3）&gt;，首先以（2,3）作为当前最近邻点，计算其到查询点（2.1,3.1）的距离为0.1414，然后回溯到其父节点（5,4），并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以（2.1,3.1）为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入（5,4）节点右子空间中去搜索。<br><img src=\"/images/kd_1.png\"><br>回溯到（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。</p>\n<p>　　一个复杂点了例子如查找点为（2，4.5）。同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;（7,2），（5,4），（4,7）&gt;，取（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到（5,4），计算其与查找点之间的距离为3.041。以（2，4.5）为圆心，以3.041为半径作圆，如图5所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找。此时需将（2,3）节点加入搜索路径中得&lt;（7,2），（2,3）&gt;。回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5。回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图6所示。至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。<br><img src=\"/images/kd_2.png\"></p>\n<p><img src=\"/images/kd_3.png\"></p>\n<font color=\"red\">如果实例点是随机分布的，kd树搜索的平均计算复杂度是O（log N），这里N是训练实数，kd树更适用于训练实例数大于空间维数的k近邻搜索，当空间维数接近训练实例时，它的效率会迅速下降，几乎接近线性扫描</font>\n\n<p><strong>参考博客</strong>：</p>\n<p><a href=\"https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html\" target=\"_blank\" rel=\"noopener\">https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"k近邻法\"><a href=\"#k近邻法\" class=\"headerlink\" title=\"k近邻法\"></a>k近邻法</h1><p>给定一个训练数据集，对新输入的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该输入实例归入到这个类</p>\n<h2 id=\"k值的选择\"><a href=\"#k值的选择\" class=\"headerlink\" title=\"k值的选择\"></a>k值的选择</h2><p>如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，‘学习’的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是‘学习’的估计误差会变大，预测结果对近邻的实例点非常敏感，如果紧邻点正好是噪声，预测就会出错。换句话说，k的减小意味着整体模型变复杂，容易发生过拟合</p>\n<p>如果选择较大的k值，学习估计误差变小，学习近似误差增大，这时与输入实例较远的训练实例也会起预测作用。k值变大意味着模型变得简单</p>\n<h1 id=\"k近邻法的实现：kd树\"><a href=\"#k近邻法的实现：kd树\" class=\"headerlink\" title=\"k近邻法的实现：kd树\"></a>k近邻法的实现：kd树</h1><p><strong>常规的k-d tree的构建过程为：循环依序取数据点的各维度来作为切分维度，取数据点在该维度的中值作为切分超平面，将中值左侧的数据点挂在其左子树，将中值右侧的数据点挂在其右子树。递归处理其子树，直至所有数据点挂载完毕。</strong><br><img src=\"/images/kd_example.png\" width=\"640\"><br>k-d树上的最邻近查找算法</p>\n<p>　　在k-d树中进行数据的查找也是特征匹配的重要环节，其目的是检索在k-d树中与查询点距离最近的数据点。这里先以一个简单的实例来描述最邻近查找的基本思路。</p>\n<p>　　星号表示要查询的点（2.1,3.1）。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点（2,3）。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行’回溯’操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从（7,2）点开始进行二叉查找，然后到达（5,4），最后到达（2,3），此时搜索路径中的节点为&lt;（7,2），（5,4），（2,3）&gt;，首先以（2,3）作为当前最近邻点，计算其到查询点（2.1,3.1）的距离为0.1414，然后回溯到其父节点（5,4），并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以（2.1,3.1）为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入（5,4）节点右子空间中去搜索。<br><img src=\"/images/kd_1.png\"><br>回溯到（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。</p>\n<p>　　一个复杂点了例子如查找点为（2，4.5）。同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;（7,2），（5,4），（4,7）&gt;，取（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到（5,4），计算其与查找点之间的距离为3.041。以（2，4.5）为圆心，以3.041为半径作圆，如图5所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找。此时需将（2,3）节点加入搜索路径中得&lt;（7,2），（2,3）&gt;。回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5。回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图6所示。至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。<br><img src=\"/images/kd_2.png\"></p>\n<p><img src=\"/images/kd_3.png\"></p>\n<font color=\"red\">如果实例点是随机分布的，kd树搜索的平均计算复杂度是O（log N），这里N是训练实数，kd树更适用于训练实例数大于空间维数的k近邻搜索，当空间维数接近训练实例时，它的效率会迅速下降，几乎接近线性扫描</font>\n\n<p><strong>参考博客</strong>：</p>\n<p><a href=\"https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html\" target=\"_blank\" rel=\"noopener\">https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html</a></p>\n"},{"title":"LightGBM: A Highly Efficient Gradient Boosting Decision Tree（NIPS 2017）","date":"2018-11-14T13:09:19.000Z","_content":"<font color='red'>无敌超级详细版</font>\n# 先验知识 #\n## 决策树 ##\n### 基本流程 ###\n决策树是一种常见的机器学习算法。其基本思想很是简单，如下图所示\n<img src=\"/images/paper/lgb01.png\" width=640/>\n我们判断一个西瓜是否是好瓜，根据生活经验，我们首先判断色泽是什么？判断色泽完后再判断根蒂是否蜷缩？然后再判断敲声是否浊响，最后我们根据这些特征的取值一步一步走到最后的叶子节点，判断它是否是好瓜\n\n其决策树的基本算法如下图所示\n<img src=\"/images/paper/lgb02.png\" width=640/>\n\n显然决策树的生成是一个递归过程，在决策树算法中有三种情况会导致递归返回\n1. 当前节点包含的样本完全属于同一类别，无需划分\n2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分\n3. 当前节点包含的样本集合为空，无法划分\n\n对于第2种情况，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别\n\n对于第3种情况，同样把当前节点标记为叶节点，但其类别设定为其父节点所含样本最多的类别\n### 划分选择 ###\n我们从上图4.2算法的第8行得知，我们需要从属性集中，寻找最优划分属性，那么如何寻找最优划分属性呢？一般而言随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的‘纯度’越来越高，基于这个思想我们可以根据每个属性划分的信息增益、信息增益率、基尼指数等属性的互相比较，来选出最优划分属性，详细属性划分选择可以观看<a href='https://sumenpuyuan.github.io/2018/10/15/%E5%86%B3%E7%AD%96%E6%A0%91/'><font color='red'>这里</font></a>，点击目录 右侧划分选择 即可\n### 连续值处理 ###\n之前我们的属性值都是离散的，但是现实生活中很多属性值都是连续的，接下来讨论属性是连续值的解决方法，**采用二分法对连续属性进行处理，这正是C4.5决策树算法采用的机制**\n\n假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\\\({a^1,a^2……，a^n}\\\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合\n$$ T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}$$\n我们取\\\\(a^i,a^{i+1}\\\\)的中位点\\\\(\\frac{a^i+a^{i+1}}{2}\\\\)作为候选划分点，然后就可以像离散值一样来考察这些划分点。\n### 回归树的生成 ###\n<font color='red'>这里要重点看！！！,因为最原始的gbdt是基于回归树来说明的</font>，我们用CART算法来说明一颗回归树的生成过程，这是回归树算法流程\n<img src=\"/images/paper/lgb03.png\" width=640/>\n直接看算法不太容易看懂，我们来举一个例子进行理解\n\n假设我们有训练数据如下，目标是得到一颗最小二乘回归树\n<img src=\"/images/paper/lgb04.png\" />\n\n1. 选择最优划分属性j与划分点s\n因为只有一个属性x，因此最优切分变量就是x\n\n接下来考虑9个切分点[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5],损失函数定义为\\\\(Loss=(y,f(x))=(f(x)-y)^2\\\\),将9个切分点一一代入下个式子，其中\\\\(c_m=ave(y_i|x_i \\in R_m)\\\\)(即我们通过切割点将数据集分为两部分，每部分我们的预测值就是这一部分所有样本取值的平均值)\n$$ \\min_{j,s}[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)}(y_i -c_1)^2+\\min_{c_2}\\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2] $$\n例如我们取s=1.5，R1={1},R2={2,3,4,5,6,7,8,9,10},这两个区域的输出值分别是\nc1=5.56，c2=1/9（5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05）=7.50，得到下表\n<img src=\"/images/paper/lgb05.png\" />\n将以上各个c1 c2带入平方误差损失函数可得下表\n<img src=\"/images/paper/lgb06.png\" />\n显然s=6.5是，m(s)最小，因此第一个划分变量是j=x，s=6.5\n\n用选定的（j,s）对数据进行划分，并决定输出值。目前我们分的两个区域分别是R1={1,2,3,4,5,6}，R2={7,8,9,10}输出值\\\\(c_m=ave(y_i|x_i \\in R_m)\\\\)，c1=6.24，c2=8.91，对两个子区域继续重复上述步骤，直到满足停止条件（如限制深度或叶子结点个数）\n<font color='red'>可以看出我们每次划分最优属性与最优切割点，都需要遍历所有属性，所有切割点，所有训练样本，这一步骤是很耗时的</font>\n## 集成学习 ##\n### Adaboost ###\n### Boosting Tree###\n### GBDT  ###\n# 论文开始 #\n\n","source":"_posts/LightGBM-A-Highly-Efficient-Gradient-Boosting-Decision-Tree（NIPS-2017）.md","raw":"---\ntitle: 'LightGBM: A Highly Efficient Gradient Boosting Decision Tree（NIPS 2017）'\ncategories:\n  - 论文阅读\ndate: 2018-11-14 21:09:19\ntags:\n---\n<font color='red'>无敌超级详细版</font>\n# 先验知识 #\n## 决策树 ##\n### 基本流程 ###\n决策树是一种常见的机器学习算法。其基本思想很是简单，如下图所示\n<img src=\"/images/paper/lgb01.png\" width=640/>\n我们判断一个西瓜是否是好瓜，根据生活经验，我们首先判断色泽是什么？判断色泽完后再判断根蒂是否蜷缩？然后再判断敲声是否浊响，最后我们根据这些特征的取值一步一步走到最后的叶子节点，判断它是否是好瓜\n\n其决策树的基本算法如下图所示\n<img src=\"/images/paper/lgb02.png\" width=640/>\n\n显然决策树的生成是一个递归过程，在决策树算法中有三种情况会导致递归返回\n1. 当前节点包含的样本完全属于同一类别，无需划分\n2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分\n3. 当前节点包含的样本集合为空，无法划分\n\n对于第2种情况，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别\n\n对于第3种情况，同样把当前节点标记为叶节点，但其类别设定为其父节点所含样本最多的类别\n### 划分选择 ###\n我们从上图4.2算法的第8行得知，我们需要从属性集中，寻找最优划分属性，那么如何寻找最优划分属性呢？一般而言随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的‘纯度’越来越高，基于这个思想我们可以根据每个属性划分的信息增益、信息增益率、基尼指数等属性的互相比较，来选出最优划分属性，详细属性划分选择可以观看<a href='https://sumenpuyuan.github.io/2018/10/15/%E5%86%B3%E7%AD%96%E6%A0%91/'><font color='red'>这里</font></a>，点击目录 右侧划分选择 即可\n### 连续值处理 ###\n之前我们的属性值都是离散的，但是现实生活中很多属性值都是连续的，接下来讨论属性是连续值的解决方法，**采用二分法对连续属性进行处理，这正是C4.5决策树算法采用的机制**\n\n假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\\\({a^1,a^2……，a^n}\\\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合\n$$ T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}$$\n我们取\\\\(a^i,a^{i+1}\\\\)的中位点\\\\(\\frac{a^i+a^{i+1}}{2}\\\\)作为候选划分点，然后就可以像离散值一样来考察这些划分点。\n### 回归树的生成 ###\n<font color='red'>这里要重点看！！！,因为最原始的gbdt是基于回归树来说明的</font>，我们用CART算法来说明一颗回归树的生成过程，这是回归树算法流程\n<img src=\"/images/paper/lgb03.png\" width=640/>\n直接看算法不太容易看懂，我们来举一个例子进行理解\n\n假设我们有训练数据如下，目标是得到一颗最小二乘回归树\n<img src=\"/images/paper/lgb04.png\" />\n\n1. 选择最优划分属性j与划分点s\n因为只有一个属性x，因此最优切分变量就是x\n\n接下来考虑9个切分点[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5],损失函数定义为\\\\(Loss=(y,f(x))=(f(x)-y)^2\\\\),将9个切分点一一代入下个式子，其中\\\\(c_m=ave(y_i|x_i \\in R_m)\\\\)(即我们通过切割点将数据集分为两部分，每部分我们的预测值就是这一部分所有样本取值的平均值)\n$$ \\min_{j,s}[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)}(y_i -c_1)^2+\\min_{c_2}\\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2] $$\n例如我们取s=1.5，R1={1},R2={2,3,4,5,6,7,8,9,10},这两个区域的输出值分别是\nc1=5.56，c2=1/9（5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05）=7.50，得到下表\n<img src=\"/images/paper/lgb05.png\" />\n将以上各个c1 c2带入平方误差损失函数可得下表\n<img src=\"/images/paper/lgb06.png\" />\n显然s=6.5是，m(s)最小，因此第一个划分变量是j=x，s=6.5\n\n用选定的（j,s）对数据进行划分，并决定输出值。目前我们分的两个区域分别是R1={1,2,3,4,5,6}，R2={7,8,9,10}输出值\\\\(c_m=ave(y_i|x_i \\in R_m)\\\\)，c1=6.24，c2=8.91，对两个子区域继续重复上述步骤，直到满足停止条件（如限制深度或叶子结点个数）\n<font color='red'>可以看出我们每次划分最优属性与最优切割点，都需要遍历所有属性，所有切割点，所有训练样本，这一步骤是很耗时的</font>\n## 集成学习 ##\n### Adaboost ###\n### Boosting Tree###\n### GBDT  ###\n# 论文开始 #\n\n","slug":"LightGBM-A-Highly-Efficient-Gradient-Boosting-Decision-Tree（NIPS-2017）","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwn80003m4v53u5kne91","content":"<p><font color=\"red\">无敌超级详细版</font></p>\n<h1 id=\"先验知识\"><a href=\"#先验知识\" class=\"headerlink\" title=\"先验知识\"></a>先验知识</h1><h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><h3 id=\"基本流程\"><a href=\"#基本流程\" class=\"headerlink\" title=\"基本流程\"></a>基本流程</h3><p>决策树是一种常见的机器学习算法。其基本思想很是简单，如下图所示<br><img src=\"/images/paper/lgb01.png\" width=\"640/\"><br>我们判断一个西瓜是否是好瓜，根据生活经验，我们首先判断色泽是什么？判断色泽完后再判断根蒂是否蜷缩？然后再判断敲声是否浊响，最后我们根据这些特征的取值一步一步走到最后的叶子节点，判断它是否是好瓜</p>\n<p>其决策树的基本算法如下图所示<br><img src=\"/images/paper/lgb02.png\" width=\"640/\"></p>\n<p>显然决策树的生成是一个递归过程，在决策树算法中有三种情况会导致递归返回</p>\n<ol>\n<li>当前节点包含的样本完全属于同一类别，无需划分</li>\n<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分</li>\n<li>当前节点包含的样本集合为空，无法划分</li>\n</ol>\n<p>对于第2种情况，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别</p>\n<p>对于第3种情况，同样把当前节点标记为叶节点，但其类别设定为其父节点所含样本最多的类别</p>\n<h3 id=\"划分选择\"><a href=\"#划分选择\" class=\"headerlink\" title=\"划分选择\"></a>划分选择</h3><p>我们从上图4.2算法的第8行得知，我们需要从属性集中，寻找最优划分属性，那么如何寻找最优划分属性呢？一般而言随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的‘纯度’越来越高，基于这个思想我们可以根据每个属性划分的信息增益、信息增益率、基尼指数等属性的互相比较，来选出最优划分属性，详细属性划分选择可以观看<a href=\"https://sumenpuyuan.github.io/2018/10/15/%E5%86%B3%E7%AD%96%E6%A0%91/\" target=\"_blank\" rel=\"noopener\"><font color=\"red\">这里</font></a>，点击目录 右侧划分选择 即可</p>\n<h3 id=\"连续值处理\"><a href=\"#连续值处理\" class=\"headerlink\" title=\"连续值处理\"></a>连续值处理</h3><p>之前我们的属性值都是离散的，但是现实生活中很多属性值都是连续的，接下来讨论属性是连续值的解决方法，<strong>采用二分法对连续属性进行处理，这正是C4.5决策树算法采用的机制</strong></p>\n<p>假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color=\"red\">将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合</p>\n<script type=\"math/tex; mode=display\">T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}</script><p>我们取\\(a^i,a^{i+1}\\)的中位点\\(\\frac{a^i+a^{i+1}}{2}\\)作为候选划分点，然后就可以像离散值一样来考察这些划分点。</p>\n<h3 id=\"回归树的生成\"><a href=\"#回归树的生成\" class=\"headerlink\" title=\"回归树的生成\"></a>回归树的生成</h3><p><font color=\"red\">这里要重点看！！！,因为最原始的gbdt是基于回归树来说明的</font>，我们用CART算法来说明一颗回归树的生成过程，这是回归树算法流程<br><img src=\"/images/paper/lgb03.png\" width=\"640/\"><br>直接看算法不太容易看懂，我们来举一个例子进行理解</p>\n<p>假设我们有训练数据如下，目标是得到一颗最小二乘回归树<br><img src=\"/images/paper/lgb04.png\"></p>\n<ol>\n<li>选择最优划分属性j与划分点s<br>因为只有一个属性x，因此最优切分变量就是x</li>\n</ol>\n<p>接下来考虑9个切分点[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5],损失函数定义为\\(Loss=(y,f(x))=(f(x)-y)^2\\),将9个切分点一一代入下个式子，其中\\(c_m=ave(y_i|x_i \\in R_m)\\)(即我们通过切割点将数据集分为两部分，每部分我们的预测值就是这一部分所有样本取值的平均值)</p>\n<script type=\"math/tex; mode=display\">\\min_{j,s}[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)}(y_i -c_1)^2+\\min_{c_2}\\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2]</script><p>例如我们取s=1.5，R1={1},R2={2,3,4,5,6,7,8,9,10},这两个区域的输出值分别是<br>c1=5.56，c2=1/9（5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05）=7.50，得到下表<br><img src=\"/images/paper/lgb05.png\"><br>将以上各个c1 c2带入平方误差损失函数可得下表<br><img src=\"/images/paper/lgb06.png\"><br>显然s=6.5是，m(s)最小，因此第一个划分变量是j=x，s=6.5</p>\n<p>用选定的（j,s）对数据进行划分，并决定输出值。目前我们分的两个区域分别是R1={1,2,3,4,5,6}，R2={7,8,9,10}输出值\\(c_m=ave(y_i|x_i \\in R_m)\\)，c1=6.24，c2=8.91，对两个子区域继续重复上述步骤，直到满足停止条件（如限制深度或叶子结点个数）</p>\n<p><font color=\"red\">可以看出我们每次划分最优属性与最优切割点，都需要遍历所有属性，所有切割点，所有训练样本，这一步骤是很耗时的</font></p>\n<h2 id=\"集成学习\"><a href=\"#集成学习\" class=\"headerlink\" title=\"集成学习\"></a>集成学习</h2><h3 id=\"Adaboost\"><a href=\"#Adaboost\" class=\"headerlink\" title=\"Adaboost\"></a>Adaboost</h3><h3 id=\"Boosting-Tree\"><a href=\"#Boosting-Tree\" class=\"headerlink\" title=\"Boosting Tree\"></a>Boosting Tree</h3><h3 id=\"GBDT\"><a href=\"#GBDT\" class=\"headerlink\" title=\"GBDT\"></a>GBDT</h3><h1 id=\"论文开始\"><a href=\"#论文开始\" class=\"headerlink\" title=\"论文开始\"></a>论文开始</h1>","site":{"data":{}},"excerpt":"","more":"<p><font color=\"red\">无敌超级详细版</font></p>\n<h1 id=\"先验知识\"><a href=\"#先验知识\" class=\"headerlink\" title=\"先验知识\"></a>先验知识</h1><h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><h3 id=\"基本流程\"><a href=\"#基本流程\" class=\"headerlink\" title=\"基本流程\"></a>基本流程</h3><p>决策树是一种常见的机器学习算法。其基本思想很是简单，如下图所示<br><img src=\"/images/paper/lgb01.png\" width=\"640/\"><br>我们判断一个西瓜是否是好瓜，根据生活经验，我们首先判断色泽是什么？判断色泽完后再判断根蒂是否蜷缩？然后再判断敲声是否浊响，最后我们根据这些特征的取值一步一步走到最后的叶子节点，判断它是否是好瓜</p>\n<p>其决策树的基本算法如下图所示<br><img src=\"/images/paper/lgb02.png\" width=\"640/\"></p>\n<p>显然决策树的生成是一个递归过程，在决策树算法中有三种情况会导致递归返回</p>\n<ol>\n<li>当前节点包含的样本完全属于同一类别，无需划分</li>\n<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分</li>\n<li>当前节点包含的样本集合为空，无法划分</li>\n</ol>\n<p>对于第2种情况，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别</p>\n<p>对于第3种情况，同样把当前节点标记为叶节点，但其类别设定为其父节点所含样本最多的类别</p>\n<h3 id=\"划分选择\"><a href=\"#划分选择\" class=\"headerlink\" title=\"划分选择\"></a>划分选择</h3><p>我们从上图4.2算法的第8行得知，我们需要从属性集中，寻找最优划分属性，那么如何寻找最优划分属性呢？一般而言随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的‘纯度’越来越高，基于这个思想我们可以根据每个属性划分的信息增益、信息增益率、基尼指数等属性的互相比较，来选出最优划分属性，详细属性划分选择可以观看<a href=\"https://sumenpuyuan.github.io/2018/10/15/%E5%86%B3%E7%AD%96%E6%A0%91/\" target=\"_blank\" rel=\"noopener\"><font color=\"red\">这里</font></a>，点击目录 右侧划分选择 即可</p>\n<h3 id=\"连续值处理\"><a href=\"#连续值处理\" class=\"headerlink\" title=\"连续值处理\"></a>连续值处理</h3><p>之前我们的属性值都是离散的，但是现实生活中很多属性值都是连续的，接下来讨论属性是连续值的解决方法，<strong>采用二分法对连续属性进行处理，这正是C4.5决策树算法采用的机制</strong></p>\n<p>假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color=\"red\">将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合</p>\n<script type=\"math/tex; mode=display\">T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}</script><p>我们取\\(a^i,a^{i+1}\\)的中位点\\(\\frac{a^i+a^{i+1}}{2}\\)作为候选划分点，然后就可以像离散值一样来考察这些划分点。</p>\n<h3 id=\"回归树的生成\"><a href=\"#回归树的生成\" class=\"headerlink\" title=\"回归树的生成\"></a>回归树的生成</h3><p><font color=\"red\">这里要重点看！！！,因为最原始的gbdt是基于回归树来说明的</font>，我们用CART算法来说明一颗回归树的生成过程，这是回归树算法流程<br><img src=\"/images/paper/lgb03.png\" width=\"640/\"><br>直接看算法不太容易看懂，我们来举一个例子进行理解</p>\n<p>假设我们有训练数据如下，目标是得到一颗最小二乘回归树<br><img src=\"/images/paper/lgb04.png\"></p>\n<ol>\n<li>选择最优划分属性j与划分点s<br>因为只有一个属性x，因此最优切分变量就是x</li>\n</ol>\n<p>接下来考虑9个切分点[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5],损失函数定义为\\(Loss=(y,f(x))=(f(x)-y)^2\\),将9个切分点一一代入下个式子，其中\\(c_m=ave(y_i|x_i \\in R_m)\\)(即我们通过切割点将数据集分为两部分，每部分我们的预测值就是这一部分所有样本取值的平均值)</p>\n<script type=\"math/tex; mode=display\">\\min_{j,s}[\\min_{c_1} \\sum_{x_i \\in R_1(j,s)}(y_i -c_1)^2+\\min_{c_2}\\sum_{x_i \\in R_2(j,s)}(y_i-c_2)^2]</script><p>例如我们取s=1.5，R1={1},R2={2,3,4,5,6,7,8,9,10},这两个区域的输出值分别是<br>c1=5.56，c2=1/9（5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05）=7.50，得到下表<br><img src=\"/images/paper/lgb05.png\"><br>将以上各个c1 c2带入平方误差损失函数可得下表<br><img src=\"/images/paper/lgb06.png\"><br>显然s=6.5是，m(s)最小，因此第一个划分变量是j=x，s=6.5</p>\n<p>用选定的（j,s）对数据进行划分，并决定输出值。目前我们分的两个区域分别是R1={1,2,3,4,5,6}，R2={7,8,9,10}输出值\\(c_m=ave(y_i|x_i \\in R_m)\\)，c1=6.24，c2=8.91，对两个子区域继续重复上述步骤，直到满足停止条件（如限制深度或叶子结点个数）</p>\n<p><font color=\"red\">可以看出我们每次划分最优属性与最优切割点，都需要遍历所有属性，所有切割点，所有训练样本，这一步骤是很耗时的</font></p>\n<h2 id=\"集成学习\"><a href=\"#集成学习\" class=\"headerlink\" title=\"集成学习\"></a>集成学习</h2><h3 id=\"Adaboost\"><a href=\"#Adaboost\" class=\"headerlink\" title=\"Adaboost\"></a>Adaboost</h3><h3 id=\"Boosting-Tree\"><a href=\"#Boosting-Tree\" class=\"headerlink\" title=\"Boosting Tree\"></a>Boosting Tree</h3><h3 id=\"GBDT\"><a href=\"#GBDT\" class=\"headerlink\" title=\"GBDT\"></a>GBDT</h3><h1 id=\"论文开始\"><a href=\"#论文开始\" class=\"headerlink\" title=\"论文开始\"></a>论文开始</h1>"},{"title":"ImageNet Classification with Deep Convolutional Neural Networks（Alexnet）","date":"2018-11-09T13:02:33.000Z","_content":"ImageNet Classification with Deep Convolutional Neural Networks（Alexnet）\n#1.创新的地方#\n（1）使用ReLu函数作为CNN的激活函数\n\n（2）使用dropout防止过拟合\n\n（3）使用重叠最大池化\n\n（4）提出了LRN层\n\n#2.摘要#\n（1）提出了一个深度卷积网络，在ImageNet LSVRC2010，top1和top5错误分别是37.5%、17.0%远远好于之前的表现\n\n（2）神经网络有6000万个参数，650000神经元，5个卷积层（某些卷积层后面有池化层）和三个全连接层\n\n（3）为减少全连接层的过拟合，使用了一个dropout的正则化方法\n\n#3.图片预处理#\n（1）大小归一化\n将图像进行下采样到固定的256*256分辨率，给定一个矩形图像，首先缩放图像短边长度为256，然后从结果图像中裁剪中心的256*256大小的图像块\n（2）减去像素平均值\n所有图片的每个像素值都减去所有训练集图片的平均值\n#3.架构#\n##3.1ReLU非线性##\n训练速度比之前的sigmod和tanh快，并解决了之前激活函数的梯度消失问题，（详见5ReLU函数）\n##3.2局部响应归一化##\n提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。（ps，博客上都说他其实没什么用）\n##3.3重叠池化##\n在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。Alexnet设置s=2，z=3，这个方案分别降低了top-1 0.4%，top5 0.3%的错误率。且论文提到重叠池化的模型更难过拟合。\n##3.4整体架构##\n8个带权重的层，前5层是卷积层，剩下3层是全连接层。最后一层是1000维softmax输入\n\n<img src=\"/images/paper/alex1.png\" width=\"640\"/>\n\n<img src=\"/images/paper/alex2.png\" width=\"640\"/>\n##3.5减少过拟合##\n###3.5.1.数据增强###\n\n方法一：水平翻转和图像变换（随机裁剪）\n\n训练时：随机裁剪224*224的图片和进行水平翻转，所以数据增大了（250-224）平方*2=2048倍\n\n在测试时，网络通过提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。\n\n方法二：PCA Jittering（详见7 PCA Jittering）（减少了TOP1错误率 1%以上）\n\n###3.5.2.dropout###\n\n训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。（也有论文，单独看下）\n朴素思想：以0.5的概率让隐藏层的输出为0，失活的神经元不再进行前向传播且不参与反向传播\n\nalexnet：前两个全连接层使用dropout，会减少过拟合，但会使训练迭代次数翻一倍\n\n#4.学习细节#\n随机梯度下降来训练模型，batch size为128，动量是0.9（详见6.动量），权重衰减为0.0005\n<img src=\"/images/paper/alex3.png\" width=\"640\"/>\n使用均值是0，标准差是0.01的高斯分布对每一层的权重进行初始化。2,4,5卷积层和全连接层偏置初始化为1，其余层偏置初始化为0\n\n\n#5.ReLu函数#\nAlexNet成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。而且训练速度也比之前的sigmod和tanh快。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。\n##5.1训练速度##\n使用ReLu的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍\n<img src='/images/paper/alex4.png' width=\"640\"/>\n##5.2梯度消失和梯度爆炸##\n<img src=\"/images/paper/alex5.jpg\" width='640'/>\n如上图所示的网络结构，我们在进行反向传播更新w1权值的时候，公式如下\n<img src='/images/paper/alex6.jpg' width='640'/>\n\nsigmod函数导数如下图所示\n<img src=\"/images/paper/alex7.png\" width='640'/>\n从图中可以看出最大值是0.25，所以存在一下两种现象\n<img src=\"/images/paper/alex8.jpg\" width=\"640\"/>\n##5.3 ReLu函数##\n\nrelu函数解决问题的方式很简单，如果偏导数是1的话就不存在爆炸与消失的问题了，relu函数图像如下\n<img src=\"/images/paper/alex9.jpg\" width=\"640\"/>\n从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。\n#6.动量#\n在物理世界，球体运动是有惯性的，所以可能翻过山头达到全局最优解\n<img src=\"/images/paper/alex10.png\" width=\"640\"/>\n把这种概念引入到梯度下降中去，每次更新时不仅考虑当前梯度影响，还要考虑上一步的梯度影响\n<img src=\"/images/paper/alex11.png\" width=\"640\"/>\n有可能会翻过去找到全局最优解\n<img src=\"/images/paper/alex12.png\" width=\"640\"/>\n#7 PCA Jittering#\n##7.1PCA主成分分析##\n主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。\n主要计算过程：\n假设有一数组\n<img src=\"/images/paper/alex13.jpg\" width=\"640\"/>\n第一步：分别求出x和y的平均值，对于每个样例减去均值得到\n<img src=\"/images/paper/alex14.jpg\" width=\"640\"/>\n第二步，求协方差矩阵得到\n这是3维的协方差形式\n<img src=\"/images/paper/alex15.jpg\" width=\"640\"/>\n\n协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。（这里的方差是修正样本方差，分母是 样本数量-1）\n<img src=\"/images/paper/alex16.jpg\" width=\"640\"/>\n协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。\n第三步：求协方差的特征值和特征向量\n<img src=\"/images/paper/alex17.png\" width=\"640\"/>\n<img src=\"/images/paper/alex18.jpg\" width=\"640\"/>\n在数学上，特别是线性代数总，对于一个给定的矩阵A，它的特征向量v进行经过这个线性变化后，得到新的向量仍然和原来的v保持在一条直线上，但其长度或方向或许会改变，即\n\nAv=λν\n\nλ是特征值，v是特征向量\n（1）如果矩阵可以变换成为对角矩阵，那么它的特征值就是他对角线上的元素，而特征向量就是相应的基：例如矩阵\n<img src=\"/images/paper/alex19.png\" width=\"640\"/>\n的特征值就是2和4,2对应的特征向量就是所有形同（a，b，0）T的非零向量，4对应的特征向量是所有形同（0,0，c）T的非零向量\n（2）对于一般的矩阵\n<img src=\"/images/paper/alex20.jpg\" width=\"640\"/>\n第四步：将特征值按照从大到小进行排序，选择其中最大的k个，然后将其对应的特征向量分别作为列向量组成特征向量矩阵\n第五步：将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为\nFinalData(10*1) = DataAdjust(10*2矩阵) x 特征向量(-0.677873399, -0.735178656)T\n最后结果是他\n<img src=\"/images/paper/alex21.jpg\" width=\"640\"/>\n这样就把原始的n维降到了k维","source":"_posts/ImageNet-Classification-with-Deep-Convolutional-Neural-Networks（Alexnet）.md","raw":"---\ntitle: ImageNet Classification with Deep Convolutional Neural Networks（Alexnet）\ncategories:\n  - 论文阅读\ndate: 2018-11-09 21:02:33\ntags:\n---\nImageNet Classification with Deep Convolutional Neural Networks（Alexnet）\n#1.创新的地方#\n（1）使用ReLu函数作为CNN的激活函数\n\n（2）使用dropout防止过拟合\n\n（3）使用重叠最大池化\n\n（4）提出了LRN层\n\n#2.摘要#\n（1）提出了一个深度卷积网络，在ImageNet LSVRC2010，top1和top5错误分别是37.5%、17.0%远远好于之前的表现\n\n（2）神经网络有6000万个参数，650000神经元，5个卷积层（某些卷积层后面有池化层）和三个全连接层\n\n（3）为减少全连接层的过拟合，使用了一个dropout的正则化方法\n\n#3.图片预处理#\n（1）大小归一化\n将图像进行下采样到固定的256*256分辨率，给定一个矩形图像，首先缩放图像短边长度为256，然后从结果图像中裁剪中心的256*256大小的图像块\n（2）减去像素平均值\n所有图片的每个像素值都减去所有训练集图片的平均值\n#3.架构#\n##3.1ReLU非线性##\n训练速度比之前的sigmod和tanh快，并解决了之前激活函数的梯度消失问题，（详见5ReLU函数）\n##3.2局部响应归一化##\n提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。（ps，博客上都说他其实没什么用）\n##3.3重叠池化##\n在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。Alexnet设置s=2，z=3，这个方案分别降低了top-1 0.4%，top5 0.3%的错误率。且论文提到重叠池化的模型更难过拟合。\n##3.4整体架构##\n8个带权重的层，前5层是卷积层，剩下3层是全连接层。最后一层是1000维softmax输入\n\n<img src=\"/images/paper/alex1.png\" width=\"640\"/>\n\n<img src=\"/images/paper/alex2.png\" width=\"640\"/>\n##3.5减少过拟合##\n###3.5.1.数据增强###\n\n方法一：水平翻转和图像变换（随机裁剪）\n\n训练时：随机裁剪224*224的图片和进行水平翻转，所以数据增大了（250-224）平方*2=2048倍\n\n在测试时，网络通过提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。\n\n方法二：PCA Jittering（详见7 PCA Jittering）（减少了TOP1错误率 1%以上）\n\n###3.5.2.dropout###\n\n训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。（也有论文，单独看下）\n朴素思想：以0.5的概率让隐藏层的输出为0，失活的神经元不再进行前向传播且不参与反向传播\n\nalexnet：前两个全连接层使用dropout，会减少过拟合，但会使训练迭代次数翻一倍\n\n#4.学习细节#\n随机梯度下降来训练模型，batch size为128，动量是0.9（详见6.动量），权重衰减为0.0005\n<img src=\"/images/paper/alex3.png\" width=\"640\"/>\n使用均值是0，标准差是0.01的高斯分布对每一层的权重进行初始化。2,4,5卷积层和全连接层偏置初始化为1，其余层偏置初始化为0\n\n\n#5.ReLu函数#\nAlexNet成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。而且训练速度也比之前的sigmod和tanh快。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。\n##5.1训练速度##\n使用ReLu的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍\n<img src='/images/paper/alex4.png' width=\"640\"/>\n##5.2梯度消失和梯度爆炸##\n<img src=\"/images/paper/alex5.jpg\" width='640'/>\n如上图所示的网络结构，我们在进行反向传播更新w1权值的时候，公式如下\n<img src='/images/paper/alex6.jpg' width='640'/>\n\nsigmod函数导数如下图所示\n<img src=\"/images/paper/alex7.png\" width='640'/>\n从图中可以看出最大值是0.25，所以存在一下两种现象\n<img src=\"/images/paper/alex8.jpg\" width=\"640\"/>\n##5.3 ReLu函数##\n\nrelu函数解决问题的方式很简单，如果偏导数是1的话就不存在爆炸与消失的问题了，relu函数图像如下\n<img src=\"/images/paper/alex9.jpg\" width=\"640\"/>\n从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。\n#6.动量#\n在物理世界，球体运动是有惯性的，所以可能翻过山头达到全局最优解\n<img src=\"/images/paper/alex10.png\" width=\"640\"/>\n把这种概念引入到梯度下降中去，每次更新时不仅考虑当前梯度影响，还要考虑上一步的梯度影响\n<img src=\"/images/paper/alex11.png\" width=\"640\"/>\n有可能会翻过去找到全局最优解\n<img src=\"/images/paper/alex12.png\" width=\"640\"/>\n#7 PCA Jittering#\n##7.1PCA主成分分析##\n主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。\n主要计算过程：\n假设有一数组\n<img src=\"/images/paper/alex13.jpg\" width=\"640\"/>\n第一步：分别求出x和y的平均值，对于每个样例减去均值得到\n<img src=\"/images/paper/alex14.jpg\" width=\"640\"/>\n第二步，求协方差矩阵得到\n这是3维的协方差形式\n<img src=\"/images/paper/alex15.jpg\" width=\"640\"/>\n\n协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。（这里的方差是修正样本方差，分母是 样本数量-1）\n<img src=\"/images/paper/alex16.jpg\" width=\"640\"/>\n协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。\n第三步：求协方差的特征值和特征向量\n<img src=\"/images/paper/alex17.png\" width=\"640\"/>\n<img src=\"/images/paper/alex18.jpg\" width=\"640\"/>\n在数学上，特别是线性代数总，对于一个给定的矩阵A，它的特征向量v进行经过这个线性变化后，得到新的向量仍然和原来的v保持在一条直线上，但其长度或方向或许会改变，即\n\nAv=λν\n\nλ是特征值，v是特征向量\n（1）如果矩阵可以变换成为对角矩阵，那么它的特征值就是他对角线上的元素，而特征向量就是相应的基：例如矩阵\n<img src=\"/images/paper/alex19.png\" width=\"640\"/>\n的特征值就是2和4,2对应的特征向量就是所有形同（a，b，0）T的非零向量，4对应的特征向量是所有形同（0,0，c）T的非零向量\n（2）对于一般的矩阵\n<img src=\"/images/paper/alex20.jpg\" width=\"640\"/>\n第四步：将特征值按照从大到小进行排序，选择其中最大的k个，然后将其对应的特征向量分别作为列向量组成特征向量矩阵\n第五步：将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为\nFinalData(10*1) = DataAdjust(10*2矩阵) x 特征向量(-0.677873399, -0.735178656)T\n最后结果是他\n<img src=\"/images/paper/alex21.jpg\" width=\"640\"/>\n这样就把原始的n维降到了k维","slug":"ImageNet-Classification-with-Deep-Convolutional-Neural-Networks（Alexnet）","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwna0004m4v52kbr676b","content":"<p>ImageNet Classification with Deep Convolutional Neural Networks（Alexnet）</p>\n<h1 id=\"1-创新的地方\"><a href=\"#1-创新的地方\" class=\"headerlink\" title=\"1.创新的地方\"></a>1.创新的地方</h1><p>（1）使用ReLu函数作为CNN的激活函数</p>\n<p>（2）使用dropout防止过拟合</p>\n<p>（3）使用重叠最大池化</p>\n<p>（4）提出了LRN层</p>\n<h1 id=\"2-摘要\"><a href=\"#2-摘要\" class=\"headerlink\" title=\"2.摘要\"></a>2.摘要</h1><p>（1）提出了一个深度卷积网络，在ImageNet LSVRC2010，top1和top5错误分别是37.5%、17.0%远远好于之前的表现</p>\n<p>（2）神经网络有6000万个参数，650000神经元，5个卷积层（某些卷积层后面有池化层）和三个全连接层</p>\n<p>（3）为减少全连接层的过拟合，使用了一个dropout的正则化方法</p>\n<h1 id=\"3-图片预处理\"><a href=\"#3-图片预处理\" class=\"headerlink\" title=\"3.图片预处理\"></a>3.图片预处理</h1><p>（1）大小归一化<br>将图像进行下采样到固定的256<em>256分辨率，给定一个矩形图像，首先缩放图像短边长度为256，然后从结果图像中裁剪中心的256</em>256大小的图像块<br>（2）减去像素平均值<br>所有图片的每个像素值都减去所有训练集图片的平均值</p>\n<h1 id=\"3-架构\"><a href=\"#3-架构\" class=\"headerlink\" title=\"3.架构\"></a>3.架构</h1><h2 id=\"3-1ReLU非线性\"><a href=\"#3-1ReLU非线性\" class=\"headerlink\" title=\"3.1ReLU非线性\"></a>3.1ReLU非线性</h2><p>训练速度比之前的sigmod和tanh快，并解决了之前激活函数的梯度消失问题，（详见5ReLU函数）</p>\n<h2 id=\"3-2局部响应归一化\"><a href=\"#3-2局部响应归一化\" class=\"headerlink\" title=\"3.2局部响应归一化\"></a>3.2局部响应归一化</h2><p>提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。（ps，博客上都说他其实没什么用）</p>\n<h2 id=\"3-3重叠池化\"><a href=\"#3-3重叠池化\" class=\"headerlink\" title=\"3.3重叠池化\"></a>3.3重叠池化</h2><p>在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。Alexnet设置s=2，z=3，这个方案分别降低了top-1 0.4%，top5 0.3%的错误率。且论文提到重叠池化的模型更难过拟合。</p>\n<h2 id=\"3-4整体架构\"><a href=\"#3-4整体架构\" class=\"headerlink\" title=\"3.4整体架构\"></a>3.4整体架构</h2><p>8个带权重的层，前5层是卷积层，剩下3层是全连接层。最后一层是1000维softmax输入</p>\n<p><img src=\"/images/paper/alex1.png\" width=\"640\"></p>\n<p><img src=\"/images/paper/alex2.png\" width=\"640\"></p>\n<h2 id=\"3-5减少过拟合\"><a href=\"#3-5减少过拟合\" class=\"headerlink\" title=\"3.5减少过拟合\"></a>3.5减少过拟合</h2><h3 id=\"3-5-1-数据增强\"><a href=\"#3-5-1-数据增强\" class=\"headerlink\" title=\"3.5.1.数据增强\"></a>3.5.1.数据增强</h3><p>方法一：水平翻转和图像变换（随机裁剪）</p>\n<p>训练时：随机裁剪224<em>224的图片和进行水平翻转，所以数据增大了（250-224）平方</em>2=2048倍</p>\n<p>在测试时，网络通过提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。</p>\n<p>方法二：PCA Jittering（详见7 PCA Jittering）（减少了TOP1错误率 1%以上）</p>\n<h3 id=\"3-5-2-dropout\"><a href=\"#3-5-2-dropout\" class=\"headerlink\" title=\"3.5.2.dropout\"></a>3.5.2.dropout</h3><p>训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。（也有论文，单独看下）<br>朴素思想：以0.5的概率让隐藏层的输出为0，失活的神经元不再进行前向传播且不参与反向传播</p>\n<p>alexnet：前两个全连接层使用dropout，会减少过拟合，但会使训练迭代次数翻一倍</p>\n<h1 id=\"4-学习细节\"><a href=\"#4-学习细节\" class=\"headerlink\" title=\"4.学习细节\"></a>4.学习细节</h1><p>随机梯度下降来训练模型，batch size为128，动量是0.9（详见6.动量），权重衰减为0.0005<br><img src=\"/images/paper/alex3.png\" width=\"640\"><br>使用均值是0，标准差是0.01的高斯分布对每一层的权重进行初始化。2,4,5卷积层和全连接层偏置初始化为1，其余层偏置初始化为0</p>\n<h1 id=\"5-ReLu函数\"><a href=\"#5-ReLu函数\" class=\"headerlink\" title=\"5.ReLu函数\"></a>5.ReLu函数</h1><p>AlexNet成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。而且训练速度也比之前的sigmod和tanh快。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。</p>\n<h2 id=\"5-1训练速度\"><a href=\"#5-1训练速度\" class=\"headerlink\" title=\"5.1训练速度\"></a>5.1训练速度</h2><p>使用ReLu的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍<br><img src=\"/images/paper/alex4.png\" width=\"640\"></p>\n<h2 id=\"5-2梯度消失和梯度爆炸\"><a href=\"#5-2梯度消失和梯度爆炸\" class=\"headerlink\" title=\"5.2梯度消失和梯度爆炸\"></a>5.2梯度消失和梯度爆炸</h2><p><img src=\"/images/paper/alex5.jpg\" width=\"640\"><br>如上图所示的网络结构，我们在进行反向传播更新w1权值的时候，公式如下<br><img src=\"/images/paper/alex6.jpg\" width=\"640\"></p>\n<p>sigmod函数导数如下图所示<br><img src=\"/images/paper/alex7.png\" width=\"640\"><br>从图中可以看出最大值是0.25，所以存在一下两种现象<br><img src=\"/images/paper/alex8.jpg\" width=\"640\"></p>\n<h2 id=\"5-3-ReLu函数\"><a href=\"#5-3-ReLu函数\" class=\"headerlink\" title=\"5.3 ReLu函数\"></a>5.3 ReLu函数</h2><p>relu函数解决问题的方式很简单，如果偏导数是1的话就不存在爆炸与消失的问题了，relu函数图像如下<br><img src=\"/images/paper/alex9.jpg\" width=\"640\"><br>从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</p>\n<h1 id=\"6-动量\"><a href=\"#6-动量\" class=\"headerlink\" title=\"6.动量\"></a>6.动量</h1><p>在物理世界，球体运动是有惯性的，所以可能翻过山头达到全局最优解<br><img src=\"/images/paper/alex10.png\" width=\"640\"><br>把这种概念引入到梯度下降中去，每次更新时不仅考虑当前梯度影响，还要考虑上一步的梯度影响<br><img src=\"/images/paper/alex11.png\" width=\"640\"><br>有可能会翻过去找到全局最优解<br><img src=\"/images/paper/alex12.png\" width=\"640\"></p>\n<h1 id=\"7-PCA-Jittering\"><a href=\"#7-PCA-Jittering\" class=\"headerlink\" title=\"7 PCA Jittering\"></a>7 PCA Jittering</h1><h2 id=\"7-1PCA主成分分析\"><a href=\"#7-1PCA主成分分析\" class=\"headerlink\" title=\"7.1PCA主成分分析\"></a>7.1PCA主成分分析</h2><p>主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。<br>主要计算过程：<br>假设有一数组<br><img src=\"/images/paper/alex13.jpg\" width=\"640\"><br>第一步：分别求出x和y的平均值，对于每个样例减去均值得到<br><img src=\"/images/paper/alex14.jpg\" width=\"640\"><br>第二步，求协方差矩阵得到<br>这是3维的协方差形式<br><img src=\"/images/paper/alex15.jpg\" width=\"640\"></p>\n<p>协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。（这里的方差是修正样本方差，分母是 样本数量-1）<br><img src=\"/images/paper/alex16.jpg\" width=\"640\"><br>协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。<br>第三步：求协方差的特征值和特征向量<br><img src=\"/images/paper/alex17.png\" width=\"640\"><br><img src=\"/images/paper/alex18.jpg\" width=\"640\"><br>在数学上，特别是线性代数总，对于一个给定的矩阵A，它的特征向量v进行经过这个线性变化后，得到新的向量仍然和原来的v保持在一条直线上，但其长度或方向或许会改变，即</p>\n<p>Av=λν</p>\n<p>λ是特征值，v是特征向量<br>（1）如果矩阵可以变换成为对角矩阵，那么它的特征值就是他对角线上的元素，而特征向量就是相应的基：例如矩阵<br><img src=\"/images/paper/alex19.png\" width=\"640\"><br>的特征值就是2和4,2对应的特征向量就是所有形同（a，b，0）T的非零向量，4对应的特征向量是所有形同（0,0，c）T的非零向量<br>（2）对于一般的矩阵<br><img src=\"/images/paper/alex20.jpg\" width=\"640\"><br>第四步：将特征值按照从大到小进行排序，选择其中最大的k个，然后将其对应的特征向量分别作为列向量组成特征向量矩阵<br>第五步：将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m<em>n)，协方差矩阵是n</em>n，选取的k个特征向量组成的矩阵为EigenVectors(n<em>k)。那么投影后的数据FinalData为<br>FinalData(10</em>1) = DataAdjust(10*2矩阵) x 特征向量(-0.677873399, -0.735178656)T<br>最后结果是他<br><img src=\"/images/paper/alex21.jpg\" width=\"640\"><br>这样就把原始的n维降到了k维</p>\n","site":{"data":{}},"excerpt":"","more":"<p>ImageNet Classification with Deep Convolutional Neural Networks（Alexnet）</p>\n<h1 id=\"1-创新的地方\"><a href=\"#1-创新的地方\" class=\"headerlink\" title=\"1.创新的地方\"></a>1.创新的地方</h1><p>（1）使用ReLu函数作为CNN的激活函数</p>\n<p>（2）使用dropout防止过拟合</p>\n<p>（3）使用重叠最大池化</p>\n<p>（4）提出了LRN层</p>\n<h1 id=\"2-摘要\"><a href=\"#2-摘要\" class=\"headerlink\" title=\"2.摘要\"></a>2.摘要</h1><p>（1）提出了一个深度卷积网络，在ImageNet LSVRC2010，top1和top5错误分别是37.5%、17.0%远远好于之前的表现</p>\n<p>（2）神经网络有6000万个参数，650000神经元，5个卷积层（某些卷积层后面有池化层）和三个全连接层</p>\n<p>（3）为减少全连接层的过拟合，使用了一个dropout的正则化方法</p>\n<h1 id=\"3-图片预处理\"><a href=\"#3-图片预处理\" class=\"headerlink\" title=\"3.图片预处理\"></a>3.图片预处理</h1><p>（1）大小归一化<br>将图像进行下采样到固定的256<em>256分辨率，给定一个矩形图像，首先缩放图像短边长度为256，然后从结果图像中裁剪中心的256</em>256大小的图像块<br>（2）减去像素平均值<br>所有图片的每个像素值都减去所有训练集图片的平均值</p>\n<h1 id=\"3-架构\"><a href=\"#3-架构\" class=\"headerlink\" title=\"3.架构\"></a>3.架构</h1><h2 id=\"3-1ReLU非线性\"><a href=\"#3-1ReLU非线性\" class=\"headerlink\" title=\"3.1ReLU非线性\"></a>3.1ReLU非线性</h2><p>训练速度比之前的sigmod和tanh快，并解决了之前激活函数的梯度消失问题，（详见5ReLU函数）</p>\n<h2 id=\"3-2局部响应归一化\"><a href=\"#3-2局部响应归一化\" class=\"headerlink\" title=\"3.2局部响应归一化\"></a>3.2局部响应归一化</h2><p>提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。（ps，博客上都说他其实没什么用）</p>\n<h2 id=\"3-3重叠池化\"><a href=\"#3-3重叠池化\" class=\"headerlink\" title=\"3.3重叠池化\"></a>3.3重叠池化</h2><p>在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。并且AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。Alexnet设置s=2，z=3，这个方案分别降低了top-1 0.4%，top5 0.3%的错误率。且论文提到重叠池化的模型更难过拟合。</p>\n<h2 id=\"3-4整体架构\"><a href=\"#3-4整体架构\" class=\"headerlink\" title=\"3.4整体架构\"></a>3.4整体架构</h2><p>8个带权重的层，前5层是卷积层，剩下3层是全连接层。最后一层是1000维softmax输入</p>\n<p><img src=\"/images/paper/alex1.png\" width=\"640\"></p>\n<p><img src=\"/images/paper/alex2.png\" width=\"640\"></p>\n<h2 id=\"3-5减少过拟合\"><a href=\"#3-5减少过拟合\" class=\"headerlink\" title=\"3.5减少过拟合\"></a>3.5减少过拟合</h2><h3 id=\"3-5-1-数据增强\"><a href=\"#3-5-1-数据增强\" class=\"headerlink\" title=\"3.5.1.数据增强\"></a>3.5.1.数据增强</h3><p>方法一：水平翻转和图像变换（随机裁剪）</p>\n<p>训练时：随机裁剪224<em>224的图片和进行水平翻转，所以数据增大了（250-224）平方</em>2=2048倍</p>\n<p>在测试时，网络通过提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。</p>\n<p>方法二：PCA Jittering（详见7 PCA Jittering）（减少了TOP1错误率 1%以上）</p>\n<h3 id=\"3-5-2-dropout\"><a href=\"#3-5-2-dropout\" class=\"headerlink\" title=\"3.5.2.dropout\"></a>3.5.2.dropout</h3><p>训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。Dropout虽有单独的论文论述，但是AlexNet将其实用化，通过实践证实了它的效果。在AlexNet中主要是最后几个全连接层使用了Dropout。（也有论文，单独看下）<br>朴素思想：以0.5的概率让隐藏层的输出为0，失活的神经元不再进行前向传播且不参与反向传播</p>\n<p>alexnet：前两个全连接层使用dropout，会减少过拟合，但会使训练迭代次数翻一倍</p>\n<h1 id=\"4-学习细节\"><a href=\"#4-学习细节\" class=\"headerlink\" title=\"4.学习细节\"></a>4.学习细节</h1><p>随机梯度下降来训练模型，batch size为128，动量是0.9（详见6.动量），权重衰减为0.0005<br><img src=\"/images/paper/alex3.png\" width=\"640\"><br>使用均值是0，标准差是0.01的高斯分布对每一层的权重进行初始化。2,4,5卷积层和全连接层偏置初始化为1，其余层偏置初始化为0</p>\n<h1 id=\"5-ReLu函数\"><a href=\"#5-ReLu函数\" class=\"headerlink\" title=\"5.ReLu函数\"></a>5.ReLu函数</h1><p>AlexNet成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。而且训练速度也比之前的sigmod和tanh快。虽然ReLU激活函数在很久之前就被提出了，但是直到AlexNet的出现才将其发扬光大。</p>\n<h2 id=\"5-1训练速度\"><a href=\"#5-1训练速度\" class=\"headerlink\" title=\"5.1训练速度\"></a>5.1训练速度</h2><p>使用ReLu的四层卷积神经网络在CIFAR-10数据集上达到25%的训练误差比使用tanh神经元的等价网络（虚线）快六倍<br><img src=\"/images/paper/alex4.png\" width=\"640\"></p>\n<h2 id=\"5-2梯度消失和梯度爆炸\"><a href=\"#5-2梯度消失和梯度爆炸\" class=\"headerlink\" title=\"5.2梯度消失和梯度爆炸\"></a>5.2梯度消失和梯度爆炸</h2><p><img src=\"/images/paper/alex5.jpg\" width=\"640\"><br>如上图所示的网络结构，我们在进行反向传播更新w1权值的时候，公式如下<br><img src=\"/images/paper/alex6.jpg\" width=\"640\"></p>\n<p>sigmod函数导数如下图所示<br><img src=\"/images/paper/alex7.png\" width=\"640\"><br>从图中可以看出最大值是0.25，所以存在一下两种现象<br><img src=\"/images/paper/alex8.jpg\" width=\"640\"></p>\n<h2 id=\"5-3-ReLu函数\"><a href=\"#5-3-ReLu函数\" class=\"headerlink\" title=\"5.3 ReLu函数\"></a>5.3 ReLu函数</h2><p>relu函数解决问题的方式很简单，如果偏导数是1的话就不存在爆炸与消失的问题了，relu函数图像如下<br><img src=\"/images/paper/alex9.jpg\" width=\"640\"><br>从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。</p>\n<h1 id=\"6-动量\"><a href=\"#6-动量\" class=\"headerlink\" title=\"6.动量\"></a>6.动量</h1><p>在物理世界，球体运动是有惯性的，所以可能翻过山头达到全局最优解<br><img src=\"/images/paper/alex10.png\" width=\"640\"><br>把这种概念引入到梯度下降中去，每次更新时不仅考虑当前梯度影响，还要考虑上一步的梯度影响<br><img src=\"/images/paper/alex11.png\" width=\"640\"><br>有可能会翻过去找到全局最优解<br><img src=\"/images/paper/alex12.png\" width=\"640\"></p>\n<h1 id=\"7-PCA-Jittering\"><a href=\"#7-PCA-Jittering\" class=\"headerlink\" title=\"7 PCA Jittering\"></a>7 PCA Jittering</h1><h2 id=\"7-1PCA主成分分析\"><a href=\"#7-1PCA主成分分析\" class=\"headerlink\" title=\"7.1PCA主成分分析\"></a>7.1PCA主成分分析</h2><p>主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。<br>主要计算过程：<br>假设有一数组<br><img src=\"/images/paper/alex13.jpg\" width=\"640\"><br>第一步：分别求出x和y的平均值，对于每个样例减去均值得到<br><img src=\"/images/paper/alex14.jpg\" width=\"640\"><br>第二步，求协方差矩阵得到<br>这是3维的协方差形式<br><img src=\"/images/paper/alex15.jpg\" width=\"640\"></p>\n<p>协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。（这里的方差是修正样本方差，分母是 样本数量-1）<br><img src=\"/images/paper/alex16.jpg\" width=\"640\"><br>协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。<br>第三步：求协方差的特征值和特征向量<br><img src=\"/images/paper/alex17.png\" width=\"640\"><br><img src=\"/images/paper/alex18.jpg\" width=\"640\"><br>在数学上，特别是线性代数总，对于一个给定的矩阵A，它的特征向量v进行经过这个线性变化后，得到新的向量仍然和原来的v保持在一条直线上，但其长度或方向或许会改变，即</p>\n<p>Av=λν</p>\n<p>λ是特征值，v是特征向量<br>（1）如果矩阵可以变换成为对角矩阵，那么它的特征值就是他对角线上的元素，而特征向量就是相应的基：例如矩阵<br><img src=\"/images/paper/alex19.png\" width=\"640\"><br>的特征值就是2和4,2对应的特征向量就是所有形同（a，b，0）T的非零向量，4对应的特征向量是所有形同（0,0，c）T的非零向量<br>（2）对于一般的矩阵<br><img src=\"/images/paper/alex20.jpg\" width=\"640\"><br>第四步：将特征值按照从大到小进行排序，选择其中最大的k个，然后将其对应的特征向量分别作为列向量组成特征向量矩阵<br>第五步：将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m<em>n)，协方差矩阵是n</em>n，选取的k个特征向量组成的矩阵为EigenVectors(n<em>k)。那么投影后的数据FinalData为<br>FinalData(10</em>1) = DataAdjust(10*2矩阵) x 特征向量(-0.677873399, -0.735178656)T<br>最后结果是他<br><img src=\"/images/paper/alex21.jpg\" width=\"640\"><br>这样就把原始的n维降到了k维</p>\n"},{"title":"vijosP1006晴天小猪历险记","date":"2018-11-01T13:08:25.000Z","_content":"#背景#\n在很久很久以前，有一个动物村庄，那里是猪的乐园（^_^），村民们勤劳、勇敢、善良、团结……\n不过有一天，最小的小小猪生病了，而这种病是极其罕见的，因此大家都没有储存这种药物。所以晴天小猪自告奋勇，要去采取这种药草。于是，晴天小猪的传奇故事便由此展开……\n\n#描述#\n这一天，他来到了一座深山的山脚下，因为只有这座深山中的一位隐者才知道这种药草的所在。但是上山的路错综复杂，由于小小猪的病情，晴天小猪想找一条需时最少的路到达山顶，但现在它一头雾水，所以向你求助。\n\n山用一个三角形表示，从山顶依次向下有1段、2段、3段等山路，每一段用一个数字T（1<=T<=100）表示，代表晴天小猪在这一段山路上需要爬的时间，每一次它都可以朝左、右、左上、右上四个方向走。山是环形的。（**注意**：在任意一层的第一段也可以走到本层的最后一段或上一层的最后一段）。\n\n晴天小猪从山的左下角出发，目的地为山顶，即隐者的小屋。\n\n★★★**本题为vijos早年陈题，描述晦涩，现重新描述题面如下**★★★\n有一个数字三角形，共nn行，依次编号为第一行，第二行至第nn行。其中第ii行有ii个数字，位置依次记为(i,1),(i,2)(i,1),(i,2)到(i,i)(i,i)。\n现在从第nn层的第一个位置出发（即(n,1)(n,1)），每一步移到相邻的，且行编号小于或等于当前行编号的一个位置中，直到(1,1)(1,1)结束，在不重复经过任何位置的情形下，路过的所有位置（包括端点）的对应数字之和最小。\n\n下面详细定义相邻关系。\n同一层内连续的两个位置相邻，特别的有每一层第一个位置与最后一个位置相邻。\n对于位置(i,j)(i,j)，它与(i-1,j-1)(i−1,j−1)以及(i-1,j)(i−1,j)相邻，特别的(i,1)(i,1)与(i-1,i-1)(i−1,i−1)相邻，且(i,i)(i,i)与(i-1,1)(i−1,1)相邻。\n\n#格式#\n##输入格式##\n第一行有一个数n（2<=n<=1000），表示山的高度。\n\n从第二行至第n+1行，第i+1行有i个数，每个数表示晴天小猪在这一段山路上需要爬的时间。\n\n##输出格式##\n一个数，即晴天小猪所需要的最短时间。\n\n##样例1##\n样例输入1\n5\n\n1\n\n2 3\n\n4 5 6\n\n10 1 7 8\n\n1 1 4 5 6\n\n样例输出1\n\n10\n\n限制\n\n各个测试点1s\n\n\n提示\n\n在山的两侧的走法略有特殊，请自己模拟一下，开始我自己都弄错了……\n\n来源\nSunnypig\n\nf[i][j]表示第i行第j个点到目标终点(1,1)的最小时间\n\n则转换为数字三角形问题，但是只是多了几种走法，不断更新最小值就好了\n\n但是问题就来了，这样动态规划具有最优子结构吗？\n\n注意这是个环形走法\n\n答案是不成立于的，怎么说？\n\n我们来看一下这样一个例子，假设某个数据的第某层的时间为\n\n1，1，1，1，1，1\n\n而从下往上推上来一开始的初值f[][]分别为\n\n1，2，4，3，9，10\n\n那么我们先进行第一次同行内从左往右的更新的递推(可以看代码内的推法)\n\n则有更新为\n\n1，2，3，3，4，5\n\n再从右往左更新推一遍\n\n1，2，3，3，4，2(左端的1可以走到右端来更新了右端的时间)\n\n那么这样就完了吗？不，我们可以发现我们可以用新更新的2去更新推出更优的解\n\n则应该为\n\n1，2，3，3，3，2\n\n所以从这个样例中我们可以看出一次两边推根本的不出最优解\n\n为什么呢？\n\n我们看某次往一边推，由于是环形，所以可能从右向左推，用第一个更新了最右端的那个点\n\n但是最右端的那个点在更新之前已经推完了右边数的第二个点\n\n就是新更新的这个右端点并没有用来当作\"下家\"来更新别的点使别的点更优\n\n同理从左往右也是一样\n\n那么怎么办呢？\n\n我们可以推两遍，这样假如更新了某个端点的值，在下一次递推时一定能用来作为\"下家\"尝试再更新别的点\n\n那么这样问题就解决了\n\n我们总结一下做法\n\n首先每个点的初值为从下一层走到这一层的两个更优解\n\n然后我们在同层迭代递推，左推一遍右推一遍，然后再重复推一遍\n\n问题就解决了，so easy\n\nExplanation: [4,-1,2,1] has the largest sum = 6.\n```cpp\n#include<cstdio>\n#define MAXN 1010\n#define INF 2000000010\nint Min(int a,int b){return a<b?a:b;}\nint road[MAXN][MAXN];\nint f[MAXN][MAXN];\nint n;\nint main()\n{\n    scanf(\"%d\",&n);\n    int i,j;\n    for(i=1;i<=n;++i)\n        for(j=1;j<=i;++j)\n            scanf(\"%d\",&road[i][j]);\n     \n    f[n][1]=road[n][1];//开始节点\n\t//处理最后一层 最后一层不需要从上一层传数据\n    for(i=2;i<=n;++i)f[n][i]=INF;\n    for(i=2;i<=n;++i)f[n][i]=Min(f[n][i],f[n][i-1]+road[n][i]);\n    for(i=n-1;i>=1;--i)f[n][i]=Min(f[n][i],f[n][i+1]+road[n][i]);\n    f[n][n]=Min(f[n][n],f[n][1]+road[n][n]);\n     \n    for(i=n-1;i>=1;--i)\n    {\n\t\t//先求出从上一层推出的最小值\n        for(j=1;j<=i;++j)\n            f[i][j]=Min(f[i+1][j],f[i+1][j+1])+road[i][j];\n\t\t//处理两个边界点\n        f[i][i]=Min(f[i][i],f[i+1][1]+road[i][i]);\n        f[i][1]=Min(f[i][1],f[i+1][i+1]+road[i][1]);\n         \n        for(j=2;j<=i;++j)f[i][j]=Min(f[i][j],f[i][j-1]+road[i][j]);//左\n        f[i][1]=Min(f[i][1],f[i][i]+road[i][1]);\n         \n        for(j=i-1;j>=1;--j)f[i][j]=Min(f[i][j],f[i][j+1]+road[i][j]);//右\n        f[i][i]=Min(f[i][i],f[i][1]+road[i][i]);\n         \n        for(j=2;j<=i;++j)f[i][j]=Min(f[i][j],f[i][j-1]+road[i][j]);//左\n        f[i][1]=Min(f[i][1],f[i][i]+road[i][1]);\n         \n        for(j=i-1;j>=1;--j)f[i][j]=Min(f[i][j],f[i][j+1]+road[i][j]);//右\n        f[i][i]=Min(f[i][i],f[i][1]+road[i][i]);\n    }\n     \n    printf(\"%d\\n\",f[1][1]);\n}\n```","source":"_posts/vijosP1006晴天小猪历险记.md","raw":"---\ntitle: vijosP1006晴天小猪历险记\ncategories:\n  - 算法\ndate: 2018-11-01 21:08:25\ntags:\n---\n#背景#\n在很久很久以前，有一个动物村庄，那里是猪的乐园（^_^），村民们勤劳、勇敢、善良、团结……\n不过有一天，最小的小小猪生病了，而这种病是极其罕见的，因此大家都没有储存这种药物。所以晴天小猪自告奋勇，要去采取这种药草。于是，晴天小猪的传奇故事便由此展开……\n\n#描述#\n这一天，他来到了一座深山的山脚下，因为只有这座深山中的一位隐者才知道这种药草的所在。但是上山的路错综复杂，由于小小猪的病情，晴天小猪想找一条需时最少的路到达山顶，但现在它一头雾水，所以向你求助。\n\n山用一个三角形表示，从山顶依次向下有1段、2段、3段等山路，每一段用一个数字T（1<=T<=100）表示，代表晴天小猪在这一段山路上需要爬的时间，每一次它都可以朝左、右、左上、右上四个方向走。山是环形的。（**注意**：在任意一层的第一段也可以走到本层的最后一段或上一层的最后一段）。\n\n晴天小猪从山的左下角出发，目的地为山顶，即隐者的小屋。\n\n★★★**本题为vijos早年陈题，描述晦涩，现重新描述题面如下**★★★\n有一个数字三角形，共nn行，依次编号为第一行，第二行至第nn行。其中第ii行有ii个数字，位置依次记为(i,1),(i,2)(i,1),(i,2)到(i,i)(i,i)。\n现在从第nn层的第一个位置出发（即(n,1)(n,1)），每一步移到相邻的，且行编号小于或等于当前行编号的一个位置中，直到(1,1)(1,1)结束，在不重复经过任何位置的情形下，路过的所有位置（包括端点）的对应数字之和最小。\n\n下面详细定义相邻关系。\n同一层内连续的两个位置相邻，特别的有每一层第一个位置与最后一个位置相邻。\n对于位置(i,j)(i,j)，它与(i-1,j-1)(i−1,j−1)以及(i-1,j)(i−1,j)相邻，特别的(i,1)(i,1)与(i-1,i-1)(i−1,i−1)相邻，且(i,i)(i,i)与(i-1,1)(i−1,1)相邻。\n\n#格式#\n##输入格式##\n第一行有一个数n（2<=n<=1000），表示山的高度。\n\n从第二行至第n+1行，第i+1行有i个数，每个数表示晴天小猪在这一段山路上需要爬的时间。\n\n##输出格式##\n一个数，即晴天小猪所需要的最短时间。\n\n##样例1##\n样例输入1\n5\n\n1\n\n2 3\n\n4 5 6\n\n10 1 7 8\n\n1 1 4 5 6\n\n样例输出1\n\n10\n\n限制\n\n各个测试点1s\n\n\n提示\n\n在山的两侧的走法略有特殊，请自己模拟一下，开始我自己都弄错了……\n\n来源\nSunnypig\n\nf[i][j]表示第i行第j个点到目标终点(1,1)的最小时间\n\n则转换为数字三角形问题，但是只是多了几种走法，不断更新最小值就好了\n\n但是问题就来了，这样动态规划具有最优子结构吗？\n\n注意这是个环形走法\n\n答案是不成立于的，怎么说？\n\n我们来看一下这样一个例子，假设某个数据的第某层的时间为\n\n1，1，1，1，1，1\n\n而从下往上推上来一开始的初值f[][]分别为\n\n1，2，4，3，9，10\n\n那么我们先进行第一次同行内从左往右的更新的递推(可以看代码内的推法)\n\n则有更新为\n\n1，2，3，3，4，5\n\n再从右往左更新推一遍\n\n1，2，3，3，4，2(左端的1可以走到右端来更新了右端的时间)\n\n那么这样就完了吗？不，我们可以发现我们可以用新更新的2去更新推出更优的解\n\n则应该为\n\n1，2，3，3，3，2\n\n所以从这个样例中我们可以看出一次两边推根本的不出最优解\n\n为什么呢？\n\n我们看某次往一边推，由于是环形，所以可能从右向左推，用第一个更新了最右端的那个点\n\n但是最右端的那个点在更新之前已经推完了右边数的第二个点\n\n就是新更新的这个右端点并没有用来当作\"下家\"来更新别的点使别的点更优\n\n同理从左往右也是一样\n\n那么怎么办呢？\n\n我们可以推两遍，这样假如更新了某个端点的值，在下一次递推时一定能用来作为\"下家\"尝试再更新别的点\n\n那么这样问题就解决了\n\n我们总结一下做法\n\n首先每个点的初值为从下一层走到这一层的两个更优解\n\n然后我们在同层迭代递推，左推一遍右推一遍，然后再重复推一遍\n\n问题就解决了，so easy\n\nExplanation: [4,-1,2,1] has the largest sum = 6.\n```cpp\n#include<cstdio>\n#define MAXN 1010\n#define INF 2000000010\nint Min(int a,int b){return a<b?a:b;}\nint road[MAXN][MAXN];\nint f[MAXN][MAXN];\nint n;\nint main()\n{\n    scanf(\"%d\",&n);\n    int i,j;\n    for(i=1;i<=n;++i)\n        for(j=1;j<=i;++j)\n            scanf(\"%d\",&road[i][j]);\n     \n    f[n][1]=road[n][1];//开始节点\n\t//处理最后一层 最后一层不需要从上一层传数据\n    for(i=2;i<=n;++i)f[n][i]=INF;\n    for(i=2;i<=n;++i)f[n][i]=Min(f[n][i],f[n][i-1]+road[n][i]);\n    for(i=n-1;i>=1;--i)f[n][i]=Min(f[n][i],f[n][i+1]+road[n][i]);\n    f[n][n]=Min(f[n][n],f[n][1]+road[n][n]);\n     \n    for(i=n-1;i>=1;--i)\n    {\n\t\t//先求出从上一层推出的最小值\n        for(j=1;j<=i;++j)\n            f[i][j]=Min(f[i+1][j],f[i+1][j+1])+road[i][j];\n\t\t//处理两个边界点\n        f[i][i]=Min(f[i][i],f[i+1][1]+road[i][i]);\n        f[i][1]=Min(f[i][1],f[i+1][i+1]+road[i][1]);\n         \n        for(j=2;j<=i;++j)f[i][j]=Min(f[i][j],f[i][j-1]+road[i][j]);//左\n        f[i][1]=Min(f[i][1],f[i][i]+road[i][1]);\n         \n        for(j=i-1;j>=1;--j)f[i][j]=Min(f[i][j],f[i][j+1]+road[i][j]);//右\n        f[i][i]=Min(f[i][i],f[i][1]+road[i][i]);\n         \n        for(j=2;j<=i;++j)f[i][j]=Min(f[i][j],f[i][j-1]+road[i][j]);//左\n        f[i][1]=Min(f[i][1],f[i][i]+road[i][1]);\n         \n        for(j=i-1;j>=1;--j)f[i][j]=Min(f[i][j],f[i][j+1]+road[i][j]);//右\n        f[i][i]=Min(f[i][i],f[i][1]+road[i][i]);\n    }\n     \n    printf(\"%d\\n\",f[1][1]);\n}\n```","slug":"vijosP1006晴天小猪历险记","published":1,"updated":"2018-11-01T15:44:49.177Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwnc0005m4v5ghwlcv6b","content":"<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>在很久很久以前，有一个动物村庄，那里是猪的乐园（^_^），村民们勤劳、勇敢、善良、团结……<br>不过有一天，最小的小小猪生病了，而这种病是极其罕见的，因此大家都没有储存这种药物。所以晴天小猪自告奋勇，要去采取这种药草。于是，晴天小猪的传奇故事便由此展开……</p>\n<h1 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h1><p>这一天，他来到了一座深山的山脚下，因为只有这座深山中的一位隐者才知道这种药草的所在。但是上山的路错综复杂，由于小小猪的病情，晴天小猪想找一条需时最少的路到达山顶，但现在它一头雾水，所以向你求助。</p>\n<p>山用一个三角形表示，从山顶依次向下有1段、2段、3段等山路，每一段用一个数字T（1&lt;=T&lt;=100）表示，代表晴天小猪在这一段山路上需要爬的时间，每一次它都可以朝左、右、左上、右上四个方向走。山是环形的。（<strong>注意</strong>：在任意一层的第一段也可以走到本层的最后一段或上一层的最后一段）。</p>\n<p>晴天小猪从山的左下角出发，目的地为山顶，即隐者的小屋。</p>\n<p>★★★<strong>本题为vijos早年陈题，描述晦涩，现重新描述题面如下</strong>★★★<br>有一个数字三角形，共nn行，依次编号为第一行，第二行至第nn行。其中第ii行有ii个数字，位置依次记为(i,1),(i,2)(i,1),(i,2)到(i,i)(i,i)。<br>现在从第nn层的第一个位置出发（即(n,1)(n,1)），每一步移到相邻的，且行编号小于或等于当前行编号的一个位置中，直到(1,1)(1,1)结束，在不重复经过任何位置的情形下，路过的所有位置（包括端点）的对应数字之和最小。</p>\n<p>下面详细定义相邻关系。<br>同一层内连续的两个位置相邻，特别的有每一层第一个位置与最后一个位置相邻。<br>对于位置(i,j)(i,j)，它与(i-1,j-1)(i−1,j−1)以及(i-1,j)(i−1,j)相邻，特别的(i,1)(i,1)与(i-1,i-1)(i−1,i−1)相邻，且(i,i)(i,i)与(i-1,1)(i−1,1)相邻。</p>\n<h1 id=\"格式\"><a href=\"#格式\" class=\"headerlink\" title=\"格式\"></a>格式</h1><h2 id=\"输入格式\"><a href=\"#输入格式\" class=\"headerlink\" title=\"输入格式\"></a>输入格式</h2><p>第一行有一个数n（2&lt;=n&lt;=1000），表示山的高度。</p>\n<p>从第二行至第n+1行，第i+1行有i个数，每个数表示晴天小猪在这一段山路上需要爬的时间。</p>\n<h2 id=\"输出格式\"><a href=\"#输出格式\" class=\"headerlink\" title=\"输出格式\"></a>输出格式</h2><p>一个数，即晴天小猪所需要的最短时间。</p>\n<h2 id=\"样例1\"><a href=\"#样例1\" class=\"headerlink\" title=\"样例1\"></a>样例1</h2><p>样例输入1<br>5</p>\n<p>1</p>\n<p>2 3</p>\n<p>4 5 6</p>\n<p>10 1 7 8</p>\n<p>1 1 4 5 6</p>\n<p>样例输出1</p>\n<p>10</p>\n<p>限制</p>\n<p>各个测试点1s</p>\n<p>提示</p>\n<p>在山的两侧的走法略有特殊，请自己模拟一下，开始我自己都弄错了……</p>\n<p>来源<br>Sunnypig</p>\n<p>f[i][j]表示第i行第j个点到目标终点(1,1)的最小时间</p>\n<p>则转换为数字三角形问题，但是只是多了几种走法，不断更新最小值就好了</p>\n<p>但是问题就来了，这样动态规划具有最优子结构吗？</p>\n<p>注意这是个环形走法</p>\n<p>答案是不成立于的，怎么说？</p>\n<p>我们来看一下这样一个例子，假设某个数据的第某层的时间为</p>\n<p>1，1，1，1，1，1</p>\n<p>而从下往上推上来一开始的初值f[][]分别为</p>\n<p>1，2，4，3，9，10</p>\n<p>那么我们先进行第一次同行内从左往右的更新的递推(可以看代码内的推法)</p>\n<p>则有更新为</p>\n<p>1，2，3，3，4，5</p>\n<p>再从右往左更新推一遍</p>\n<p>1，2，3，3，4，2(左端的1可以走到右端来更新了右端的时间)</p>\n<p>那么这样就完了吗？不，我们可以发现我们可以用新更新的2去更新推出更优的解</p>\n<p>则应该为</p>\n<p>1，2，3，3，3，2</p>\n<p>所以从这个样例中我们可以看出一次两边推根本的不出最优解</p>\n<p>为什么呢？</p>\n<p>我们看某次往一边推，由于是环形，所以可能从右向左推，用第一个更新了最右端的那个点</p>\n<p>但是最右端的那个点在更新之前已经推完了右边数的第二个点</p>\n<p>就是新更新的这个右端点并没有用来当作”下家”来更新别的点使别的点更优</p>\n<p>同理从左往右也是一样</p>\n<p>那么怎么办呢？</p>\n<p>我们可以推两遍，这样假如更新了某个端点的值，在下一次递推时一定能用来作为”下家”尝试再更新别的点</p>\n<p>那么这样问题就解决了</p>\n<p>我们总结一下做法</p>\n<p>首先每个点的初值为从下一层走到这一层的两个更优解</p>\n<p>然后我们在同层迭代递推，左推一遍右推一遍，然后再重复推一遍</p>\n<p>问题就解决了，so easy</p>\n<p>Explanation: [4,-1,2,1] has the largest sum = 6.<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> MAXN 1010</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> INF 2000000010</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">Min</span><span class=\"params\">(<span class=\"keyword\">int</span> a,<span class=\"keyword\">int</span> b)</span></span>&#123;<span class=\"keyword\">return</span> a&lt;b?a:b;&#125;</span><br><span class=\"line\"><span class=\"keyword\">int</span> road[MAXN][MAXN];</span><br><span class=\"line\"><span class=\"keyword\">int</span> f[MAXN][MAXN];</span><br><span class=\"line\"><span class=\"keyword\">int</span> n;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">\"%d\"</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i,j;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=n;++i)</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">1</span>;j&lt;=i;++j)</span><br><span class=\"line\">            <span class=\"built_in\">scanf</span>(<span class=\"string\">\"%d\"</span>,&amp;road[i][j]);</span><br><span class=\"line\">     </span><br><span class=\"line\">    f[n][<span class=\"number\">1</span>]=road[n][<span class=\"number\">1</span>];<span class=\"comment\">//开始节点</span></span><br><span class=\"line\">\t<span class=\"comment\">//处理最后一层 最后一层不需要从上一层传数据</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">2</span>;i&lt;=n;++i)f[n][i]=INF;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">2</span>;i&lt;=n;++i)f[n][i]=Min(f[n][i],f[n][i<span class=\"number\">-1</span>]+road[n][i]);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=n<span class=\"number\">-1</span>;i&gt;=<span class=\"number\">1</span>;--i)f[n][i]=Min(f[n][i],f[n][i+<span class=\"number\">1</span>]+road[n][i]);</span><br><span class=\"line\">    f[n][n]=Min(f[n][n],f[n][<span class=\"number\">1</span>]+road[n][n]);</span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=n<span class=\"number\">-1</span>;i&gt;=<span class=\"number\">1</span>;--i)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">//先求出从上一层推出的最小值</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">1</span>;j&lt;=i;++j)</span><br><span class=\"line\">            f[i][j]=Min(f[i+<span class=\"number\">1</span>][j],f[i+<span class=\"number\">1</span>][j+<span class=\"number\">1</span>])+road[i][j];</span><br><span class=\"line\">\t\t<span class=\"comment\">//处理两个边界点</span></span><br><span class=\"line\">        f[i][i]=Min(f[i][i],f[i+<span class=\"number\">1</span>][<span class=\"number\">1</span>]+road[i][i]);</span><br><span class=\"line\">        f[i][<span class=\"number\">1</span>]=Min(f[i][<span class=\"number\">1</span>],f[i+<span class=\"number\">1</span>][i+<span class=\"number\">1</span>]+road[i][<span class=\"number\">1</span>]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">2</span>;j&lt;=i;++j)f[i][j]=Min(f[i][j],f[i][j<span class=\"number\">-1</span>]+road[i][j]);<span class=\"comment\">//左</span></span><br><span class=\"line\">        f[i][<span class=\"number\">1</span>]=Min(f[i][<span class=\"number\">1</span>],f[i][i]+road[i][<span class=\"number\">1</span>]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=i<span class=\"number\">-1</span>;j&gt;=<span class=\"number\">1</span>;--j)f[i][j]=Min(f[i][j],f[i][j+<span class=\"number\">1</span>]+road[i][j]);<span class=\"comment\">//右</span></span><br><span class=\"line\">        f[i][i]=Min(f[i][i],f[i][<span class=\"number\">1</span>]+road[i][i]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">2</span>;j&lt;=i;++j)f[i][j]=Min(f[i][j],f[i][j<span class=\"number\">-1</span>]+road[i][j]);<span class=\"comment\">//左</span></span><br><span class=\"line\">        f[i][<span class=\"number\">1</span>]=Min(f[i][<span class=\"number\">1</span>],f[i][i]+road[i][<span class=\"number\">1</span>]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=i<span class=\"number\">-1</span>;j&gt;=<span class=\"number\">1</span>;--j)f[i][j]=Min(f[i][j],f[i][j+<span class=\"number\">1</span>]+road[i][j]);<span class=\"comment\">//右</span></span><br><span class=\"line\">        f[i][i]=Min(f[i][i],f[i][<span class=\"number\">1</span>]+road[i][i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">\"%d\\n\"</span>,f[<span class=\"number\">1</span>][<span class=\"number\">1</span>]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"背景\"><a href=\"#背景\" class=\"headerlink\" title=\"背景\"></a>背景</h1><p>在很久很久以前，有一个动物村庄，那里是猪的乐园（^_^），村民们勤劳、勇敢、善良、团结……<br>不过有一天，最小的小小猪生病了，而这种病是极其罕见的，因此大家都没有储存这种药物。所以晴天小猪自告奋勇，要去采取这种药草。于是，晴天小猪的传奇故事便由此展开……</p>\n<h1 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h1><p>这一天，他来到了一座深山的山脚下，因为只有这座深山中的一位隐者才知道这种药草的所在。但是上山的路错综复杂，由于小小猪的病情，晴天小猪想找一条需时最少的路到达山顶，但现在它一头雾水，所以向你求助。</p>\n<p>山用一个三角形表示，从山顶依次向下有1段、2段、3段等山路，每一段用一个数字T（1&lt;=T&lt;=100）表示，代表晴天小猪在这一段山路上需要爬的时间，每一次它都可以朝左、右、左上、右上四个方向走。山是环形的。（<strong>注意</strong>：在任意一层的第一段也可以走到本层的最后一段或上一层的最后一段）。</p>\n<p>晴天小猪从山的左下角出发，目的地为山顶，即隐者的小屋。</p>\n<p>★★★<strong>本题为vijos早年陈题，描述晦涩，现重新描述题面如下</strong>★★★<br>有一个数字三角形，共nn行，依次编号为第一行，第二行至第nn行。其中第ii行有ii个数字，位置依次记为(i,1),(i,2)(i,1),(i,2)到(i,i)(i,i)。<br>现在从第nn层的第一个位置出发（即(n,1)(n,1)），每一步移到相邻的，且行编号小于或等于当前行编号的一个位置中，直到(1,1)(1,1)结束，在不重复经过任何位置的情形下，路过的所有位置（包括端点）的对应数字之和最小。</p>\n<p>下面详细定义相邻关系。<br>同一层内连续的两个位置相邻，特别的有每一层第一个位置与最后一个位置相邻。<br>对于位置(i,j)(i,j)，它与(i-1,j-1)(i−1,j−1)以及(i-1,j)(i−1,j)相邻，特别的(i,1)(i,1)与(i-1,i-1)(i−1,i−1)相邻，且(i,i)(i,i)与(i-1,1)(i−1,1)相邻。</p>\n<h1 id=\"格式\"><a href=\"#格式\" class=\"headerlink\" title=\"格式\"></a>格式</h1><h2 id=\"输入格式\"><a href=\"#输入格式\" class=\"headerlink\" title=\"输入格式\"></a>输入格式</h2><p>第一行有一个数n（2&lt;=n&lt;=1000），表示山的高度。</p>\n<p>从第二行至第n+1行，第i+1行有i个数，每个数表示晴天小猪在这一段山路上需要爬的时间。</p>\n<h2 id=\"输出格式\"><a href=\"#输出格式\" class=\"headerlink\" title=\"输出格式\"></a>输出格式</h2><p>一个数，即晴天小猪所需要的最短时间。</p>\n<h2 id=\"样例1\"><a href=\"#样例1\" class=\"headerlink\" title=\"样例1\"></a>样例1</h2><p>样例输入1<br>5</p>\n<p>1</p>\n<p>2 3</p>\n<p>4 5 6</p>\n<p>10 1 7 8</p>\n<p>1 1 4 5 6</p>\n<p>样例输出1</p>\n<p>10</p>\n<p>限制</p>\n<p>各个测试点1s</p>\n<p>提示</p>\n<p>在山的两侧的走法略有特殊，请自己模拟一下，开始我自己都弄错了……</p>\n<p>来源<br>Sunnypig</p>\n<p>f[i][j]表示第i行第j个点到目标终点(1,1)的最小时间</p>\n<p>则转换为数字三角形问题，但是只是多了几种走法，不断更新最小值就好了</p>\n<p>但是问题就来了，这样动态规划具有最优子结构吗？</p>\n<p>注意这是个环形走法</p>\n<p>答案是不成立于的，怎么说？</p>\n<p>我们来看一下这样一个例子，假设某个数据的第某层的时间为</p>\n<p>1，1，1，1，1，1</p>\n<p>而从下往上推上来一开始的初值f[][]分别为</p>\n<p>1，2，4，3，9，10</p>\n<p>那么我们先进行第一次同行内从左往右的更新的递推(可以看代码内的推法)</p>\n<p>则有更新为</p>\n<p>1，2，3，3，4，5</p>\n<p>再从右往左更新推一遍</p>\n<p>1，2，3，3，4，2(左端的1可以走到右端来更新了右端的时间)</p>\n<p>那么这样就完了吗？不，我们可以发现我们可以用新更新的2去更新推出更优的解</p>\n<p>则应该为</p>\n<p>1，2，3，3，3，2</p>\n<p>所以从这个样例中我们可以看出一次两边推根本的不出最优解</p>\n<p>为什么呢？</p>\n<p>我们看某次往一边推，由于是环形，所以可能从右向左推，用第一个更新了最右端的那个点</p>\n<p>但是最右端的那个点在更新之前已经推完了右边数的第二个点</p>\n<p>就是新更新的这个右端点并没有用来当作”下家”来更新别的点使别的点更优</p>\n<p>同理从左往右也是一样</p>\n<p>那么怎么办呢？</p>\n<p>我们可以推两遍，这样假如更新了某个端点的值，在下一次递推时一定能用来作为”下家”尝试再更新别的点</p>\n<p>那么这样问题就解决了</p>\n<p>我们总结一下做法</p>\n<p>首先每个点的初值为从下一层走到这一层的两个更优解</p>\n<p>然后我们在同层迭代递推，左推一遍右推一遍，然后再重复推一遍</p>\n<p>问题就解决了，so easy</p>\n<p>Explanation: [4,-1,2,1] has the largest sum = 6.<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;cstdio&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> MAXN 1010</span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">define</span> INF 2000000010</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">Min</span><span class=\"params\">(<span class=\"keyword\">int</span> a,<span class=\"keyword\">int</span> b)</span></span>&#123;<span class=\"keyword\">return</span> a&lt;b?a:b;&#125;</span><br><span class=\"line\"><span class=\"keyword\">int</span> road[MAXN][MAXN];</span><br><span class=\"line\"><span class=\"keyword\">int</span> f[MAXN][MAXN];</span><br><span class=\"line\"><span class=\"keyword\">int</span> n;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">scanf</span>(<span class=\"string\">\"%d\"</span>,&amp;n);</span><br><span class=\"line\">    <span class=\"keyword\">int</span> i,j;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=n;++i)</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">1</span>;j&lt;=i;++j)</span><br><span class=\"line\">            <span class=\"built_in\">scanf</span>(<span class=\"string\">\"%d\"</span>,&amp;road[i][j]);</span><br><span class=\"line\">     </span><br><span class=\"line\">    f[n][<span class=\"number\">1</span>]=road[n][<span class=\"number\">1</span>];<span class=\"comment\">//开始节点</span></span><br><span class=\"line\">\t<span class=\"comment\">//处理最后一层 最后一层不需要从上一层传数据</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">2</span>;i&lt;=n;++i)f[n][i]=INF;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=<span class=\"number\">2</span>;i&lt;=n;++i)f[n][i]=Min(f[n][i],f[n][i<span class=\"number\">-1</span>]+road[n][i]);</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=n<span class=\"number\">-1</span>;i&gt;=<span class=\"number\">1</span>;--i)f[n][i]=Min(f[n][i],f[n][i+<span class=\"number\">1</span>]+road[n][i]);</span><br><span class=\"line\">    f[n][n]=Min(f[n][n],f[n][<span class=\"number\">1</span>]+road[n][n]);</span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"keyword\">for</span>(i=n<span class=\"number\">-1</span>;i&gt;=<span class=\"number\">1</span>;--i)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">\t\t<span class=\"comment\">//先求出从上一层推出的最小值</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">1</span>;j&lt;=i;++j)</span><br><span class=\"line\">            f[i][j]=Min(f[i+<span class=\"number\">1</span>][j],f[i+<span class=\"number\">1</span>][j+<span class=\"number\">1</span>])+road[i][j];</span><br><span class=\"line\">\t\t<span class=\"comment\">//处理两个边界点</span></span><br><span class=\"line\">        f[i][i]=Min(f[i][i],f[i+<span class=\"number\">1</span>][<span class=\"number\">1</span>]+road[i][i]);</span><br><span class=\"line\">        f[i][<span class=\"number\">1</span>]=Min(f[i][<span class=\"number\">1</span>],f[i+<span class=\"number\">1</span>][i+<span class=\"number\">1</span>]+road[i][<span class=\"number\">1</span>]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">2</span>;j&lt;=i;++j)f[i][j]=Min(f[i][j],f[i][j<span class=\"number\">-1</span>]+road[i][j]);<span class=\"comment\">//左</span></span><br><span class=\"line\">        f[i][<span class=\"number\">1</span>]=Min(f[i][<span class=\"number\">1</span>],f[i][i]+road[i][<span class=\"number\">1</span>]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=i<span class=\"number\">-1</span>;j&gt;=<span class=\"number\">1</span>;--j)f[i][j]=Min(f[i][j],f[i][j+<span class=\"number\">1</span>]+road[i][j]);<span class=\"comment\">//右</span></span><br><span class=\"line\">        f[i][i]=Min(f[i][i],f[i][<span class=\"number\">1</span>]+road[i][i]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=<span class=\"number\">2</span>;j&lt;=i;++j)f[i][j]=Min(f[i][j],f[i][j<span class=\"number\">-1</span>]+road[i][j]);<span class=\"comment\">//左</span></span><br><span class=\"line\">        f[i][<span class=\"number\">1</span>]=Min(f[i][<span class=\"number\">1</span>],f[i][i]+road[i][<span class=\"number\">1</span>]);</span><br><span class=\"line\">         </span><br><span class=\"line\">        <span class=\"keyword\">for</span>(j=i<span class=\"number\">-1</span>;j&gt;=<span class=\"number\">1</span>;--j)f[i][j]=Min(f[i][j],f[i][j+<span class=\"number\">1</span>]+road[i][j]);<span class=\"comment\">//右</span></span><br><span class=\"line\">        f[i][i]=Min(f[i][i],f[i][<span class=\"number\">1</span>]+road[i][i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">     </span><br><span class=\"line\">    <span class=\"built_in\">printf</span>(<span class=\"string\">\"%d\\n\"</span>,f[<span class=\"number\">1</span>][<span class=\"number\">1</span>]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"Leetcode53最大子序列和","date":"2018-10-15T10:36:14.000Z","_content":"\nInput: [-2,1,-3,4,-1,2,1,-5,4],\nOutput: 6\nExplanation: [4,-1,2,1] has the largest sum = 6.\n```cpp\n#include<iostream>\n#include<limits.h>\n#include<vector>\nusing namespace std;\n\nclass Solution {\npublic:\n    int maxSubArray(vector<int>& nums) {\n        int ans = 0, maxn = INT_MIN;\n        int len = nums.size();\n        for(int i = 0; i < len; i++){\n            if(ans < 0) ans = 0;  //如果前面的和小0，那么重新开始求和\n            ans += nums[i];\n            maxn = max(maxn, ans);\n        }\n        return maxn;\n    }\n};\nint main(){\n    Solution s;\n    int A[]={-2,1,-3,4,-1,2,1,-5,4};\n    vector<int> B(A,A+9);\n    cout<<s.maxSubArray(B);\n}\n\n```\n\n","source":"_posts/Leetcode53最大子序列和.md","raw":"---\ntitle: Leetcode53最大子序列和\ndate: 2018-10-15 18:36:14\ncategories:\n  - 算法\ntags:\n---\n\nInput: [-2,1,-3,4,-1,2,1,-5,4],\nOutput: 6\nExplanation: [4,-1,2,1] has the largest sum = 6.\n```cpp\n#include<iostream>\n#include<limits.h>\n#include<vector>\nusing namespace std;\n\nclass Solution {\npublic:\n    int maxSubArray(vector<int>& nums) {\n        int ans = 0, maxn = INT_MIN;\n        int len = nums.size();\n        for(int i = 0; i < len; i++){\n            if(ans < 0) ans = 0;  //如果前面的和小0，那么重新开始求和\n            ans += nums[i];\n            maxn = max(maxn, ans);\n        }\n        return maxn;\n    }\n};\nint main(){\n    Solution s;\n    int A[]={-2,1,-3,4,-1,2,1,-5,4};\n    vector<int> B(A,A+9);\n    cout<<s.maxSubArray(B);\n}\n\n```\n\n","slug":"Leetcode53最大子序列和","published":1,"updated":"2018-10-18T15:02:39.575Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwnf0007m4v5xr80slve","content":"<p>Input: [-2,1,-3,4,-1,2,1,-5,4],<br>Output: 6<br>Explanation: [4,-1,2,1] has the largest sum = 6.<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;limits.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">maxSubArray</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>, maxn = INT_MIN;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> len = nums.size();</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; len; i++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(ans &lt; <span class=\"number\">0</span>) ans = <span class=\"number\">0</span>;  <span class=\"comment\">//如果前面的和小0，那么重新开始求和</span></span><br><span class=\"line\">            ans += nums[i];</span><br><span class=\"line\">            maxn = max(maxn, ans);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> maxn;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    Solution s;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> A[]=&#123;<span class=\"number\">-2</span>,<span class=\"number\">1</span>,<span class=\"number\">-3</span>,<span class=\"number\">4</span>,<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-5</span>,<span class=\"number\">4</span>&#125;;</span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; B(A,A+<span class=\"number\">9</span>);</span><br><span class=\"line\">    <span class=\"built_in\">cout</span>&lt;&lt;s.maxSubArray(B);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Input: [-2,1,-3,4,-1,2,1,-5,4],<br>Output: 6<br>Explanation: [4,-1,2,1] has the largest sum = 6.<br><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;limits.h&gt;</span></span></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"meta-keyword\">include</span><span class=\"meta-string\">&lt;vector&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> <span class=\"built_in\">std</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Solution</span> &#123;</span></span><br><span class=\"line\"><span class=\"keyword\">public</span>:</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">maxSubArray</span><span class=\"params\">(<span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> ans = <span class=\"number\">0</span>, maxn = INT_MIN;</span><br><span class=\"line\">        <span class=\"keyword\">int</span> len = nums.size();</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; len; i++)&#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span>(ans &lt; <span class=\"number\">0</span>) ans = <span class=\"number\">0</span>;  <span class=\"comment\">//如果前面的和小0，那么重新开始求和</span></span><br><span class=\"line\">            ans += nums[i];</span><br><span class=\"line\">            maxn = max(maxn, ans);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> maxn;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    Solution s;</span><br><span class=\"line\">    <span class=\"keyword\">int</span> A[]=&#123;<span class=\"number\">-2</span>,<span class=\"number\">1</span>,<span class=\"number\">-3</span>,<span class=\"number\">4</span>,<span class=\"number\">-1</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,<span class=\"number\">-5</span>,<span class=\"number\">4</span>&#125;;</span><br><span class=\"line\">    <span class=\"built_in\">vector</span>&lt;<span class=\"keyword\">int</span>&gt; B(A,A+<span class=\"number\">9</span>);</span><br><span class=\"line\">    <span class=\"built_in\">cout</span>&lt;&lt;s.maxSubArray(B);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n"},{"title":"决策树","date":"2018-10-15T02:59:23.000Z","_content":"#  基本流程 #\n\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-17_221227.png\" width=\"640\"/>\n三种情况导致递归返回\n1. 当前节点包含的样本完全属于同一类别，无需划分\n2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分\n3. 当前节点包含样本集合为空，不能划分\n\n第二种情况，把当前节点标记为叶节点，将其类别设定为该节点所含样本最多的类别\n\n第三种情况，当前设为叶子节点，类别设为其父节点样本最多的类别\n# 划分选择 #\n难点在第八行，即选择最优划分属性，一般而言我们希望随着划分的进行，分支节点所包含的样本尽可能属于同一类，即节点纯度越来越高\n## 信息增益 ##\n**信息熵**\n假定当前样本集合D中第k类样本所占比例是Pk（k=1,2……y）,则信息熵定义为\n$$ Ent(D)=-\\Sigma^y_{k=1} p_k log_2p_k $$\nEnt（D）越小，D的纯度越高 \n\n**信息增益**\n\n假定离散属性a有V个可能的取值\\\\({a^1,a^2……a^V}\\\\)使用属性a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为\\\\(a^v\\\\)的样本，记为\\\\(D^v\\\\)\n\n根据信息熵公式算出信息熵，再考虑到不同分支节点包含的样本数不同，给分支节点赋予权重\\\\(|D^v|/|D|\\\\)，即样本数越多的分支节点的影响越大，于是得到**信息增益公式**\n$$ Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V \\frac {|D^v|}{|D|}Ent(D^v) \\qquad (1)$$\n\n一般而言，信息增益对俄大，则意味着使用属性a来进行划分所获得“纯度提升”越大，所以使用信息增益来进行最优划分属性选择。**著名的ID3决策树算法就是以信息增益为准则来选择划分属性**\n## 增益率 ##\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-18_152827.png\" width=\"640\"/>\n假设我们使用“编号”这一属性来作为一个候选划分，算出信息增益为0.998远大于其他候选属性划分，这很容易理解，“编号”属性产生17个分支，每个分支仅包含一个样本，纯度已达最高，然而这样的决策树不具有泛化能力，无法对新样本进行有效预测\n\n**实际上，信息增益准则对取值数目较多的属性有所偏好**,为减少这种不利影响，**著名C4.5决策树算法使用增益率**选择最优划分属性，公式定义为\n$$ Gain\\_ratio(D,a)=\\frac {Gain(D,a)}{IV(a)} $$\n其中\n$$ IV(a)=-\\sum^V_{v=1} \\frac {|D^v|}{|D|} log_2 \\frac{|D^v|}{|D|} $$\n称为属性a的固有值，属性a的可能取值数目越多（即V越大）,则IV（a）的值通常会越大\n\n需要注意的是，**增益率准则对于可取值数目较少的属性有所偏好**,所以C4.5算法并不是选择增益率最高的属性作为划分，而是采用一个启发式方法**先从候选划分找出信息增益高于平均水平的属性，然后再选择增益率最高的**\n## 基尼指数 ##\nCART决策树使用基尼指数来选择划分属性，数据集D的纯度用基尼值来度量，\n$$ Gini(D)=\\sum^{|y|}_{k=1}\\sum_{k' \\neq k}p_kp_{k'} $$\n$$ =1-\\sum^{|y|}_{k=1}p^2_k $$\n直观来说，基尼反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，**因此基尼越小，则数据集D的纯度越高**,按照（1）式格式，得a属性的基尼指数为\n$$ Gini\\_index(D,a)=\\sum^V_{v=1}\\frac {|D^v|}{|D|}Gini(D^v) $$\n# 剪枝处理 #\n剪枝是决策树学习算法对付过拟合的主要手段\n1. 预剪枝，在决策树生成过程中，对每个节点在划分前进行估计，若当前节点的划分不能导致决策树泛化能力提升，停止划分，并将当前节点标记为叶节点\n2. 后剪枝则是先从训练街生成一颗完整的决策树，然后自底向上对非叶子节点进行考察，若将该节点对应的字数替换为叶节点导致泛华能力提升，则子树替换为叶节点\n\n两者对比\n1. 后剪枝通常比与预剪枝决策树保留更多的分支，所以后剪枝不容易欠拟合，泛化能力往往优于预剪枝决策树\n2. 后剪枝是在生成完全决策树之后进行的，并且要自底向上的对书上的所有非叶子节点进行一一考察，所以开销要比预剪枝决策树大\n\n# 连续与缺失值 #\n## 连续值处理 ##\n属性是连续值，**采用二分法对连续属性进行处理，这正是C4.5决策苏算法采用的机制**\n\n假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\\\({a^1,a^2……，a^n}\\\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合\n$$ T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}$$\n\n**与离散值不同的是，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性 **\n## 缺失值处理 ##\n需要解决两个问题\n1. 如何在属性值缺失的情况下进行最优划分属性选择\n2. 给定划分属性，若样本在该属性值上的值缺失，如何多样本进行划分？\n\n**先来解决第一个问题**\n\n给定训练集D和属性a，令\\\\(D^-\\\\)表示D中属性a没有缺失值的样本子集，对于问题1，显然我们仅可根据\\\\(D^-\\\\)判断属性a的优劣。假设属性a有V个可取值\\\\(a^1,a^2……a^V\\\\)\n\n令\\\\(D^{-v}\\\\)表示\\\\(D^-\\\\)在属性a上取值为\\\\(a^v\\\\)的样本子集\n\n\\\\(D^-_k\\\\)表示\\\\(D^-\\\\)中属于第k类（k=1,2……，|y|）的样本子集\n\n假定我们为每一个样本x赋予一个权重\\\\(w_x\\\\)，并定义\n$$ \\rho = \\frac {\\Sigma_{x \\in D^\\- }w_x}{\\Sigma_{x \\in D}w_x} $$\n$$ p^-_k=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq |y|)$$\n$$ r^-_v=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq V)$$\n基于上述定义，我们将信息增益的计算式推广为，\n$$ Gain(D,a)=\\rho *Gain(D^-,a) $$\n$$ =\\rho *(Ent(D^-)-\\sum^V_{v=1}r^-_vEnt(D^-v))$$\n$$ Ent(D^-)=-\\sum^{|y|}_{k=1}p^-_klog_2p^-_k $$\n**解决第二个问题**\n划分属性是a，样本x在属性a的取值未知，则将x同时划入所有子节点，且样本权值在属性值a v对应的子节点调整为\\\\(r^-_v*w_x\\\\)，直观的看，就是让同一个样本以不同的概率划入到不同子节点中去","source":"_posts/决策树.md","raw":"---\ntitle: 决策树\ndate: 2018-10-15 10:59:23\ncategories:\n  - 机器学习\ntags:\n---\n#  基本流程 #\n\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-17_221227.png\" width=\"640\"/>\n三种情况导致递归返回\n1. 当前节点包含的样本完全属于同一类别，无需划分\n2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分\n3. 当前节点包含样本集合为空，不能划分\n\n第二种情况，把当前节点标记为叶节点，将其类别设定为该节点所含样本最多的类别\n\n第三种情况，当前设为叶子节点，类别设为其父节点样本最多的类别\n# 划分选择 #\n难点在第八行，即选择最优划分属性，一般而言我们希望随着划分的进行，分支节点所包含的样本尽可能属于同一类，即节点纯度越来越高\n## 信息增益 ##\n**信息熵**\n假定当前样本集合D中第k类样本所占比例是Pk（k=1,2……y）,则信息熵定义为\n$$ Ent(D)=-\\Sigma^y_{k=1} p_k log_2p_k $$\nEnt（D）越小，D的纯度越高 \n\n**信息增益**\n\n假定离散属性a有V个可能的取值\\\\({a^1,a^2……a^V}\\\\)使用属性a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为\\\\(a^v\\\\)的样本，记为\\\\(D^v\\\\)\n\n根据信息熵公式算出信息熵，再考虑到不同分支节点包含的样本数不同，给分支节点赋予权重\\\\(|D^v|/|D|\\\\)，即样本数越多的分支节点的影响越大，于是得到**信息增益公式**\n$$ Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V \\frac {|D^v|}{|D|}Ent(D^v) \\qquad (1)$$\n\n一般而言，信息增益对俄大，则意味着使用属性a来进行划分所获得“纯度提升”越大，所以使用信息增益来进行最优划分属性选择。**著名的ID3决策树算法就是以信息增益为准则来选择划分属性**\n## 增益率 ##\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-18_152827.png\" width=\"640\"/>\n假设我们使用“编号”这一属性来作为一个候选划分，算出信息增益为0.998远大于其他候选属性划分，这很容易理解，“编号”属性产生17个分支，每个分支仅包含一个样本，纯度已达最高，然而这样的决策树不具有泛化能力，无法对新样本进行有效预测\n\n**实际上，信息增益准则对取值数目较多的属性有所偏好**,为减少这种不利影响，**著名C4.5决策树算法使用增益率**选择最优划分属性，公式定义为\n$$ Gain\\_ratio(D,a)=\\frac {Gain(D,a)}{IV(a)} $$\n其中\n$$ IV(a)=-\\sum^V_{v=1} \\frac {|D^v|}{|D|} log_2 \\frac{|D^v|}{|D|} $$\n称为属性a的固有值，属性a的可能取值数目越多（即V越大）,则IV（a）的值通常会越大\n\n需要注意的是，**增益率准则对于可取值数目较少的属性有所偏好**,所以C4.5算法并不是选择增益率最高的属性作为划分，而是采用一个启发式方法**先从候选划分找出信息增益高于平均水平的属性，然后再选择增益率最高的**\n## 基尼指数 ##\nCART决策树使用基尼指数来选择划分属性，数据集D的纯度用基尼值来度量，\n$$ Gini(D)=\\sum^{|y|}_{k=1}\\sum_{k' \\neq k}p_kp_{k'} $$\n$$ =1-\\sum^{|y|}_{k=1}p^2_k $$\n直观来说，基尼反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，**因此基尼越小，则数据集D的纯度越高**,按照（1）式格式，得a属性的基尼指数为\n$$ Gini\\_index(D,a)=\\sum^V_{v=1}\\frac {|D^v|}{|D|}Gini(D^v) $$\n# 剪枝处理 #\n剪枝是决策树学习算法对付过拟合的主要手段\n1. 预剪枝，在决策树生成过程中，对每个节点在划分前进行估计，若当前节点的划分不能导致决策树泛化能力提升，停止划分，并将当前节点标记为叶节点\n2. 后剪枝则是先从训练街生成一颗完整的决策树，然后自底向上对非叶子节点进行考察，若将该节点对应的字数替换为叶节点导致泛华能力提升，则子树替换为叶节点\n\n两者对比\n1. 后剪枝通常比与预剪枝决策树保留更多的分支，所以后剪枝不容易欠拟合，泛化能力往往优于预剪枝决策树\n2. 后剪枝是在生成完全决策树之后进行的，并且要自底向上的对书上的所有非叶子节点进行一一考察，所以开销要比预剪枝决策树大\n\n# 连续与缺失值 #\n## 连续值处理 ##\n属性是连续值，**采用二分法对连续属性进行处理，这正是C4.5决策苏算法采用的机制**\n\n假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\\\({a^1,a^2……，a^n}\\\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合\n$$ T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}$$\n\n**与离散值不同的是，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性 **\n## 缺失值处理 ##\n需要解决两个问题\n1. 如何在属性值缺失的情况下进行最优划分属性选择\n2. 给定划分属性，若样本在该属性值上的值缺失，如何多样本进行划分？\n\n**先来解决第一个问题**\n\n给定训练集D和属性a，令\\\\(D^-\\\\)表示D中属性a没有缺失值的样本子集，对于问题1，显然我们仅可根据\\\\(D^-\\\\)判断属性a的优劣。假设属性a有V个可取值\\\\(a^1,a^2……a^V\\\\)\n\n令\\\\(D^{-v}\\\\)表示\\\\(D^-\\\\)在属性a上取值为\\\\(a^v\\\\)的样本子集\n\n\\\\(D^-_k\\\\)表示\\\\(D^-\\\\)中属于第k类（k=1,2……，|y|）的样本子集\n\n假定我们为每一个样本x赋予一个权重\\\\(w_x\\\\)，并定义\n$$ \\rho = \\frac {\\Sigma_{x \\in D^\\- }w_x}{\\Sigma_{x \\in D}w_x} $$\n$$ p^-_k=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq |y|)$$\n$$ r^-_v=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq V)$$\n基于上述定义，我们将信息增益的计算式推广为，\n$$ Gain(D,a)=\\rho *Gain(D^-,a) $$\n$$ =\\rho *(Ent(D^-)-\\sum^V_{v=1}r^-_vEnt(D^-v))$$\n$$ Ent(D^-)=-\\sum^{|y|}_{k=1}p^-_klog_2p^-_k $$\n**解决第二个问题**\n划分属性是a，样本x在属性a的取值未知，则将x同时划入所有子节点，且样本权值在属性值a v对应的子节点调整为\\\\(r^-_v*w_x\\\\)，直观的看，就是让同一个样本以不同的概率划入到不同子节点中去","slug":"决策树","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwni0008m4v592kcfkjc","content":"<h1 id=\"基本流程\"><a href=\"#基本流程\" class=\"headerlink\" title=\"基本流程\"></a>基本流程</h1><p><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-17_221227.png\" width=\"640\"><br>三种情况导致递归返回</p>\n<ol>\n<li>当前节点包含的样本完全属于同一类别，无需划分</li>\n<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分</li>\n<li>当前节点包含样本集合为空，不能划分</li>\n</ol>\n<p>第二种情况，把当前节点标记为叶节点，将其类别设定为该节点所含样本最多的类别</p>\n<p>第三种情况，当前设为叶子节点，类别设为其父节点样本最多的类别</p>\n<h1 id=\"划分选择\"><a href=\"#划分选择\" class=\"headerlink\" title=\"划分选择\"></a>划分选择</h1><p>难点在第八行，即选择最优划分属性，一般而言我们希望随着划分的进行，分支节点所包含的样本尽可能属于同一类，即节点纯度越来越高</p>\n<h2 id=\"信息增益\"><a href=\"#信息增益\" class=\"headerlink\" title=\"信息增益\"></a>信息增益</h2><p><strong>信息熵</strong><br>假定当前样本集合D中第k类样本所占比例是Pk（k=1,2……y）,则信息熵定义为</p>\n<script type=\"math/tex; mode=display\">Ent(D)=-\\Sigma^y_{k=1} p_k log_2p_k</script><p>Ent（D）越小，D的纯度越高 </p>\n<p><strong>信息增益</strong></p>\n<p>假定离散属性a有V个可能的取值\\({a^1,a^2……a^V}\\)使用属性a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为\\(a^v\\)的样本，记为\\(D^v\\)</p>\n<p>根据信息熵公式算出信息熵，再考虑到不同分支节点包含的样本数不同，给分支节点赋予权重\\(|D^v|/|D|\\)，即样本数越多的分支节点的影响越大，于是得到<strong>信息增益公式</strong></p>\n<script type=\"math/tex; mode=display\">Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V \\frac {|D^v|}{|D|}Ent(D^v) \\qquad (1)</script><p>一般而言，信息增益对俄大，则意味着使用属性a来进行划分所获得“纯度提升”越大，所以使用信息增益来进行最优划分属性选择。<strong>著名的ID3决策树算法就是以信息增益为准则来选择划分属性</strong></p>\n<h2 id=\"增益率\"><a href=\"#增益率\" class=\"headerlink\" title=\"增益率\"></a>增益率</h2><p><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-18_152827.png\" width=\"640\"><br>假设我们使用“编号”这一属性来作为一个候选划分，算出信息增益为0.998远大于其他候选属性划分，这很容易理解，“编号”属性产生17个分支，每个分支仅包含一个样本，纯度已达最高，然而这样的决策树不具有泛化能力，无法对新样本进行有效预测</p>\n<p><strong>实际上，信息增益准则对取值数目较多的属性有所偏好</strong>,为减少这种不利影响，<strong>著名C4.5决策树算法使用增益率</strong>选择最优划分属性，公式定义为</p>\n<script type=\"math/tex; mode=display\">Gain\\_ratio(D,a)=\\frac {Gain(D,a)}{IV(a)}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">IV(a)=-\\sum^V_{v=1} \\frac {|D^v|}{|D|} log_2 \\frac{|D^v|}{|D|}</script><p>称为属性a的固有值，属性a的可能取值数目越多（即V越大）,则IV（a）的值通常会越大</p>\n<p>需要注意的是，<strong>增益率准则对于可取值数目较少的属性有所偏好</strong>,所以C4.5算法并不是选择增益率最高的属性作为划分，而是采用一个启发式方法<strong>先从候选划分找出信息增益高于平均水平的属性，然后再选择增益率最高的</strong></p>\n<h2 id=\"基尼指数\"><a href=\"#基尼指数\" class=\"headerlink\" title=\"基尼指数\"></a>基尼指数</h2><p>CART决策树使用基尼指数来选择划分属性，数据集D的纯度用基尼值来度量，</p>\n<script type=\"math/tex; mode=display\">Gini(D)=\\sum^{|y|}_{k=1}\\sum_{k' \\neq k}p_kp_{k'}</script><script type=\"math/tex; mode=display\">=1-\\sum^{|y|}_{k=1}p^2_k</script><p>直观来说，基尼反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，<strong>因此基尼越小，则数据集D的纯度越高</strong>,按照（1）式格式，得a属性的基尼指数为</p>\n<script type=\"math/tex; mode=display\">Gini\\_index(D,a)=\\sum^V_{v=1}\\frac {|D^v|}{|D|}Gini(D^v)</script><h1 id=\"剪枝处理\"><a href=\"#剪枝处理\" class=\"headerlink\" title=\"剪枝处理\"></a>剪枝处理</h1><p>剪枝是决策树学习算法对付过拟合的主要手段</p>\n<ol>\n<li>预剪枝，在决策树生成过程中，对每个节点在划分前进行估计，若当前节点的划分不能导致决策树泛化能力提升，停止划分，并将当前节点标记为叶节点</li>\n<li>后剪枝则是先从训练街生成一颗完整的决策树，然后自底向上对非叶子节点进行考察，若将该节点对应的字数替换为叶节点导致泛华能力提升，则子树替换为叶节点</li>\n</ol>\n<p>两者对比</p>\n<ol>\n<li>后剪枝通常比与预剪枝决策树保留更多的分支，所以后剪枝不容易欠拟合，泛化能力往往优于预剪枝决策树</li>\n<li>后剪枝是在生成完全决策树之后进行的，并且要自底向上的对书上的所有非叶子节点进行一一考察，所以开销要比预剪枝决策树大</li>\n</ol>\n<h1 id=\"连续与缺失值\"><a href=\"#连续与缺失值\" class=\"headerlink\" title=\"连续与缺失值\"></a>连续与缺失值</h1><h2 id=\"连续值处理\"><a href=\"#连续值处理\" class=\"headerlink\" title=\"连续值处理\"></a>连续值处理</h2><p>属性是连续值，<strong>采用二分法对连续属性进行处理，这正是C4.5决策苏算法采用的机制</strong></p>\n<p>假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color=\"red\">将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合</p>\n<script type=\"math/tex; mode=display\">T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}</script><p><strong>与离散值不同的是，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性 </strong></p>\n<h2 id=\"缺失值处理\"><a href=\"#缺失值处理\" class=\"headerlink\" title=\"缺失值处理\"></a>缺失值处理</h2><p>需要解决两个问题</p>\n<ol>\n<li>如何在属性值缺失的情况下进行最优划分属性选择</li>\n<li>给定划分属性，若样本在该属性值上的值缺失，如何多样本进行划分？</li>\n</ol>\n<p><strong>先来解决第一个问题</strong></p>\n<p>给定训练集D和属性a，令\\(D^-\\)表示D中属性a没有缺失值的样本子集，对于问题1，显然我们仅可根据\\(D^-\\)判断属性a的优劣。假设属性a有V个可取值\\(a^1,a^2……a^V\\)</p>\n<p>令\\(D^{-v}\\)表示\\(D^-\\)在属性a上取值为\\(a^v\\)的样本子集</p>\n<p>\\(D^-_k\\)表示\\(D^-\\)中属于第k类（k=1,2……，|y|）的样本子集</p>\n<p>假定我们为每一个样本x赋予一个权重\\(w_x\\)，并定义</p>\n<script type=\"math/tex; mode=display\">\\rho = \\frac {\\Sigma_{x \\in D^\\- }w_x}{\\Sigma_{x \\in D}w_x}</script><script type=\"math/tex; mode=display\">p^-_k=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq |y|)</script><script type=\"math/tex; mode=display\">r^-_v=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq V)</script><p>基于上述定义，我们将信息增益的计算式推广为，</p>\n<script type=\"math/tex; mode=display\">Gain(D,a)=\\rho *Gain(D^-,a)</script><script type=\"math/tex; mode=display\">=\\rho *(Ent(D^-)-\\sum^V_{v=1}r^-_vEnt(D^-v))</script><script type=\"math/tex; mode=display\">Ent(D^-)=-\\sum^{|y|}_{k=1}p^-_klog_2p^-_k</script><p><strong>解决第二个问题</strong><br>划分属性是a，样本x在属性a的取值未知，则将x同时划入所有子节点，且样本权值在属性值a v对应的子节点调整为\\(r^-_v*w_x\\)，直观的看，就是让同一个样本以不同的概率划入到不同子节点中去</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"基本流程\"><a href=\"#基本流程\" class=\"headerlink\" title=\"基本流程\"></a>基本流程</h1><p><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-17_221227.png\" width=\"640\"><br>三种情况导致递归返回</p>\n<ol>\n<li>当前节点包含的样本完全属于同一类别，无需划分</li>\n<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分</li>\n<li>当前节点包含样本集合为空，不能划分</li>\n</ol>\n<p>第二种情况，把当前节点标记为叶节点，将其类别设定为该节点所含样本最多的类别</p>\n<p>第三种情况，当前设为叶子节点，类别设为其父节点样本最多的类别</p>\n<h1 id=\"划分选择\"><a href=\"#划分选择\" class=\"headerlink\" title=\"划分选择\"></a>划分选择</h1><p>难点在第八行，即选择最优划分属性，一般而言我们希望随着划分的进行，分支节点所包含的样本尽可能属于同一类，即节点纯度越来越高</p>\n<h2 id=\"信息增益\"><a href=\"#信息增益\" class=\"headerlink\" title=\"信息增益\"></a>信息增益</h2><p><strong>信息熵</strong><br>假定当前样本集合D中第k类样本所占比例是Pk（k=1,2……y）,则信息熵定义为</p>\n<script type=\"math/tex; mode=display\">Ent(D)=-\\Sigma^y_{k=1} p_k log_2p_k</script><p>Ent（D）越小，D的纯度越高 </p>\n<p><strong>信息增益</strong></p>\n<p>假定离散属性a有V个可能的取值\\({a^1,a^2……a^V}\\)使用属性a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为\\(a^v\\)的样本，记为\\(D^v\\)</p>\n<p>根据信息熵公式算出信息熵，再考虑到不同分支节点包含的样本数不同，给分支节点赋予权重\\(|D^v|/|D|\\)，即样本数越多的分支节点的影响越大，于是得到<strong>信息增益公式</strong></p>\n<script type=\"math/tex; mode=display\">Gain(D,a)=Ent(D)-\\sum\\limits_{v=1}^V \\frac {|D^v|}{|D|}Ent(D^v) \\qquad (1)</script><p>一般而言，信息增益对俄大，则意味着使用属性a来进行划分所获得“纯度提升”越大，所以使用信息增益来进行最优划分属性选择。<strong>著名的ID3决策树算法就是以信息增益为准则来选择划分属性</strong></p>\n<h2 id=\"增益率\"><a href=\"#增益率\" class=\"headerlink\" title=\"增益率\"></a>增益率</h2><p><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-10-18_152827.png\" width=\"640\"><br>假设我们使用“编号”这一属性来作为一个候选划分，算出信息增益为0.998远大于其他候选属性划分，这很容易理解，“编号”属性产生17个分支，每个分支仅包含一个样本，纯度已达最高，然而这样的决策树不具有泛化能力，无法对新样本进行有效预测</p>\n<p><strong>实际上，信息增益准则对取值数目较多的属性有所偏好</strong>,为减少这种不利影响，<strong>著名C4.5决策树算法使用增益率</strong>选择最优划分属性，公式定义为</p>\n<script type=\"math/tex; mode=display\">Gain\\_ratio(D,a)=\\frac {Gain(D,a)}{IV(a)}</script><p>其中</p>\n<script type=\"math/tex; mode=display\">IV(a)=-\\sum^V_{v=1} \\frac {|D^v|}{|D|} log_2 \\frac{|D^v|}{|D|}</script><p>称为属性a的固有值，属性a的可能取值数目越多（即V越大）,则IV（a）的值通常会越大</p>\n<p>需要注意的是，<strong>增益率准则对于可取值数目较少的属性有所偏好</strong>,所以C4.5算法并不是选择增益率最高的属性作为划分，而是采用一个启发式方法<strong>先从候选划分找出信息增益高于平均水平的属性，然后再选择增益率最高的</strong></p>\n<h2 id=\"基尼指数\"><a href=\"#基尼指数\" class=\"headerlink\" title=\"基尼指数\"></a>基尼指数</h2><p>CART决策树使用基尼指数来选择划分属性，数据集D的纯度用基尼值来度量，</p>\n<script type=\"math/tex; mode=display\">Gini(D)=\\sum^{|y|}_{k=1}\\sum_{k' \\neq k}p_kp_{k'}</script><script type=\"math/tex; mode=display\">=1-\\sum^{|y|}_{k=1}p^2_k</script><p>直观来说，基尼反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，<strong>因此基尼越小，则数据集D的纯度越高</strong>,按照（1）式格式，得a属性的基尼指数为</p>\n<script type=\"math/tex; mode=display\">Gini\\_index(D,a)=\\sum^V_{v=1}\\frac {|D^v|}{|D|}Gini(D^v)</script><h1 id=\"剪枝处理\"><a href=\"#剪枝处理\" class=\"headerlink\" title=\"剪枝处理\"></a>剪枝处理</h1><p>剪枝是决策树学习算法对付过拟合的主要手段</p>\n<ol>\n<li>预剪枝，在决策树生成过程中，对每个节点在划分前进行估计，若当前节点的划分不能导致决策树泛化能力提升，停止划分，并将当前节点标记为叶节点</li>\n<li>后剪枝则是先从训练街生成一颗完整的决策树，然后自底向上对非叶子节点进行考察，若将该节点对应的字数替换为叶节点导致泛华能力提升，则子树替换为叶节点</li>\n</ol>\n<p>两者对比</p>\n<ol>\n<li>后剪枝通常比与预剪枝决策树保留更多的分支，所以后剪枝不容易欠拟合，泛化能力往往优于预剪枝决策树</li>\n<li>后剪枝是在生成完全决策树之后进行的，并且要自底向上的对书上的所有非叶子节点进行一一考察，所以开销要比预剪枝决策树大</li>\n</ol>\n<h1 id=\"连续与缺失值\"><a href=\"#连续与缺失值\" class=\"headerlink\" title=\"连续与缺失值\"></a>连续与缺失值</h1><h2 id=\"连续值处理\"><a href=\"#连续值处理\" class=\"headerlink\" title=\"连续值处理\"></a>连续值处理</h2><p>属性是连续值，<strong>采用二分法对连续属性进行处理，这正是C4.5决策苏算法采用的机制</strong></p>\n<p>假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color=\"red\">将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合</p>\n<script type=\"math/tex; mode=display\">T_a=\\{\\frac {a^i+a^{i+1}}{2} | 1 \\leq i \\leq n-1\\}</script><p><strong>与离散值不同的是，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性 </strong></p>\n<h2 id=\"缺失值处理\"><a href=\"#缺失值处理\" class=\"headerlink\" title=\"缺失值处理\"></a>缺失值处理</h2><p>需要解决两个问题</p>\n<ol>\n<li>如何在属性值缺失的情况下进行最优划分属性选择</li>\n<li>给定划分属性，若样本在该属性值上的值缺失，如何多样本进行划分？</li>\n</ol>\n<p><strong>先来解决第一个问题</strong></p>\n<p>给定训练集D和属性a，令\\(D^-\\)表示D中属性a没有缺失值的样本子集，对于问题1，显然我们仅可根据\\(D^-\\)判断属性a的优劣。假设属性a有V个可取值\\(a^1,a^2……a^V\\)</p>\n<p>令\\(D^{-v}\\)表示\\(D^-\\)在属性a上取值为\\(a^v\\)的样本子集</p>\n<p>\\(D^-_k\\)表示\\(D^-\\)中属于第k类（k=1,2……，|y|）的样本子集</p>\n<p>假定我们为每一个样本x赋予一个权重\\(w_x\\)，并定义</p>\n<script type=\"math/tex; mode=display\">\\rho = \\frac {\\Sigma_{x \\in D^\\- }w_x}{\\Sigma_{x \\in D}w_x}</script><script type=\"math/tex; mode=display\">p^-_k=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq |y|)</script><script type=\"math/tex; mode=display\">r^-_v=\\frac {\\Sigma_{x \\in D^-_k}w_x}{\\Sigma_{x \\in D^-}w_x} \\qquad (1 \\leq k \\leq V)</script><p>基于上述定义，我们将信息增益的计算式推广为，</p>\n<script type=\"math/tex; mode=display\">Gain(D,a)=\\rho *Gain(D^-,a)</script><script type=\"math/tex; mode=display\">=\\rho *(Ent(D^-)-\\sum^V_{v=1}r^-_vEnt(D^-v))</script><script type=\"math/tex; mode=display\">Ent(D^-)=-\\sum^{|y|}_{k=1}p^-_klog_2p^-_k</script><p><strong>解决第二个问题</strong><br>划分属性是a，样本x在属性a的取值未知，则将x同时划入所有子节点，且样本权值在属性值a v对应的子节点调整为\\(r^-_v*w_x\\)，直观的看，就是让同一个样本以不同的概率划入到不同子节点中去</p>\n"},{"title":"提升方法","date":"2018-10-27T15:47:03.000Z","_content":"# 提升方法的adaboost #\n提升方法基于这么一种思想，对于一个复杂人物来讲，将多个专家的判断进行适当的综合判断所得出的结论，要比其中任何一个专家单独的判断好，实际上就是三个臭皮匠顶个诸葛亮\n\n**一种直观的adaboost解释**\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_214930.png\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215044.png\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215112.png\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/546ABABE-3C1C-443A-9059-F2F73EEA3C8C.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/B685CD1F-9D3D-4B30-98D9-311C89F4C97B.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/187EB007-2168-41F0-A81E-71BE7546A034.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/08ECBDEC-D992-4D1C-A5D8-885A4E1C145C.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/FB52240D-ADED-4D99-A92C-DAE34F53649E.jpeg\" width=\"640\"/>\n# 提升树 #\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2D5FAAA3-11CF-4C2A-8E41-4D1ED9AE7931.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/8691BE0A-3ABD-45A2-848B-58D715BE68B2.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2DF154A7-D4F1-44D1-B8FF-A0FF2C37EB47.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/BA6993CF-ED4F-49A5-80CC-24543A56104E.jpeg\" width=\"640\"/>\n","source":"_posts/提升方法.md","raw":"---\ntitle: 提升方法\ncategories:\n  - 机器学习\ndate: 2018-10-27 23:47:03\ntags:\n---\n# 提升方法的adaboost #\n提升方法基于这么一种思想，对于一个复杂人物来讲，将多个专家的判断进行适当的综合判断所得出的结论，要比其中任何一个专家单独的判断好，实际上就是三个臭皮匠顶个诸葛亮\n\n**一种直观的adaboost解释**\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_214930.png\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215044.png\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215112.png\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/546ABABE-3C1C-443A-9059-F2F73EEA3C8C.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/B685CD1F-9D3D-4B30-98D9-311C89F4C97B.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/187EB007-2168-41F0-A81E-71BE7546A034.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/08ECBDEC-D992-4D1C-A5D8-885A4E1C145C.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/FB52240D-ADED-4D99-A92C-DAE34F53649E.jpeg\" width=\"640\"/>\n# 提升树 #\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2D5FAAA3-11CF-4C2A-8E41-4D1ED9AE7931.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/8691BE0A-3ABD-45A2-848B-58D715BE68B2.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/2DF154A7-D4F1-44D1-B8FF-A0FF2C37EB47.jpeg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/BA6993CF-ED4F-49A5-80CC-24543A56104E.jpeg\" width=\"640\"/>\n","slug":"提升方法","published":1,"updated":"2018-11-01T15:44:49.178Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwwe000jm4v57wvlnnli","content":"<h1 id=\"提升方法的adaboost\"><a href=\"#提升方法的adaboost\" class=\"headerlink\" title=\"提升方法的adaboost\"></a>提升方法的adaboost</h1><p>提升方法基于这么一种思想，对于一个复杂人物来讲，将多个专家的判断进行适当的综合判断所得出的结论，要比其中任何一个专家单独的判断好，实际上就是三个臭皮匠顶个诸葛亮</p>\n<p><strong>一种直观的adaboost解释</strong><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_214930.png\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215044.png\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215112.png\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/546ABABE-3C1C-443A-9059-F2F73EEA3C8C.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/B685CD1F-9D3D-4B30-98D9-311C89F4C97B.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/187EB007-2168-41F0-A81E-71BE7546A034.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/08ECBDEC-D992-4D1C-A5D8-885A4E1C145C.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/FB52240D-ADED-4D99-A92C-DAE34F53649E.jpeg\" width=\"640\"></p>\n<h1 id=\"提升树\"><a href=\"#提升树\" class=\"headerlink\" title=\"提升树\"></a>提升树</h1><p><img src=\"http://pgmz9e1an.bkt.clouddn.com/2D5FAAA3-11CF-4C2A-8E41-4D1ED9AE7931.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/8691BE0A-3ABD-45A2-848B-58D715BE68B2.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2DF154A7-D4F1-44D1-B8FF-A0FF2C37EB47.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/BA6993CF-ED4F-49A5-80CC-24543A56104E.jpeg\" width=\"640\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"提升方法的adaboost\"><a href=\"#提升方法的adaboost\" class=\"headerlink\" title=\"提升方法的adaboost\"></a>提升方法的adaboost</h1><p>提升方法基于这么一种思想，对于一个复杂人物来讲，将多个专家的判断进行适当的综合判断所得出的结论，要比其中任何一个专家单独的判断好，实际上就是三个臭皮匠顶个诸葛亮</p>\n<p><strong>一种直观的adaboost解释</strong><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_214930.png\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215044.png\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2018-11-01_215112.png\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/546ABABE-3C1C-443A-9059-F2F73EEA3C8C.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/B685CD1F-9D3D-4B30-98D9-311C89F4C97B.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/187EB007-2168-41F0-A81E-71BE7546A034.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/08ECBDEC-D992-4D1C-A5D8-885A4E1C145C.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/FB52240D-ADED-4D99-A92C-DAE34F53649E.jpeg\" width=\"640\"></p>\n<h1 id=\"提升树\"><a href=\"#提升树\" class=\"headerlink\" title=\"提升树\"></a>提升树</h1><p><img src=\"http://pgmz9e1an.bkt.clouddn.com/2D5FAAA3-11CF-4C2A-8E41-4D1ED9AE7931.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/8691BE0A-3ABD-45A2-848B-58D715BE68B2.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/2DF154A7-D4F1-44D1-B8FF-A0FF2C37EB47.jpeg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/BA6993CF-ED4F-49A5-80CC-24543A56104E.jpeg\" width=\"640\"></p>\n"},{"title":"支持向量机","date":"2018-10-19T12:24:12.000Z","_content":"\n#线性可支持向量机与硬间隔最大化#\n## 线性可支持向量机 ##\n训练集\\\\(D={(x_1,y_1),(x_2,y_2)……(x_m,y_m)},\\qquad y_i \\in {-1,+1}\\\\)能将样本分开的超平面有很多，我们应该用哪个\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20181020000158.png\" width=\"640\"/>\n在样本空间中，划分超平面可通过如下线性方程来描述\n$$ w^Tx+b=0 $$\n其中w是法向量，决定了超平面的方向，b为位移项，决定了超平面和原点之间的距离，显然超平面可被法向量w和位移确定。\n假设超平面（w,b）能将训练样本正确分类，即对于\\\\((x_i,y_i) \\in D\\\\),若yi=+1，则有\\\\(w^T+b>0\\\\),若yi=-1，则小于0\n\n\n##函数间隔与几何间隔##\n###函数间隔 ###\n对于给定的训练集D和超平面（w,b）定义超平面（w,b）关于样本点(xi,yi)的函数间隔为\n$$ \\hat{\\gamma}_i=y_i(w*x_i+b)$$\n定义超平面（w,b）关于数据集D的函数间隔为超平面（w,b）关于D中所有样本点(xi,yi)的函数间隔的最小值，即\n\n\n\n$$ \\hat{\\gamma}=\\min\\limits_{(i=1,2……m)}\\hat{\\gamma}_i $$ \n函数间隔可以表示分类预测的正确性和确信度，但是选择分离超平面时，只有函数间隔还不够，因为我们只要成比例的改变w和b。超平面并没有改变，但函数间隔却变成之前的比例倍\n###几何间隔###\n对于给定的训练数据集T和超平面（w,b）定义超平面关于样本点(xi,yi)的几何间隔为\n$$ \\gamma_i=y_i(\\frac{w}{||w||}*x_i+\\frac{b}{||w||}) $$\n定义超平面关于样本集D的几何间隔为\n$$ \\gamma=\\min\\limits_{i=1,2……m}\\gamma_i $$\n<font color='red'>超平面（w,b）关于样本点(xi,yi)的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离</font>\n\n从函数间隔与几何间隔定义，可知以下关系式\n$$\\gamma=\\frac{\\hat{\\gamma}}{||w||} $$\n##间隔最大化##\n<font color='red'>支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面</font>\n### 最大间隔分离超平面###\n最大几何间隔分离超平面，表示为下面的约束最优化问题\n$$ \\max\\limits_{w,b} \\qquad \\gamma $$\n$$ s.t \\qquad y_i{\\frac{w}{||w||}*x_i+\\frac{b}{||w||}}\\geq\\gamma,\\qquad i=1,2,3,……m $$\n考虑到几何间隔与函数间隔的关系式，这个问题可写为\n$$ \\max\\limits_{w,b} \\qquad \\frac{\\hat{\\gamma}}{||w||} $$\n$$ s.t. \\qquad y_i(w*x_i+b)\\geq\\hat{\\gamma},\\qquad i=1,2,3……m $$\n函数间隔\\\\(\\hat{\\gamma}\\\\)的取值不影响最优化问题的解。事实上，假设将w和b按比例的改变为\\\\(\\lambda w\\\\)和\\\\(\\lambda b\\\\)，此时函数间隔成为\\\\(\\lambda \\hat{\\gamma}\\\\)函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，他产生了一个等价的最优化问题，这样取\\\\(\\hat{\\gamma}\\\\)=1，代入上式，且最大化\\\\(\\frac{1}{||w||}\\\\)和最小化\\\\(1/2 ||w||^2 \\\\)是等价的，于是得到一下最优化问题\n\n$$ \\min\\limits_{w,b}\\qquad\\frac{1}{2}||w||^2 $$\n$$ s.t. \\qquad y_i(w*x_i)-1\\geq0 \\qquad i=1,2,……m $$ \n\n### 学习的对偶算法 ###\n\n<a href=\"#duiou\"><font color=\"red\">关于拉格朗日对偶性数学推导看这里</font></a>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152008.jpg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152013.jpg?imageMogr2/auto-orient\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152017.jpg\" width=\"640\"/>\n## 对偶问题 ##\n我们希望求解上式得到最大间隔划分超平面模型\n$$ f(x)=w^Tx+b$$\n(1)式是一个凸二次规划问题，<a href=\"#zuiyou\"><font color='red'>一些最优化知识</font></a>使用拉格朗日乘子法得到其对偶问题，该问题的拉格朗日函数可写为\n$$ L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum ^m_{i=1}\\alpha_i(1-y_i(w^Tx_i+b)) \\qquad (2)$$\n其中\\\\(\\alpha=(\\alpha_1；……\\alpha_m)\\\\)，令L(w,b,a)对w和b的偏导为0得到\n$$ w=\\sum^m_{i=1}\\alpha_iy_ix_i \\qquad (3)$$\n$$ 0=\\sum^m_{i=1}\\alpha_i y_i\\qquad(4)$$\n把（3）式代入（2）消去w，b。在考虑（4）的约束，就得到（1）的对偶问题\n$$max(\\alpha)=\\sum^m_{i=1}\\alpha_i-1/2 \\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_jx^T_ix_j$$\n$$ s.t \\sum^m_{i=1}\\alpha_iy_i=0$$\n$$ \\alpha_i \\geq 0,\\qquad i=1,2,……m$$\n<div id=\"duiou\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104007.jpg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104013.jpg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104017.jpg\" width=\"640\"/>\n<div id=\"zuiyou\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181019202304.jpg\" width=\"640\"/>\n","source":"_posts/支持向量机.md","raw":"---\ntitle: 支持向量机\ndate: 2018-10-19 20:24:12\ncategories:\n  - 机器学习\ntags:\n---\n\n#线性可支持向量机与硬间隔最大化#\n## 线性可支持向量机 ##\n训练集\\\\(D={(x_1,y_1),(x_2,y_2)……(x_m,y_m)},\\qquad y_i \\in {-1,+1}\\\\)能将样本分开的超平面有很多，我们应该用哪个\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20181020000158.png\" width=\"640\"/>\n在样本空间中，划分超平面可通过如下线性方程来描述\n$$ w^Tx+b=0 $$\n其中w是法向量，决定了超平面的方向，b为位移项，决定了超平面和原点之间的距离，显然超平面可被法向量w和位移确定。\n假设超平面（w,b）能将训练样本正确分类，即对于\\\\((x_i,y_i) \\in D\\\\),若yi=+1，则有\\\\(w^T+b>0\\\\),若yi=-1，则小于0\n\n\n##函数间隔与几何间隔##\n###函数间隔 ###\n对于给定的训练集D和超平面（w,b）定义超平面（w,b）关于样本点(xi,yi)的函数间隔为\n$$ \\hat{\\gamma}_i=y_i(w*x_i+b)$$\n定义超平面（w,b）关于数据集D的函数间隔为超平面（w,b）关于D中所有样本点(xi,yi)的函数间隔的最小值，即\n\n\n\n$$ \\hat{\\gamma}=\\min\\limits_{(i=1,2……m)}\\hat{\\gamma}_i $$ \n函数间隔可以表示分类预测的正确性和确信度，但是选择分离超平面时，只有函数间隔还不够，因为我们只要成比例的改变w和b。超平面并没有改变，但函数间隔却变成之前的比例倍\n###几何间隔###\n对于给定的训练数据集T和超平面（w,b）定义超平面关于样本点(xi,yi)的几何间隔为\n$$ \\gamma_i=y_i(\\frac{w}{||w||}*x_i+\\frac{b}{||w||}) $$\n定义超平面关于样本集D的几何间隔为\n$$ \\gamma=\\min\\limits_{i=1,2……m}\\gamma_i $$\n<font color='red'>超平面（w,b）关于样本点(xi,yi)的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离</font>\n\n从函数间隔与几何间隔定义，可知以下关系式\n$$\\gamma=\\frac{\\hat{\\gamma}}{||w||} $$\n##间隔最大化##\n<font color='red'>支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面</font>\n### 最大间隔分离超平面###\n最大几何间隔分离超平面，表示为下面的约束最优化问题\n$$ \\max\\limits_{w,b} \\qquad \\gamma $$\n$$ s.t \\qquad y_i{\\frac{w}{||w||}*x_i+\\frac{b}{||w||}}\\geq\\gamma,\\qquad i=1,2,3,……m $$\n考虑到几何间隔与函数间隔的关系式，这个问题可写为\n$$ \\max\\limits_{w,b} \\qquad \\frac{\\hat{\\gamma}}{||w||} $$\n$$ s.t. \\qquad y_i(w*x_i+b)\\geq\\hat{\\gamma},\\qquad i=1,2,3……m $$\n函数间隔\\\\(\\hat{\\gamma}\\\\)的取值不影响最优化问题的解。事实上，假设将w和b按比例的改变为\\\\(\\lambda w\\\\)和\\\\(\\lambda b\\\\)，此时函数间隔成为\\\\(\\lambda \\hat{\\gamma}\\\\)函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，他产生了一个等价的最优化问题，这样取\\\\(\\hat{\\gamma}\\\\)=1，代入上式，且最大化\\\\(\\frac{1}{||w||}\\\\)和最小化\\\\(1/2 ||w||^2 \\\\)是等价的，于是得到一下最优化问题\n\n$$ \\min\\limits_{w,b}\\qquad\\frac{1}{2}||w||^2 $$\n$$ s.t. \\qquad y_i(w*x_i)-1\\geq0 \\qquad i=1,2,……m $$ \n\n### 学习的对偶算法 ###\n\n<a href=\"#duiou\"><font color=\"red\">关于拉格朗日对偶性数学推导看这里</font></a>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152008.jpg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152013.jpg?imageMogr2/auto-orient\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152017.jpg\" width=\"640\"/>\n## 对偶问题 ##\n我们希望求解上式得到最大间隔划分超平面模型\n$$ f(x)=w^Tx+b$$\n(1)式是一个凸二次规划问题，<a href=\"#zuiyou\"><font color='red'>一些最优化知识</font></a>使用拉格朗日乘子法得到其对偶问题，该问题的拉格朗日函数可写为\n$$ L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum ^m_{i=1}\\alpha_i(1-y_i(w^Tx_i+b)) \\qquad (2)$$\n其中\\\\(\\alpha=(\\alpha_1；……\\alpha_m)\\\\)，令L(w,b,a)对w和b的偏导为0得到\n$$ w=\\sum^m_{i=1}\\alpha_iy_ix_i \\qquad (3)$$\n$$ 0=\\sum^m_{i=1}\\alpha_i y_i\\qquad(4)$$\n把（3）式代入（2）消去w，b。在考虑（4）的约束，就得到（1）的对偶问题\n$$max(\\alpha)=\\sum^m_{i=1}\\alpha_i-1/2 \\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_jx^T_ix_j$$\n$$ s.t \\sum^m_{i=1}\\alpha_iy_i=0$$\n$$ \\alpha_i \\geq 0,\\qquad i=1,2,……m$$\n<div id=\"duiou\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104007.jpg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104013.jpg\" width=\"640\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104017.jpg\" width=\"640\"/>\n<div id=\"zuiyou\"/>\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181019202304.jpg\" width=\"640\"/>\n","slug":"支持向量机","published":1,"updated":"2018-10-27T15:41:30.795Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwwl000lm4v5jz01y9v6","content":"<h1 id=\"线性可支持向量机与硬间隔最大化\"><a href=\"#线性可支持向量机与硬间隔最大化\" class=\"headerlink\" title=\"线性可支持向量机与硬间隔最大化\"></a>线性可支持向量机与硬间隔最大化</h1><h2 id=\"线性可支持向量机\"><a href=\"#线性可支持向量机\" class=\"headerlink\" title=\"线性可支持向量机\"></a>线性可支持向量机</h2><p>训练集\\(D={(x_1,y_1),(x_2,y_2)……(x_m,y_m)},\\qquad y_i \\in {-1,+1}\\)能将样本分开的超平面有很多，我们应该用哪个<br><img src=\"http://pgmz9e1an.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20181020000158.png\" width=\"640\"><br>在样本空间中，划分超平面可通过如下线性方程来描述</p>\n<script type=\"math/tex; mode=display\">w^Tx+b=0</script><p>其中w是法向量，决定了超平面的方向，b为位移项，决定了超平面和原点之间的距离，显然超平面可被法向量w和位移确定。<br>假设超平面（w,b）能将训练样本正确分类，即对于\\((x_i,y_i) \\in D\\),若yi=+1，则有\\(w^T+b&gt;0\\),若yi=-1，则小于0</p>\n<h2 id=\"函数间隔与几何间隔\"><a href=\"#函数间隔与几何间隔\" class=\"headerlink\" title=\"函数间隔与几何间隔\"></a>函数间隔与几何间隔</h2><h3 id=\"函数间隔\"><a href=\"#函数间隔\" class=\"headerlink\" title=\"函数间隔\"></a>函数间隔</h3><p>对于给定的训练集D和超平面（w,b）定义超平面（w,b）关于样本点(xi,yi)的函数间隔为</p>\n<script type=\"math/tex; mode=display\">\\hat{\\gamma}_i=y_i(w*x_i+b)</script><p>定义超平面（w,b）关于数据集D的函数间隔为超平面（w,b）关于D中所有样本点(xi,yi)的函数间隔的最小值，即</p>\n<script type=\"math/tex; mode=display\">\\hat{\\gamma}=\\min\\limits_{(i=1,2……m)}\\hat{\\gamma}_i</script><p>函数间隔可以表示分类预测的正确性和确信度，但是选择分离超平面时，只有函数间隔还不够，因为我们只要成比例的改变w和b。超平面并没有改变，但函数间隔却变成之前的比例倍</p>\n<h3 id=\"几何间隔\"><a href=\"#几何间隔\" class=\"headerlink\" title=\"几何间隔\"></a>几何间隔</h3><p>对于给定的训练数据集T和超平面（w,b）定义超平面关于样本点(xi,yi)的几何间隔为</p>\n<script type=\"math/tex; mode=display\">\\gamma_i=y_i(\\frac{w}{||w||}*x_i+\\frac{b}{||w||})</script><p>定义超平面关于样本集D的几何间隔为</p>\n<script type=\"math/tex; mode=display\">\\gamma=\\min\\limits_{i=1,2……m}\\gamma_i</script><font color=\"red\">超平面（w,b）关于样本点(xi,yi)的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离</font>\n\n<p>从函数间隔与几何间隔定义，可知以下关系式</p>\n<script type=\"math/tex; mode=display\">\\gamma=\\frac{\\hat{\\gamma}}{||w||}</script><h2 id=\"间隔最大化\"><a href=\"#间隔最大化\" class=\"headerlink\" title=\"间隔最大化\"></a>间隔最大化</h2><p><font color=\"red\">支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面</font></p>\n<h3 id=\"最大间隔分离超平面\"><a href=\"#最大间隔分离超平面\" class=\"headerlink\" title=\"最大间隔分离超平面\"></a>最大间隔分离超平面</h3><p>最大几何间隔分离超平面，表示为下面的约束最优化问题</p>\n<script type=\"math/tex; mode=display\">\\max\\limits_{w,b} \\qquad \\gamma</script><script type=\"math/tex; mode=display\">s.t \\qquad y_i{\\frac{w}{||w||}*x_i+\\frac{b}{||w||}}\\geq\\gamma,\\qquad i=1,2,3,……m</script><p>考虑到几何间隔与函数间隔的关系式，这个问题可写为</p>\n<script type=\"math/tex; mode=display\">\\max\\limits_{w,b} \\qquad \\frac{\\hat{\\gamma}}{||w||}</script><script type=\"math/tex; mode=display\">s.t. \\qquad y_i(w*x_i+b)\\geq\\hat{\\gamma},\\qquad i=1,2,3……m</script><p>函数间隔\\(\\hat{\\gamma}\\)的取值不影响最优化问题的解。事实上，假设将w和b按比例的改变为\\(\\lambda w\\)和\\(\\lambda b\\)，此时函数间隔成为\\(\\lambda \\hat{\\gamma}\\)函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，他产生了一个等价的最优化问题，这样取\\(\\hat{\\gamma}\\)=1，代入上式，且最大化\\(\\frac{1}{||w||}\\)和最小化\\(1/2 ||w||^2 \\)是等价的，于是得到一下最优化问题</p>\n<script type=\"math/tex; mode=display\">\\min\\limits_{w,b}\\qquad\\frac{1}{2}||w||^2</script><script type=\"math/tex; mode=display\">s.t. \\qquad y_i(w*x_i)-1\\geq0 \\qquad i=1,2,……m</script><h3 id=\"学习的对偶算法\"><a href=\"#学习的对偶算法\" class=\"headerlink\" title=\"学习的对偶算法\"></a>学习的对偶算法</h3><p><a href=\"#duiou\"><font color=\"red\">关于拉格朗日对偶性数学推导看这里</font></a><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152008.jpg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152013.jpg?imageMogr2/auto-orient\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152017.jpg\" width=\"640\"></p>\n<h2 id=\"对偶问题\"><a href=\"#对偶问题\" class=\"headerlink\" title=\"对偶问题\"></a>对偶问题</h2><p>我们希望求解上式得到最大间隔划分超平面模型</p>\n<script type=\"math/tex; mode=display\">f(x)=w^Tx+b</script><p>(1)式是一个凸二次规划问题，<a href=\"#zuiyou\"><font color=\"red\">一些最优化知识</font></a>使用拉格朗日乘子法得到其对偶问题，该问题的拉格朗日函数可写为</p>\n<script type=\"math/tex; mode=display\">L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum ^m_{i=1}\\alpha_i(1-y_i(w^Tx_i+b)) \\qquad (2)</script><p>其中\\(\\alpha=(\\alpha_1；……\\alpha_m)\\)，令L(w,b,a)对w和b的偏导为0得到</p>\n<script type=\"math/tex; mode=display\">w=\\sum^m_{i=1}\\alpha_iy_ix_i \\qquad (3)</script><script type=\"math/tex; mode=display\">0=\\sum^m_{i=1}\\alpha_i y_i\\qquad(4)</script><p>把（3）式代入（2）消去w，b。在考虑（4）的约束，就得到（1）的对偶问题</p>\n<script type=\"math/tex; mode=display\">max(\\alpha)=\\sum^m_{i=1}\\alpha_i-1/2 \\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_jx^T_ix_j</script><script type=\"math/tex; mode=display\">s.t \\sum^m_{i=1}\\alpha_iy_i=0</script><script type=\"math/tex; mode=display\">\\alpha_i \\geq 0,\\qquad i=1,2,……m</script><p><div id=\"duiou\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104007.jpg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104013.jpg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104017.jpg\" width=\"640\"></div></p>\n<p><div id=\"zuiyou\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181019202304.jpg\" width=\"640\"></div></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"线性可支持向量机与硬间隔最大化\"><a href=\"#线性可支持向量机与硬间隔最大化\" class=\"headerlink\" title=\"线性可支持向量机与硬间隔最大化\"></a>线性可支持向量机与硬间隔最大化</h1><h2 id=\"线性可支持向量机\"><a href=\"#线性可支持向量机\" class=\"headerlink\" title=\"线性可支持向量机\"></a>线性可支持向量机</h2><p>训练集\\(D={(x_1,y_1),(x_2,y_2)……(x_m,y_m)},\\qquad y_i \\in {-1,+1}\\)能将样本分开的超平面有很多，我们应该用哪个<br><img src=\"http://pgmz9e1an.bkt.clouddn.com/QQ%E6%88%AA%E5%9B%BE20181020000158.png\" width=\"640\"><br>在样本空间中，划分超平面可通过如下线性方程来描述</p>\n<script type=\"math/tex; mode=display\">w^Tx+b=0</script><p>其中w是法向量，决定了超平面的方向，b为位移项，决定了超平面和原点之间的距离，显然超平面可被法向量w和位移确定。<br>假设超平面（w,b）能将训练样本正确分类，即对于\\((x_i,y_i) \\in D\\),若yi=+1，则有\\(w^T+b&gt;0\\),若yi=-1，则小于0</p>\n<h2 id=\"函数间隔与几何间隔\"><a href=\"#函数间隔与几何间隔\" class=\"headerlink\" title=\"函数间隔与几何间隔\"></a>函数间隔与几何间隔</h2><h3 id=\"函数间隔\"><a href=\"#函数间隔\" class=\"headerlink\" title=\"函数间隔\"></a>函数间隔</h3><p>对于给定的训练集D和超平面（w,b）定义超平面（w,b）关于样本点(xi,yi)的函数间隔为</p>\n<script type=\"math/tex; mode=display\">\\hat{\\gamma}_i=y_i(w*x_i+b)</script><p>定义超平面（w,b）关于数据集D的函数间隔为超平面（w,b）关于D中所有样本点(xi,yi)的函数间隔的最小值，即</p>\n<script type=\"math/tex; mode=display\">\\hat{\\gamma}=\\min\\limits_{(i=1,2……m)}\\hat{\\gamma}_i</script><p>函数间隔可以表示分类预测的正确性和确信度，但是选择分离超平面时，只有函数间隔还不够，因为我们只要成比例的改变w和b。超平面并没有改变，但函数间隔却变成之前的比例倍</p>\n<h3 id=\"几何间隔\"><a href=\"#几何间隔\" class=\"headerlink\" title=\"几何间隔\"></a>几何间隔</h3><p>对于给定的训练数据集T和超平面（w,b）定义超平面关于样本点(xi,yi)的几何间隔为</p>\n<script type=\"math/tex; mode=display\">\\gamma_i=y_i(\\frac{w}{||w||}*x_i+\\frac{b}{||w||})</script><p>定义超平面关于样本集D的几何间隔为</p>\n<script type=\"math/tex; mode=display\">\\gamma=\\min\\limits_{i=1,2……m}\\gamma_i</script><font color=\"red\">超平面（w,b）关于样本点(xi,yi)的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离</font>\n\n<p>从函数间隔与几何间隔定义，可知以下关系式</p>\n<script type=\"math/tex; mode=display\">\\gamma=\\frac{\\hat{\\gamma}}{||w||}</script><h2 id=\"间隔最大化\"><a href=\"#间隔最大化\" class=\"headerlink\" title=\"间隔最大化\"></a>间隔最大化</h2><p><font color=\"red\">支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面</font></p>\n<h3 id=\"最大间隔分离超平面\"><a href=\"#最大间隔分离超平面\" class=\"headerlink\" title=\"最大间隔分离超平面\"></a>最大间隔分离超平面</h3><p>最大几何间隔分离超平面，表示为下面的约束最优化问题</p>\n<script type=\"math/tex; mode=display\">\\max\\limits_{w,b} \\qquad \\gamma</script><script type=\"math/tex; mode=display\">s.t \\qquad y_i{\\frac{w}{||w||}*x_i+\\frac{b}{||w||}}\\geq\\gamma,\\qquad i=1,2,3,……m</script><p>考虑到几何间隔与函数间隔的关系式，这个问题可写为</p>\n<script type=\"math/tex; mode=display\">\\max\\limits_{w,b} \\qquad \\frac{\\hat{\\gamma}}{||w||}</script><script type=\"math/tex; mode=display\">s.t. \\qquad y_i(w*x_i+b)\\geq\\hat{\\gamma},\\qquad i=1,2,3……m</script><p>函数间隔\\(\\hat{\\gamma}\\)的取值不影响最优化问题的解。事实上，假设将w和b按比例的改变为\\(\\lambda w\\)和\\(\\lambda b\\)，此时函数间隔成为\\(\\lambda \\hat{\\gamma}\\)函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，他产生了一个等价的最优化问题，这样取\\(\\hat{\\gamma}\\)=1，代入上式，且最大化\\(\\frac{1}{||w||}\\)和最小化\\(1/2 ||w||^2 \\)是等价的，于是得到一下最优化问题</p>\n<script type=\"math/tex; mode=display\">\\min\\limits_{w,b}\\qquad\\frac{1}{2}||w||^2</script><script type=\"math/tex; mode=display\">s.t. \\qquad y_i(w*x_i)-1\\geq0 \\qquad i=1,2,……m</script><h3 id=\"学习的对偶算法\"><a href=\"#学习的对偶算法\" class=\"headerlink\" title=\"学习的对偶算法\"></a>学习的对偶算法</h3><p><a href=\"#duiou\"><font color=\"red\">关于拉格朗日对偶性数学推导看这里</font></a><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152008.jpg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152013.jpg?imageMogr2/auto-orient\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181024152017.jpg\" width=\"640\"></p>\n<h2 id=\"对偶问题\"><a href=\"#对偶问题\" class=\"headerlink\" title=\"对偶问题\"></a>对偶问题</h2><p>我们希望求解上式得到最大间隔划分超平面模型</p>\n<script type=\"math/tex; mode=display\">f(x)=w^Tx+b</script><p>(1)式是一个凸二次规划问题，<a href=\"#zuiyou\"><font color=\"red\">一些最优化知识</font></a>使用拉格朗日乘子法得到其对偶问题，该问题的拉格朗日函数可写为</p>\n<script type=\"math/tex; mode=display\">L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum ^m_{i=1}\\alpha_i(1-y_i(w^Tx_i+b)) \\qquad (2)</script><p>其中\\(\\alpha=(\\alpha_1；……\\alpha_m)\\)，令L(w,b,a)对w和b的偏导为0得到</p>\n<script type=\"math/tex; mode=display\">w=\\sum^m_{i=1}\\alpha_iy_ix_i \\qquad (3)</script><script type=\"math/tex; mode=display\">0=\\sum^m_{i=1}\\alpha_i y_i\\qquad(4)</script><p>把（3）式代入（2）消去w，b。在考虑（4）的约束，就得到（1）的对偶问题</p>\n<script type=\"math/tex; mode=display\">max(\\alpha)=\\sum^m_{i=1}\\alpha_i-1/2 \\sum^m_{i=1}\\sum^m_{j=1}\\alpha_i\\alpha_jy_iy_jx^T_ix_j</script><script type=\"math/tex; mode=display\">s.t \\sum^m_{i=1}\\alpha_iy_i=0</script><script type=\"math/tex; mode=display\">\\alpha_i \\geq 0,\\qquad i=1,2,……m</script><p><div id=\"duiou\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104007.jpg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104013.jpg\" width=\"640\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181025104017.jpg\" width=\"640\"></div></p>\n<p><div id=\"zuiyou\"><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181019202304.jpg\" width=\"640\"></div></p>\n"},{"title":"神经网络","date":"2018-11-08T11:02:07.000Z","_content":"# 感知机 #\n\n","source":"_posts/神经网络.md","raw":"---\ntitle: 神经网络\ncategories:\n  - 机器学习\ndate: 2018-11-08 19:02:07\ntags:\n---\n# 感知机 #\n\n","slug":"神经网络","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwwq000nm4v5azfgg8z1","content":"<h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"感知机\"><a href=\"#感知机\" class=\"headerlink\" title=\"感知机\"></a>感知机</h1>"},{"title":"线性模型","date":"2018-10-11T12:10:07.000Z","_content":"# 1.基本形式 #\n给定有d个属性描述的示例\\\\(x=(x_1:x_2……x_d)\\\\),其中Xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即\n$$ f(x)=w_1x_1+w_2x_2+……+w_dx_d+b $$\n一般向量形式写成\n$$ f(x)=w^Tx+b $$\n# 2.线性回归 #\n## 2.1 x只有一种属性 ##\n我们先考一种最简单的情况，x只有一种属性，即假设函数为\n$$ f(x_i)=wx_i+b $$\n选择均方误差作为性能度量，试图使方差最下，即\n$$ (w^*,b^*)=arg min(w,b)=\\Sigma^m_{i=1}(f(x_i)-y_i)^2 $$\n$$ =arg min(w,b)\\Sigma^m_{i=1}(y_i-wx_i-b)^2 $$ \n基于均方误差进行模型求解的方法称为最小二乘法，我们将损失函数分别对w和b求导得\n<a href=\"#ercheng\" ><font color=red>接下来点击这里</font></a>\n## 2.2 x有d个属性\n此时我们试图学得\n$$ f(x_i)=w^Tx_i+b  \\qquad make \\qquad f(x_i) \\approx y_i $$\n这称为“多元线性回归”\n\n类似的，可以用最小二乘法对w和b进行估计，我们把w和b吸收入向量形式\\\\( w^+=(w;b) \\\\),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，我们需要求如下式子\n$$ w^(+*)=arg min(w^+)(y-Xw^+)^T(y-Xw^+) $$\n上式对w求导，得到\n$$ \\frac{ \\partial E_w}{\\partial w}=2X^T(Xw-y)$$\n<a href=\"#qiudao\" ><font color=red>这一步的求解参考这里</font></a>\n \n1. 当\\\\(X^TX\\\\)为满秩矩阵或正定矩阵时，上式为0可得\n$$ w^*=(X^TX)^{-1}X^Ty $$\n2. 如果不是，那么可能解出多个w，此时由学习算法的归纳偏好决定，常见的做法是引入正则化\n\n# 3.对数几率回归 #\n我们使用sigmod函数作为“广义线性模型”的单调可微函数g（.），得到\n$$ y=\\frac {1}{1+e^{-(w^T x+b)}} \\qquad (1)$$\n上式可变换为\n$$ ln \\frac{y}{1-y}=w^Tx+b \\qquad (2) $$\n若将y视为样本x作为正例的可能性，则1-y是其反例的可能性。两者的比值\\\\( \\frac{y}{1-y} \\\\)称为几率，反映了x作为正例的相对可能性，对几率去对数得到对数几率\n$$ ln \\frac {y}{1-y}    $$ \n\n由（2）式可知，实际上是用线性回归模型的预测结果去逼近真是标记的对数几率，其模型称为“**对数几率回归**”\n\n我们来看如何求w和b，我们把（1）式的y视为类后验概率估计p(y=1|x)，在（2）式重写为\n\n$$ ln \\frac {p(y=1|x)}{p(y=0|x)} =w^Tx+b $$\n\n显然有\n$$ p(y=1|x)=\\frac{e^{w^T x+b}}{1+e^{w^T x+b}} $$\n$$ p(y=0|x)=\\frac{1}{1+e^{w^T x+b}} $$\n对于给定的数据集\\\\( {(x\\_i,y\\_i)}^N\\_{i=1},y \\in {0,1} \\\\),可以应用极大似然估计估计模型参数，从而得到逻辑回归模型\n设P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)\n似然函数为\n$$ \\prod^N_{i=1} [g(x_i)]^y_i [1-g(x_i)]^{1-y_i} $$\n对数似然函数为\n$$ L(w)=\\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))] $$\n$$ =\\Sigma^N_{i=1}[y_ilog \\frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))] $$\n$$ =\\Sigma^N_{i=1}[y_i (w*x_i)-log(1+\\exp(w \\ast x_i))] $$\n对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决\n\n\n\n\n\n<br/>\n<div id='ercheng'/>\n - **西瓜书 54页3.7和3.8推导公式**\n\n<img src=\"/images/p54.jpg\" width=480 align=center/>\n\n - **西瓜书59页 3.27推导公式**\n\n<img src=\"/images/p59.jpg\" width=480 align=center/>\n- **p55 3.10**\n<div id=\"qiudao\"><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181015204258.jpg\" width=480 align=center/></div>\n** 算法形式**\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181021150851.jpg\" width=\"640\"/>","source":"_posts/线性回归.md","raw":"title: 线性模型\ncategories:\n  - 机器学习\ntags:\n  - 线性回归\n  - 对数几率回归\ndate: 2018-10-11 20:10:07\n---\n# 1.基本形式 #\n给定有d个属性描述的示例\\\\(x=(x_1:x_2……x_d)\\\\),其中Xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即\n$$ f(x)=w_1x_1+w_2x_2+……+w_dx_d+b $$\n一般向量形式写成\n$$ f(x)=w^Tx+b $$\n# 2.线性回归 #\n## 2.1 x只有一种属性 ##\n我们先考一种最简单的情况，x只有一种属性，即假设函数为\n$$ f(x_i)=wx_i+b $$\n选择均方误差作为性能度量，试图使方差最下，即\n$$ (w^*,b^*)=arg min(w,b)=\\Sigma^m_{i=1}(f(x_i)-y_i)^2 $$\n$$ =arg min(w,b)\\Sigma^m_{i=1}(y_i-wx_i-b)^2 $$ \n基于均方误差进行模型求解的方法称为最小二乘法，我们将损失函数分别对w和b求导得\n<a href=\"#ercheng\" ><font color=red>接下来点击这里</font></a>\n## 2.2 x有d个属性\n此时我们试图学得\n$$ f(x_i)=w^Tx_i+b  \\qquad make \\qquad f(x_i) \\approx y_i $$\n这称为“多元线性回归”\n\n类似的，可以用最小二乘法对w和b进行估计，我们把w和b吸收入向量形式\\\\( w^+=(w;b) \\\\),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，我们需要求如下式子\n$$ w^(+*)=arg min(w^+)(y-Xw^+)^T(y-Xw^+) $$\n上式对w求导，得到\n$$ \\frac{ \\partial E_w}{\\partial w}=2X^T(Xw-y)$$\n<a href=\"#qiudao\" ><font color=red>这一步的求解参考这里</font></a>\n \n1. 当\\\\(X^TX\\\\)为满秩矩阵或正定矩阵时，上式为0可得\n$$ w^*=(X^TX)^{-1}X^Ty $$\n2. 如果不是，那么可能解出多个w，此时由学习算法的归纳偏好决定，常见的做法是引入正则化\n\n# 3.对数几率回归 #\n我们使用sigmod函数作为“广义线性模型”的单调可微函数g（.），得到\n$$ y=\\frac {1}{1+e^{-(w^T x+b)}} \\qquad (1)$$\n上式可变换为\n$$ ln \\frac{y}{1-y}=w^Tx+b \\qquad (2) $$\n若将y视为样本x作为正例的可能性，则1-y是其反例的可能性。两者的比值\\\\( \\frac{y}{1-y} \\\\)称为几率，反映了x作为正例的相对可能性，对几率去对数得到对数几率\n$$ ln \\frac {y}{1-y}    $$ \n\n由（2）式可知，实际上是用线性回归模型的预测结果去逼近真是标记的对数几率，其模型称为“**对数几率回归**”\n\n我们来看如何求w和b，我们把（1）式的y视为类后验概率估计p(y=1|x)，在（2）式重写为\n\n$$ ln \\frac {p(y=1|x)}{p(y=0|x)} =w^Tx+b $$\n\n显然有\n$$ p(y=1|x)=\\frac{e^{w^T x+b}}{1+e^{w^T x+b}} $$\n$$ p(y=0|x)=\\frac{1}{1+e^{w^T x+b}} $$\n对于给定的数据集\\\\( {(x\\_i,y\\_i)}^N\\_{i=1},y \\in {0,1} \\\\),可以应用极大似然估计估计模型参数，从而得到逻辑回归模型\n设P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)\n似然函数为\n$$ \\prod^N_{i=1} [g(x_i)]^y_i [1-g(x_i)]^{1-y_i} $$\n对数似然函数为\n$$ L(w)=\\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))] $$\n$$ =\\Sigma^N_{i=1}[y_ilog \\frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))] $$\n$$ =\\Sigma^N_{i=1}[y_i (w*x_i)-log(1+\\exp(w \\ast x_i))] $$\n对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决\n\n\n\n\n\n<br/>\n<div id='ercheng'/>\n - **西瓜书 54页3.7和3.8推导公式**\n\n<img src=\"/images/p54.jpg\" width=480 align=center/>\n\n - **西瓜书59页 3.27推导公式**\n\n<img src=\"/images/p59.jpg\" width=480 align=center/>\n- **p55 3.10**\n<div id=\"qiudao\"><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181015204258.jpg\" width=480 align=center/></div>\n** 算法形式**\n<img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181021150851.jpg\" width=\"640\"/>","slug":"线性回归","published":1,"updated":"2018-10-22T04:36:50.920Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwwr000om4v5hlgye09m","content":"<h1 id=\"1-基本形式\"><a href=\"#1-基本形式\" class=\"headerlink\" title=\"1.基本形式\"></a>1.基本形式</h1><p>给定有d个属性描述的示例\\(x=(x_1:x_2……x_d)\\),其中Xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即</p>\n<script type=\"math/tex; mode=display\">f(x)=w_1x_1+w_2x_2+……+w_dx_d+b</script><p>一般向量形式写成</p>\n<script type=\"math/tex; mode=display\">f(x)=w^Tx+b</script><h1 id=\"2-线性回归\"><a href=\"#2-线性回归\" class=\"headerlink\" title=\"2.线性回归\"></a>2.线性回归</h1><h2 id=\"2-1-x只有一种属性\"><a href=\"#2-1-x只有一种属性\" class=\"headerlink\" title=\"2.1 x只有一种属性\"></a>2.1 x只有一种属性</h2><p>我们先考一种最简单的情况，x只有一种属性，即假设函数为</p>\n<script type=\"math/tex; mode=display\">f(x_i)=wx_i+b</script><p>选择均方误差作为性能度量，试图使方差最下，即</p>\n<script type=\"math/tex; mode=display\">(w^*,b^*)=arg min(w,b)=\\Sigma^m_{i=1}(f(x_i)-y_i)^2</script><script type=\"math/tex; mode=display\">=arg min(w,b)\\Sigma^m_{i=1}(y_i-wx_i-b)^2</script><p>基于均方误差进行模型求解的方法称为最小二乘法，我们将损失函数分别对w和b求导得<br><a href=\"#ercheng\"><font color=\"red\">接下来点击这里</font></a></p>\n<h2 id=\"2-2-x有d个属性\"><a href=\"#2-2-x有d个属性\" class=\"headerlink\" title=\"2.2 x有d个属性\"></a>2.2 x有d个属性</h2><p>此时我们试图学得</p>\n<script type=\"math/tex; mode=display\">f(x_i)=w^Tx_i+b  \\qquad make \\qquad f(x_i) \\approx y_i</script><p>这称为“多元线性回归”</p>\n<p>类似的，可以用最小二乘法对w和b进行估计，我们把w和b吸收入向量形式\\( w^+=(w;b) \\),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，我们需要求如下式子</p>\n<script type=\"math/tex; mode=display\">w^(+*)=arg min(w^+)(y-Xw^+)^T(y-Xw^+)</script><p>上式对w求导，得到</p>\n<script type=\"math/tex; mode=display\">\\frac{ \\partial E_w}{\\partial w}=2X^T(Xw-y)</script><p><a href=\"#qiudao\"><font color=\"red\">这一步的求解参考这里</font></a></p>\n<ol>\n<li>当\\(X^TX\\)为满秩矩阵或正定矩阵时，上式为0可得<script type=\"math/tex; mode=display\">w^*=(X^TX)^{-1}X^Ty</script></li>\n<li>如果不是，那么可能解出多个w，此时由学习算法的归纳偏好决定，常见的做法是引入正则化</li>\n</ol>\n<h1 id=\"3-对数几率回归\"><a href=\"#3-对数几率回归\" class=\"headerlink\" title=\"3.对数几率回归\"></a>3.对数几率回归</h1><p>我们使用sigmod函数作为“广义线性模型”的单调可微函数g（.），得到</p>\n<script type=\"math/tex; mode=display\">y=\\frac {1}{1+e^{-(w^T x+b)}} \\qquad (1)</script><p>上式可变换为</p>\n<script type=\"math/tex; mode=display\">ln \\frac{y}{1-y}=w^Tx+b \\qquad (2)</script><p>若将y视为样本x作为正例的可能性，则1-y是其反例的可能性。两者的比值\\( \\frac{y}{1-y} \\)称为几率，反映了x作为正例的相对可能性，对几率去对数得到对数几率</p>\n<script type=\"math/tex; mode=display\">ln \\frac {y}{1-y}</script><p>由（2）式可知，实际上是用线性回归模型的预测结果去逼近真是标记的对数几率，其模型称为“<strong>对数几率回归</strong>”</p>\n<p>我们来看如何求w和b，我们把（1）式的y视为类后验概率估计p(y=1|x)，在（2）式重写为</p>\n<script type=\"math/tex; mode=display\">ln \\frac {p(y=1|x)}{p(y=0|x)} =w^Tx+b</script><p>显然有</p>\n<script type=\"math/tex; mode=display\">p(y=1|x)=\\frac{e^{w^T x+b}}{1+e^{w^T x+b}}</script><script type=\"math/tex; mode=display\">p(y=0|x)=\\frac{1}{1+e^{w^T x+b}}</script><p>对于给定的数据集\\( {(x_i,y_i)}^N_{i=1},y \\in {0,1} \\),可以应用极大似然估计估计模型参数，从而得到逻辑回归模型<br>设P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)<br>似然函数为</p>\n<script type=\"math/tex; mode=display\">\\prod^N_{i=1} [g(x_i)]^y_i [1-g(x_i)]^{1-y_i}</script><p>对数似然函数为</p>\n<script type=\"math/tex; mode=display\">L(w)=\\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))]</script><script type=\"math/tex; mode=display\">=\\Sigma^N_{i=1}[y_ilog \\frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))]</script><script type=\"math/tex; mode=display\">=\\Sigma^N_{i=1}[y_i (w*x_i)-log(1+\\exp(w \\ast x_i))]</script><p>对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决</p>\n<p><br></p>\n<p><div id=\"ercheng\"></div></p>\n<ul>\n<li><strong>西瓜书 54页3.7和3.8推导公式</strong></li>\n</ul>\n<p><img src=\"/images/p54.jpg\" width=\"480\" align=\"center/\"></p>\n<ul>\n<li><strong>西瓜书59页 3.27推导公式</strong></li>\n</ul>\n<p><img src=\"/images/p59.jpg\" width=\"480\" align=\"center/\"></p>\n<ul>\n<li><strong>p55 3.10</strong><br><div id=\"qiudao\"><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181015204258.jpg\" width=\"480\" align=\"center/\"></div><br><strong> 算法形式</strong><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181021150851.jpg\" width=\"640\"></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"1-基本形式\"><a href=\"#1-基本形式\" class=\"headerlink\" title=\"1.基本形式\"></a>1.基本形式</h1><p>给定有d个属性描述的示例\\(x=(x_1:x_2……x_d)\\),其中Xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即</p>\n<script type=\"math/tex; mode=display\">f(x)=w_1x_1+w_2x_2+……+w_dx_d+b</script><p>一般向量形式写成</p>\n<script type=\"math/tex; mode=display\">f(x)=w^Tx+b</script><h1 id=\"2-线性回归\"><a href=\"#2-线性回归\" class=\"headerlink\" title=\"2.线性回归\"></a>2.线性回归</h1><h2 id=\"2-1-x只有一种属性\"><a href=\"#2-1-x只有一种属性\" class=\"headerlink\" title=\"2.1 x只有一种属性\"></a>2.1 x只有一种属性</h2><p>我们先考一种最简单的情况，x只有一种属性，即假设函数为</p>\n<script type=\"math/tex; mode=display\">f(x_i)=wx_i+b</script><p>选择均方误差作为性能度量，试图使方差最下，即</p>\n<script type=\"math/tex; mode=display\">(w^*,b^*)=arg min(w,b)=\\Sigma^m_{i=1}(f(x_i)-y_i)^2</script><script type=\"math/tex; mode=display\">=arg min(w,b)\\Sigma^m_{i=1}(y_i-wx_i-b)^2</script><p>基于均方误差进行模型求解的方法称为最小二乘法，我们将损失函数分别对w和b求导得<br><a href=\"#ercheng\"><font color=\"red\">接下来点击这里</font></a></p>\n<h2 id=\"2-2-x有d个属性\"><a href=\"#2-2-x有d个属性\" class=\"headerlink\" title=\"2.2 x有d个属性\"></a>2.2 x有d个属性</h2><p>此时我们试图学得</p>\n<script type=\"math/tex; mode=display\">f(x_i)=w^Tx_i+b  \\qquad make \\qquad f(x_i) \\approx y_i</script><p>这称为“多元线性回归”</p>\n<p>类似的，可以用最小二乘法对w和b进行估计，我们把w和b吸收入向量形式\\( w^+=(w;b) \\),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，我们需要求如下式子</p>\n<script type=\"math/tex; mode=display\">w^(+*)=arg min(w^+)(y-Xw^+)^T(y-Xw^+)</script><p>上式对w求导，得到</p>\n<script type=\"math/tex; mode=display\">\\frac{ \\partial E_w}{\\partial w}=2X^T(Xw-y)</script><p><a href=\"#qiudao\"><font color=\"red\">这一步的求解参考这里</font></a></p>\n<ol>\n<li>当\\(X^TX\\)为满秩矩阵或正定矩阵时，上式为0可得<script type=\"math/tex; mode=display\">w^*=(X^TX)^{-1}X^Ty</script></li>\n<li>如果不是，那么可能解出多个w，此时由学习算法的归纳偏好决定，常见的做法是引入正则化</li>\n</ol>\n<h1 id=\"3-对数几率回归\"><a href=\"#3-对数几率回归\" class=\"headerlink\" title=\"3.对数几率回归\"></a>3.对数几率回归</h1><p>我们使用sigmod函数作为“广义线性模型”的单调可微函数g（.），得到</p>\n<script type=\"math/tex; mode=display\">y=\\frac {1}{1+e^{-(w^T x+b)}} \\qquad (1)</script><p>上式可变换为</p>\n<script type=\"math/tex; mode=display\">ln \\frac{y}{1-y}=w^Tx+b \\qquad (2)</script><p>若将y视为样本x作为正例的可能性，则1-y是其反例的可能性。两者的比值\\( \\frac{y}{1-y} \\)称为几率，反映了x作为正例的相对可能性，对几率去对数得到对数几率</p>\n<script type=\"math/tex; mode=display\">ln \\frac {y}{1-y}</script><p>由（2）式可知，实际上是用线性回归模型的预测结果去逼近真是标记的对数几率，其模型称为“<strong>对数几率回归</strong>”</p>\n<p>我们来看如何求w和b，我们把（1）式的y视为类后验概率估计p(y=1|x)，在（2）式重写为</p>\n<script type=\"math/tex; mode=display\">ln \\frac {p(y=1|x)}{p(y=0|x)} =w^Tx+b</script><p>显然有</p>\n<script type=\"math/tex; mode=display\">p(y=1|x)=\\frac{e^{w^T x+b}}{1+e^{w^T x+b}}</script><script type=\"math/tex; mode=display\">p(y=0|x)=\\frac{1}{1+e^{w^T x+b}}</script><p>对于给定的数据集\\( {(x_i,y_i)}^N_{i=1},y \\in {0,1} \\),可以应用极大似然估计估计模型参数，从而得到逻辑回归模型<br>设P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)<br>似然函数为</p>\n<script type=\"math/tex; mode=display\">\\prod^N_{i=1} [g(x_i)]^y_i [1-g(x_i)]^{1-y_i}</script><p>对数似然函数为</p>\n<script type=\"math/tex; mode=display\">L(w)=\\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))]</script><script type=\"math/tex; mode=display\">=\\Sigma^N_{i=1}[y_ilog \\frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))]</script><script type=\"math/tex; mode=display\">=\\Sigma^N_{i=1}[y_i (w*x_i)-log(1+\\exp(w \\ast x_i))]</script><p>对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决</p>\n<p><br></p>\n<p><div id=\"ercheng\"></div></p>\n<ul>\n<li><strong>西瓜书 54页3.7和3.8推导公式</strong></li>\n</ul>\n<p><img src=\"/images/p54.jpg\" width=\"480\" align=\"center/\"></p>\n<ul>\n<li><strong>西瓜书59页 3.27推导公式</strong></li>\n</ul>\n<p><img src=\"/images/p59.jpg\" width=\"480\" align=\"center/\"></p>\n<ul>\n<li><strong>p55 3.10</strong><br><div id=\"qiudao\"><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181015204258.jpg\" width=\"480\" align=\"center/\"></div><br><strong> 算法形式</strong><br><img src=\"http://pgmz9e1an.bkt.clouddn.com/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20181021150851.jpg\" width=\"640\"></li>\n</ul>\n"},{"title":"贝叶斯分类器","date":"2018-11-08T07:36:37.000Z","_content":"# 朴素贝叶斯法 #\n## 朴素贝叶斯基本原理 ##\n <img src=\"/images/bayes1.jpg\" width=\"640\"/>\n## 朴素贝叶斯基本算法 ##\n<img src=\"/images/bayes2.png\" width=\"640\"/>\n### 处理连续属性 ###\n对连续属性可考虑概率密度函数，假定\\\\(p(x_i|c)~N(\\mu_{c,i},\\sigma^2_{c,i})\\\\),其中\\\\(\\mu_{c,i},\\sigma^2_{c,i}\\\\)分别是第c类样本在第i个属性上取值的均值和平方，则有\n$$ p(x_i|c)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{c,i}} exp(-\\frac{(x_i-\\mu_c,i)^2}{2\\sigma^2_{c,i}}) $$\n### 西瓜书实例 ###\n<img src=\"/images/xishi1.png\" width=\"640\"/>\n<img src=\"/images/xishi2.png\" width=\"640\"/>\n<img src=\"/images/xishi3.png\" width=\"640\"/>\n### 统计学习实例 ###\n<img src=\"/images/tongshi1.png\" width=\"640\"/>\n<img src=\"/images/tongshi2.png\" width=\"640\"/>\n### 拉普拉斯修正 ###\n用极大似然估计可能会出现所要估计的概率值为0的情况，这回影响到后验概率的计算结果，是分类产生偏差。在估计概率时通常进行‘平滑’，常用拉普拉斯修正。具体来说，令N表示训练集D中可能的类别数，Ni表示第i个属性可能的取值数，则\n$$ \\hat P(c)=\\frac{|D_c|+1}{|D|+N} $$\n$$ \\hat P(x_i|c)=\\frac{|D_{c,x_i}|+1}{|D_c|+N_i} $$\n","source":"_posts/贝叶斯分类器.md","raw":"---\ntitle: 贝叶斯分类器\ncategories:\n  - 机器学习\ndate: 2018-11-08 15:36:37\ntags:\n---\n# 朴素贝叶斯法 #\n## 朴素贝叶斯基本原理 ##\n <img src=\"/images/bayes1.jpg\" width=\"640\"/>\n## 朴素贝叶斯基本算法 ##\n<img src=\"/images/bayes2.png\" width=\"640\"/>\n### 处理连续属性 ###\n对连续属性可考虑概率密度函数，假定\\\\(p(x_i|c)~N(\\mu_{c,i},\\sigma^2_{c,i})\\\\),其中\\\\(\\mu_{c,i},\\sigma^2_{c,i}\\\\)分别是第c类样本在第i个属性上取值的均值和平方，则有\n$$ p(x_i|c)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{c,i}} exp(-\\frac{(x_i-\\mu_c,i)^2}{2\\sigma^2_{c,i}}) $$\n### 西瓜书实例 ###\n<img src=\"/images/xishi1.png\" width=\"640\"/>\n<img src=\"/images/xishi2.png\" width=\"640\"/>\n<img src=\"/images/xishi3.png\" width=\"640\"/>\n### 统计学习实例 ###\n<img src=\"/images/tongshi1.png\" width=\"640\"/>\n<img src=\"/images/tongshi2.png\" width=\"640\"/>\n### 拉普拉斯修正 ###\n用极大似然估计可能会出现所要估计的概率值为0的情况，这回影响到后验概率的计算结果，是分类产生偏差。在估计概率时通常进行‘平滑’，常用拉普拉斯修正。具体来说，令N表示训练集D中可能的类别数，Ni表示第i个属性可能的取值数，则\n$$ \\hat P(c)=\\frac{|D_c|+1}{|D|+N} $$\n$$ \\hat P(x_i|c)=\\frac{|D_{c,x_i}|+1}{|D_c|+N_i} $$\n","slug":"贝叶斯分类器","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwwr000qm4v5qg3eajh4","content":"<h1 id=\"朴素贝叶斯法\"><a href=\"#朴素贝叶斯法\" class=\"headerlink\" title=\"朴素贝叶斯法\"></a>朴素贝叶斯法</h1><h2 id=\"朴素贝叶斯基本原理\"><a href=\"#朴素贝叶斯基本原理\" class=\"headerlink\" title=\"朴素贝叶斯基本原理\"></a>朴素贝叶斯基本原理</h2><p> <img src=\"/images/bayes1.jpg\" width=\"640\"></p>\n<h2 id=\"朴素贝叶斯基本算法\"><a href=\"#朴素贝叶斯基本算法\" class=\"headerlink\" title=\"朴素贝叶斯基本算法\"></a>朴素贝叶斯基本算法</h2><p><img src=\"/images/bayes2.png\" width=\"640\"></p>\n<h3 id=\"处理连续属性\"><a href=\"#处理连续属性\" class=\"headerlink\" title=\"处理连续属性\"></a>处理连续属性</h3><p>对连续属性可考虑概率密度函数，假定\\(p(x<em>i|c)~N(\\mu</em>{c,i},\\sigma^2<em>{c,i})\\),其中\\(\\mu</em>{c,i},\\sigma^2_{c,i}\\)分别是第c类样本在第i个属性上取值的均值和平方，则有</p>\n<script type=\"math/tex; mode=display\">p(x_i|c)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{c,i}} exp(-\\frac{(x_i-\\mu_c,i)^2}{2\\sigma^2_{c,i}})</script><h3 id=\"西瓜书实例\"><a href=\"#西瓜书实例\" class=\"headerlink\" title=\"西瓜书实例\"></a>西瓜书实例</h3><p><img src=\"/images/xishi1.png\" width=\"640\"><br><img src=\"/images/xishi2.png\" width=\"640\"><br><img src=\"/images/xishi3.png\" width=\"640\"></p>\n<h3 id=\"统计学习实例\"><a href=\"#统计学习实例\" class=\"headerlink\" title=\"统计学习实例\"></a>统计学习实例</h3><p><img src=\"/images/tongshi1.png\" width=\"640\"><br><img src=\"/images/tongshi2.png\" width=\"640\"></p>\n<h3 id=\"拉普拉斯修正\"><a href=\"#拉普拉斯修正\" class=\"headerlink\" title=\"拉普拉斯修正\"></a>拉普拉斯修正</h3><p>用极大似然估计可能会出现所要估计的概率值为0的情况，这回影响到后验概率的计算结果，是分类产生偏差。在估计概率时通常进行‘平滑’，常用拉普拉斯修正。具体来说，令N表示训练集D中可能的类别数，Ni表示第i个属性可能的取值数，则</p>\n<script type=\"math/tex; mode=display\">\\hat P(c)=\\frac{|D_c|+1}{|D|+N}</script><script type=\"math/tex; mode=display\">\\hat P(x_i|c)=\\frac{|D_{c,x_i}|+1}{|D_c|+N_i}</script>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"朴素贝叶斯法\"><a href=\"#朴素贝叶斯法\" class=\"headerlink\" title=\"朴素贝叶斯法\"></a>朴素贝叶斯法</h1><h2 id=\"朴素贝叶斯基本原理\"><a href=\"#朴素贝叶斯基本原理\" class=\"headerlink\" title=\"朴素贝叶斯基本原理\"></a>朴素贝叶斯基本原理</h2><p> <img src=\"/images/bayes1.jpg\" width=\"640\"></p>\n<h2 id=\"朴素贝叶斯基本算法\"><a href=\"#朴素贝叶斯基本算法\" class=\"headerlink\" title=\"朴素贝叶斯基本算法\"></a>朴素贝叶斯基本算法</h2><p><img src=\"/images/bayes2.png\" width=\"640\"></p>\n<h3 id=\"处理连续属性\"><a href=\"#处理连续属性\" class=\"headerlink\" title=\"处理连续属性\"></a>处理连续属性</h3><p>对连续属性可考虑概率密度函数，假定\\(p(x<em>i|c)~N(\\mu</em>{c,i},\\sigma^2<em>{c,i})\\),其中\\(\\mu</em>{c,i},\\sigma^2_{c,i}\\)分别是第c类样本在第i个属性上取值的均值和平方，则有</p>\n<script type=\"math/tex; mode=display\">p(x_i|c)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{c,i}} exp(-\\frac{(x_i-\\mu_c,i)^2}{2\\sigma^2_{c,i}})</script><h3 id=\"西瓜书实例\"><a href=\"#西瓜书实例\" class=\"headerlink\" title=\"西瓜书实例\"></a>西瓜书实例</h3><p><img src=\"/images/xishi1.png\" width=\"640\"><br><img src=\"/images/xishi2.png\" width=\"640\"><br><img src=\"/images/xishi3.png\" width=\"640\"></p>\n<h3 id=\"统计学习实例\"><a href=\"#统计学习实例\" class=\"headerlink\" title=\"统计学习实例\"></a>统计学习实例</h3><p><img src=\"/images/tongshi1.png\" width=\"640\"><br><img src=\"/images/tongshi2.png\" width=\"640\"></p>\n<h3 id=\"拉普拉斯修正\"><a href=\"#拉普拉斯修正\" class=\"headerlink\" title=\"拉普拉斯修正\"></a>拉普拉斯修正</h3><p>用极大似然估计可能会出现所要估计的概率值为0的情况，这回影响到后验概率的计算结果，是分类产生偏差。在估计概率时通常进行‘平滑’，常用拉普拉斯修正。具体来说，令N表示训练集D中可能的类别数，Ni表示第i个属性可能的取值数，则</p>\n<script type=\"math/tex; mode=display\">\\hat P(c)=\\frac{|D_c|+1}{|D|+N}</script><script type=\"math/tex; mode=display\">\\hat P(x_i|c)=\\frac{|D_{c,x_i}|+1}{|D_c|+N_i}</script>"},{"title":"模型评估与选择","date":"2018-11-08T09:55:49.000Z","_content":"# 评估方法 #\n## 留出法 ##\n将数据集D分成两个互斥的子集，一个作为训练集，一个作为测试集\n## 交叉验证法 ##\n交叉验证法先将数据集分成k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性。\n\n然后每次使用k-1个的子集作为训练集，剩下那个作为测试集，可获得k组训练集/测试集。最后去均值\n\n## 自助法 ##\n直接以自助采样法作为基础（bootsrap）\n\n对于数据样本个数为m的数据集D，对他采样产生D‘。详细规则如下：每次随机从D中抽一个样本放入D’，并将这个样本再放回去D，保证它下次还可能被抽到，重复m次得到包含m个样本的D‘。显然，D中一部分样本会在D’出现，一部分不会，样本m次采样都没有被采到的概率是\\\\((1-\\frac{1}{m})^m\\\\),取极限得到\n$$ \\lim_{m->\\infty}(1-\\frac{1}{m})^m \\approx 0.368 $$\n我们通过自助采样，数据集D仍有36.8%的样本不会出现在D‘中，我们用D’做训练集，D-D‘作为测试集。\n\n自助法在数据集较小，难以有效划分训练测试集时很有用。\n\n自助法改变了数据集的分布，会引入估计偏差，因此数据量充足时，留出法和交叉验证更常用\n","source":"_posts/模型评估与选择.md","raw":"---\ntitle: 模型评估与选择\ncategories:\n  - 机器学习\ndate: 2018-11-08 17:55:49\ntags:\n---\n# 评估方法 #\n## 留出法 ##\n将数据集D分成两个互斥的子集，一个作为训练集，一个作为测试集\n## 交叉验证法 ##\n交叉验证法先将数据集分成k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性。\n\n然后每次使用k-1个的子集作为训练集，剩下那个作为测试集，可获得k组训练集/测试集。最后去均值\n\n## 自助法 ##\n直接以自助采样法作为基础（bootsrap）\n\n对于数据样本个数为m的数据集D，对他采样产生D‘。详细规则如下：每次随机从D中抽一个样本放入D’，并将这个样本再放回去D，保证它下次还可能被抽到，重复m次得到包含m个样本的D‘。显然，D中一部分样本会在D’出现，一部分不会，样本m次采样都没有被采到的概率是\\\\((1-\\frac{1}{m})^m\\\\),取极限得到\n$$ \\lim_{m->\\infty}(1-\\frac{1}{m})^m \\approx 0.368 $$\n我们通过自助采样，数据集D仍有36.8%的样本不会出现在D‘中，我们用D’做训练集，D-D‘作为测试集。\n\n自助法在数据集较小，难以有效划分训练测试集时很有用。\n\n自助法改变了数据集的分布，会引入估计偏差，因此数据量充足时，留出法和交叉验证更常用\n","slug":"模型评估与选择","published":1,"updated":"2018-11-14T14:58:48.352Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjohajwwr000tm4v5hfglmuj1","content":"<h1 id=\"评估方法\"><a href=\"#评估方法\" class=\"headerlink\" title=\"评估方法\"></a>评估方法</h1><h2 id=\"留出法\"><a href=\"#留出法\" class=\"headerlink\" title=\"留出法\"></a>留出法</h2><p>将数据集D分成两个互斥的子集，一个作为训练集，一个作为测试集</p>\n<h2 id=\"交叉验证法\"><a href=\"#交叉验证法\" class=\"headerlink\" title=\"交叉验证法\"></a>交叉验证法</h2><p>交叉验证法先将数据集分成k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性。</p>\n<p>然后每次使用k-1个的子集作为训练集，剩下那个作为测试集，可获得k组训练集/测试集。最后去均值</p>\n<h2 id=\"自助法\"><a href=\"#自助法\" class=\"headerlink\" title=\"自助法\"></a>自助法</h2><p>直接以自助采样法作为基础（bootsrap）</p>\n<p>对于数据样本个数为m的数据集D，对他采样产生D‘。详细规则如下：每次随机从D中抽一个样本放入D’，并将这个样本再放回去D，保证它下次还可能被抽到，重复m次得到包含m个样本的D‘。显然，D中一部分样本会在D’出现，一部分不会，样本m次采样都没有被采到的概率是\\((1-\\frac{1}{m})^m\\),取极限得到</p>\n<script type=\"math/tex; mode=display\">\\lim_{m->\\infty}(1-\\frac{1}{m})^m \\approx 0.368</script><p>我们通过自助采样，数据集D仍有36.8%的样本不会出现在D‘中，我们用D’做训练集，D-D‘作为测试集。</p>\n<p>自助法在数据集较小，难以有效划分训练测试集时很有用。</p>\n<p>自助法改变了数据集的分布，会引入估计偏差，因此数据量充足时，留出法和交叉验证更常用</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"评估方法\"><a href=\"#评估方法\" class=\"headerlink\" title=\"评估方法\"></a>评估方法</h1><h2 id=\"留出法\"><a href=\"#留出法\" class=\"headerlink\" title=\"留出法\"></a>留出法</h2><p>将数据集D分成两个互斥的子集，一个作为训练集，一个作为测试集</p>\n<h2 id=\"交叉验证法\"><a href=\"#交叉验证法\" class=\"headerlink\" title=\"交叉验证法\"></a>交叉验证法</h2><p>交叉验证法先将数据集分成k个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性。</p>\n<p>然后每次使用k-1个的子集作为训练集，剩下那个作为测试集，可获得k组训练集/测试集。最后去均值</p>\n<h2 id=\"自助法\"><a href=\"#自助法\" class=\"headerlink\" title=\"自助法\"></a>自助法</h2><p>直接以自助采样法作为基础（bootsrap）</p>\n<p>对于数据样本个数为m的数据集D，对他采样产生D‘。详细规则如下：每次随机从D中抽一个样本放入D’，并将这个样本再放回去D，保证它下次还可能被抽到，重复m次得到包含m个样本的D‘。显然，D中一部分样本会在D’出现，一部分不会，样本m次采样都没有被采到的概率是\\((1-\\frac{1}{m})^m\\),取极限得到</p>\n<script type=\"math/tex; mode=display\">\\lim_{m->\\infty}(1-\\frac{1}{m})^m \\approx 0.368</script><p>我们通过自助采样，数据集D仍有36.8%的样本不会出现在D‘中，我们用D’做训练集，D-D‘作为测试集。</p>\n<p>自助法在数据集较小，难以有效划分训练测试集时很有用。</p>\n<p>自助法改变了数据集的分布，会引入估计偏差，因此数据量充足时，留出法和交叉验证更常用</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjohajwna0004m4v52kbr676b","category_id":"cjohajwn50002m4v52rzhyu55","_id":"cjohajwnj0009m4v5zjs7sbl5"},{"post_id":"cjohajwmt0000m4v5xy55t9yu","category_id":"cjohajwn50002m4v52rzhyu55","_id":"cjohajwnn000bm4v5ifyo3k2u"},{"post_id":"cjohajwn10001m4v5w6mtp4zq","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwnp000dm4v5sv2xg09i"},{"post_id":"cjohajwni0008m4v592kcfkjc","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwnq000em4v51yb76b1x"},{"post_id":"cjohajwn80003m4v53u5kne91","category_id":"cjohajwn50002m4v52rzhyu55","_id":"cjohajwnr000fm4v5fj5twsmb"},{"post_id":"cjohajwnc0005m4v5ghwlcv6b","category_id":"cjohajwnp000cm4v5nchh7zpl","_id":"cjohajwnt000hm4v54hyoul5a"},{"post_id":"cjohajwnf0007m4v5xr80slve","category_id":"cjohajwnp000cm4v5nchh7zpl","_id":"cjohajwnt000im4v5jdw8ajcz"},{"post_id":"cjohajwwe000jm4v57wvlnnli","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwwr000pm4v542taynip"},{"post_id":"cjohajwwl000lm4v5jz01y9v6","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwwr000rm4v58hdxx9l9"},{"post_id":"cjohajwwq000nm4v5azfgg8z1","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwwr000um4v5rea50lv3"},{"post_id":"cjohajwwr000om4v5hlgye09m","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwwr000vm4v5h85tw33i"},{"post_id":"cjohajwwr000qm4v5qg3eajh4","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwwr000xm4v5pk2y5m8j"},{"post_id":"cjohajwwr000tm4v5hfglmuj1","category_id":"cjohajwne0006m4v5n4scmyao","_id":"cjohajwwr000ym4v52c45msm2"}],"PostTag":[{"post_id":"cjohajwwr000om4v5hlgye09m","tag_id":"cjohajwwr000sm4v52lot0eec","_id":"cjohajwwr000zm4v5rv083awa"},{"post_id":"cjohajwwr000om4v5hlgye09m","tag_id":"cjohajwwr000wm4v56cyyjfis","_id":"cjohajwwr0010m4v53hsc4nir"}],"Tag":[{"name":"线性回归","_id":"cjohajwwr000sm4v52lot0eec"},{"name":"对数几率回归","_id":"cjohajwwr000wm4v56cyyjfis"}]}}