---
title: 'LightGBM: A Highly Efficient Gradient Boosting Decision Tree（NIPS 2017）'
categories:
  - 论文阅读
date: 2018-11-14 21:09:19
tags:
---
<font color='red'>无敌超级详细版</font>
# 先验知识 #
## 决策树 ##
### 基本流程 ###
决策树是一种常见的机器学习算法。其基本思想很是简单，如下图所示
<img src="/images/paper/lgb01.png" width=640/>
我们判断一个西瓜是否是好瓜，根据生活经验，我们首先判断色泽是什么？判断色泽完后再判断根蒂是否蜷缩？然后再判断敲声是否浊响，最后我们根据这些特征的取值一步一步走到最后的叶子节点，判断它是否是好瓜

其决策树的基本算法如下图所示
<img src="/images/paper/lgb02.png" width=640/>

显然决策树的生成是一个递归过程，在决策树算法中有三种情况会导致递归返回
1. 当前节点包含的样本完全属于同一类别，无需划分
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
3. 当前节点包含的样本集合为空，无法划分

对于第2种情况，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别

对于第3种情况，同样把当前节点标记为叶节点，但其类别设定为其父节点所含样本最多的类别
### 划分选择 ###
我们从上图4.2算法的第8行得知，我们需要从属性集中，寻找最优划分属性，那么如何寻找最优划分属性呢？一般而言随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的‘纯度’越来越高，基于这个思想我们可以根据每个属性划分的信息增益、信息增益率、基尼指数等属性的互相比较，来选出最优划分属性，详细属性划分选择可以观看<a href='https://sumenpuyuan.github.io/2018/10/15/%E5%86%B3%E7%AD%96%E6%A0%91/'><font color='red'>这里</font></a>，点击目录 右侧划分选择 即可
### 连续值处理 ###
之前我们的属性值都是离散的，但是现实生活中很多属性值都是连续的，接下来讨论属性是连续值的解决方法，**采用二分法对连续属性进行处理，这正是C4.5决策树算法采用的机制**

假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合
$$ T_a=\{\frac {a^i+a^{i+1}}{2} | 1 \leq i \leq n-1\}$$
我们取\\(a^i,a^{i+1}\\)的中位点\\(\frac{a^i+a^{i+1}}{2}\\)作为候选划分点，然后就可以像离散值一样来考察这些划分点。
### 回归树的生成 ###
<font color='red'>这里要重点看！！！,因为最原始的gbdt是基于回归树来说明的</font>，我们用CART算法来说明一颗回归树的生成过程，这是回归树算法流程
<img src="/images/paper/lgb03.png" width=640/>
直接看算法不太容易看懂，我们来举一个例子进行理解

假设我们有训练数据如下，目标是得到一颗最小二乘回归树
<img src="/images/paper/lgb04.png" />

1. 选择最优划分属性j与划分点s
因为只有一个属性x，因此最优切分变量就是x

接下来考虑9个切分点[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5],损失函数定义为\\(Loss=(y,f(x))=(f(x)-y)^2\\),将9个切分点一一代入下个式子，其中\\(c_m=ave(y_i|x_i \in R_m)\\)(即我们通过切割点将数据集分为两部分，每部分我们的预测值就是这一部分所有样本取值的平均值)
$$ \min_{j,s}[\min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i -c_1)^2+\min_{c_2}\sum_{x_i \in R_2(j,s)}(y_i-c_2)^2] $$
例如我们取s=1.5，R1={1},R2={2,3,4,5,6,7,8,9,10},这两个区域的输出值分别是
c1=5.56，c2=1/9（5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05）=7.50，得到下表
<img src="/images/paper/lgb05.png" />
将以上各个c1 c2带入平方误差损失函数可得下表
<img src="/images/paper/lgb06.png" />
显然s=6.5是，m(s)最小，因此第一个划分变量是j=x，s=6.5

用选定的（j,s）对数据进行划分，并决定输出值。目前我们分的两个区域分别是R1={1,2,3,4,5,6}，R2={7,8,9,10}输出值\\(c_m=ave(y_i|x_i \in R_m)\\)，c1=6.24，c2=8.91，对两个子区域继续重复上述步骤，直到满足停止条件（如限制深度或叶子结点个数）
<font color='red'>可以看出我们每次划分最优属性与最优切割点，都需要遍历所有属性，所有切割点，所有训练样本，这一步骤是很耗时的</font>
## 集成学习 ##
### Adaboost ###
### Boosting Tree###
### GBDT  ###
# 论文开始 #

