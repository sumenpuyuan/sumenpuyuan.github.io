---
title: 'LightGBM: A Highly Efficient Gradient Boosting Decision Tree（NIPS 2017）'
categories:
  - 论文阅读
date: 2018-11-14 21:09:19
tags:
---
# 先验知识 #
## 决策树 ##
### 基本流程 ###
决策树是一种常见的机器学习算法。其基本思想很是简单，如下图所示
<img src="/images/paper/lgb01.png" width=640/>
我们判断一个西瓜是否是好瓜，根据生活经验，我们首先判断色泽是什么？判断色泽完后再判断根蒂是否蜷缩？然后再判断敲声是否浊响，最后我们根据这些特征的取值一步一步走到最后的叶子节点，判断它是否是好瓜

其决策树的基本算法如下图所示
<img src="/images/paper/lgb02.png" width=640/>

显然决策树的生成是一个递归过程，在决策树算法中有三种情况会导致递归返回
1. 当前节点包含的样本完全属于同一类别，无需划分
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
3. 当前节点包含的样本集合为空，无法划分

对于第2种情况，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别

对于第3种情况，同样把当前节点标记为叶节点，但其类别设定为其父节点所含样本最多的类别
### 划分选择 ###
我们从上图4.2算法的第8行得知，我们需要从属性集中，寻找最优划分属性，那么如何寻找最优划分属性呢？一般而言随着划分过程不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的‘纯度’越来越高，基于这个思想我们可以根据每个属性划分的信息增益、信息增益率、基尼指数等属性的互相比较，来选出最优划分属性，详细属性划分选择可以观看<a href='https://sumenpuyuan.github.io/2018/10/15/%E5%86%B3%E7%AD%96%E6%A0%91/'><font color='red'>这里</font></a>，点击目录 右侧划分选择 即可
### 连续值处理 ###
之前我们的属性值都是离散的，但是现实生活中很多属性值都是连续的，接下来讨论属性是连续值的解决方法，**采用二分法对连续属性进行处理，这正是C4.5决策树算法采用的机制**

假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合
$$ T_a=\{\frac {a^i+a^{i+1}}{2} | 1 \leq i \leq n-1\}$$
我们取\\(a^i,a^{i+1}\\)的中位点\\(\frac{a^i+a^{i+1}}{2}\\)作为候选划分点，然后就可以像离散值一样来考察这些划分点。
### 回归树的生成 ###
**这里要重点看！！！,因为最原始的gbdt是基于回归树来说明的**

，我们用CART算法来说明一颗回归树的生成过程，这是回归树算法流程
<img src="/images/paper/lgb03.png" width=640/>
直接看算法不太容易看懂，我们来举一个例子进行理解

假设我们有训练数据如下，目标是得到一颗最小二乘回归树
<img src="/images/paper/lgb04.png" />

1. 选择最优划分属性j与划分点s
因为只有一个属性x，因此最优切分变量就是x

接下来考虑9个切分点[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5],损失函数定义为\\(Loss=(y,f(x))=(f(x)-y)^2\\),将9个切分点一一代入下个式子，其中\\(c_m=ave(y_i|x_i \in R_m)\\)(即我们通过切割点将数据集分为两部分，每部分我们的预测值就是这一部分所有样本取值的平均值)
$$ \min_{j,s}[\min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i -c_1)^2+\min_{c_2}\sum_{x_i \in R_2(j,s)}(y_i-c_2)^2] $$
例如我们取s=1.5，R1={1},R2={2,3,4,5,6,7,8,9,10},这两个区域的输出值分别是
c1=5.56，c2=1/9（5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05）=7.50，得到下表
<img src="/images/paper/lgb05.png" />
将以上各个c1 c2带入平方误差损失函数可得下表
<img src="/images/paper/lgb06.png" />
显然s=6.5是，m(s)最小，因此第一个划分变量是j=x，s=6.5

用选定的（j,s）对数据进行划分，并决定输出值。目前我们分的两个区域分别是R1={1,2,3,4,5,6}，R2={7,8,9,10}输出值\\(c_m=ave(y_i|x_i \in R_m)\\)，c1=6.24，c2=8.91，对两个子区域继续重复上述步骤，直到满足停止条件（如限制深度或叶子结点个数）
**可以看出我们每次划分最优属性与最优切割点，都需要遍历所有属性，所有切割点，所有训练样本，这一步骤是很耗时的**
## 集成学习 ##
<img src="/images/paper/lgb07.png"/>
上图很好的总结了集成学习的基本思想，即通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能，这对‘弱学习器’尤为明显，因此集成学习的很多理论研究都是针对弱学习器的。有些类似常说的三个臭皮匠顶个诸葛亮。


根据个体学习器的生成方式不同，目前集成学习主要分为以下两类算法
1. 个体学习器存在强依赖关系，必须串行生成的序列化方法。代表：Boosting（通俗讲，就是每个个体学习器的生成都要在上一个学习器生成完后进行）
2. 个体学习器之间不存在强依赖关系，可同时生成的并行化方法。代表：Bagging和随机森林

GBDT属于boosting算法，所以这里只介绍booting的相关算法

### Adaboost ###
booting算法系列最著名的算法就是Adaboost算法
这是Adaboost的一个直观介绍

<img src="/images/paper/lgb08.png"/>
<img src="/images/paper/lgb09.png"/>
假设弱分类器有x < v,x>v或y>v,y<v组成，反映在图上就是只能使用水平线或垂直线进行分割，图（1）是我们的最原始数据集，我们找到一条垂直线，发现这条线可以时损失函数最小，得到图（2），发现右边有三个正值被错分为负值，于是增加这些错分样本的权重，形成图（3）的数据，再次找到损失函数最小的直线，如图（4），发现左边有三个负值被错分为正值，增大这几个错分样本的权重，形成图（5），然后再进行划分，最后进行组合，便可得到一个非常好的集成分类器

从上述图片描述，我们可以看出Adaboost主要有两个点要注意
1. 提高那些被前一轮弱分类器错误分类的样本的权值，而降低那些被正确分类的样本的权值
2. 加大分类误差率小的弱分类器的权值，使其在表决中其较大的作用，减小分类误差率大的分类器的权值，使其在表决中其较小的作用

这是Adaboost的基本思想，下面给出Adabosot的一个算法
<img src="/images/paper/lgb10.png"/>
<img src="/images/paper/lgb11.png"/>
我们主要关注算法中两个点
1. 权重的更新公式
根据式子8.4可得出一下公式
$$ w_{m+1,i}=
\begin{cases}
       \frac{ W_{mi}}{Z_m} e^{-\alpha_m},  & G_m(x_i)=y_i \\
        \frac{ W_{mi}}{Z_m} e^{\alpha_m}, & G_m(x_i) \neq y_i
        \end{cases}
$$
正确分类与错误分类两项比较，错误分类样本放大了\\( e^{2𝛼_m }=\frac{e_m}{1-e_m} \\)  倍
体现了我们需要注意的第一个点
2. 个体分类器前面的比例系数

由8.6式子可知，每个个体分类器前面有个比例权重\\(\alpha=\frac{1}{2}log\frac{1-e_m}{e_m} \\),em是分类错误率，可知em越小，\\(\alpha越大\\),体现了我们需要注意的第二点

### Boosting Tree###
我们先看一个直观的图介绍

<img src="/images/paper/lgb17.png"/>

图中我们想要预测男人或者老人的年龄值，我们先用一颗回归树去拟合他们年龄，得到左侧的一颗树，发现男人预测年龄与真实年龄相差+2，老人预测年龄与真实年龄相差-1，接下来我们再去构造一颗回归树去拟合上一步得到的残差，直到满足停止条件，图中最后男孩预测就是2+0.9=2.9

提升树采用加法模型（即个体分类器的线性组合）与前向分布算法，以决策树为基函数的提升方法称为提升树0。算法如下
<img src="/images/paper/lgb12.png"/>
我们举个例子进行说明,依旧是上面那个例子
<img src="/images/paper/lgb13.png"/>
<img src="/images/paper/lgb14.png"/>
<img src="/images/paper/lgb15.png"/>
### GBDT(Gradient decent +Booting Tree)  ###
提升树利用加法模型与前向分布算法实现学习的过程，当损失函数是平方损失函数和指数损失函数时，每一步的优化时很简单的，但对一般的损失函数而言，往往每一步优化并不那么容易，针对这一问题，Freidman提出了梯度提升方法
<img src="/images/paper/lgb16.png"/>
### 为什么用负梯度替代残差 ###
残差=真值-预测值，明明可以直接计算。
为什么要引入麻烦的梯度？有什么用吗？

知乎上一个比较令人信服的解释是：（1）可以扩展到更复杂的损失函数。因为我们一般采用结构风险最小化防止过拟合（在经验风险上加入正则项），所以此时的损失函数不是y^ =y时最小，所以我们需要计算损失函数的梯度，而不能用分模型来拟合残差[1]

链接地址：[1]https://www.zhihu.com/question/63560633

# 论文开始 #
1. 思考一：Gradient Boosting Decision Tree (GBDT)非常流行，当特征维度较高和数据量巨大的时候，其实现中仍然存在效率和可扩展性的问题。一个主要原因就是对于每一个特征的每一个分裂点，都需要遍历全部数据计算信息增益，这一过程非常耗时。
2. 思考二：在真实的应用中，高维度特征具有稀疏性，这样可以设计一个减少有效特征数量的无损的方法，特别是在稀疏特征中，许多特征是互斥的，出现大量0，例如one-hot。
1. 改进一：Gradient-based One-Side Sampling (GOSS) （基于梯度的one-side采样）
2. 改进二：Exclusive Feature Bundling (EFB) （互斥的特征捆绑）

## Gradient-based One-Side Sampling (GOSS) （基于梯度的one-side采样） ##
分析一：为了减小训练数据集，通常做法是下采样。例如过滤掉权重小于阈值的数据。我们首先想到Adaboost中有样本权重这一属性，可以根据权重大小取设置阈值，但是GBDT中并没有样本权重这一属性，作者想到GBDT是有梯度这一属性的，梯度越小，说明预测值与真实值之间相差较小，说明已经训练的很好了，这部分就可以去除掉，但是直接去掉又会改变原始数据集的分布。所以作者提出了一种新的方法叫goss

**GOSS是一种在减少数据量和保证精度上平衡的算法。**

GOSS基本思路

1. 保留梯度大的实例，top-a
2. 对梯度小的实例进行随机采样

### 详细步骤 ###
假设训练集有n个实例\\(𝑥_1, 𝑥_2, 𝑥_3,…… 𝑥_n\\)，特征维度为s。每次梯度迭时，模型数据变量的损失函数的负梯度方向表示为\\(g_1, g_2, g_3…… g_n\\), ，决策树通过最优切分点（最大信息增益点）将数据分到各个节点。GBDT通过分割后的方差衡量信息增益。

<img src="/images/paper/lgb18.png"/>

**Goss的做法**


GOSS保留所有的梯度较大的实例，在梯度小的实例上使用随机采样。为了抵消对数据分布的影响，计算信息增益的时候，GOSS对小梯度的数据引入常量乘数。GOSS首先根据数据的梯度绝对值排序，选取top a个实例。然后在剩余的数据中随机采样b个实例。接着计算信息增益时为采样出的小梯度数据乘以(1-a)/b，这样算法就会更关注训练不足的实例，而不会过多改变原数据集的分布


<img src="/images/paper/lgb19.png"/>


## Exclusive Feature Bundling (EFB) （互斥的特征捆绑） ##
思考1：我们可以通过减少特征量来缩短训练时间。通常用主成分分析或者投影法。当然，这些方法依赖于一个假设-特征包含高度的冗余，但实际中往往不是。（设计特征来自于其独特的贡献，移除任何一维度都可以某种程度上影响精度）。

思考二：高位的数据通常是稀疏的，这种稀疏性启发我们设计一种无损地方法来减少特征的维度。特别的，稀疏特征空间中，许多特征是互斥的，例如他们从不同时为非零值（类似one-hot编码）。我们可以绑定互斥的特征为单一特征,可以显着加快GBDT的训练，而不会影响准确性。从而引出了两个问题：

### Q1：怎么判定那些特征应该绑在一起（build bundled）？ ###

图着色问题


图的m色优化问题:给定无向连通图G,为图G的各顶点着色, 使图中任2邻接点着不同颜色,问最少需要几种颜色。所需的最少颜色的数目m称为该图的色数。


关于图的着色问题，一种解法是，如果给定我们无向图的边，贪心做法就是任选一顶点着色1，在图中尽可能多的用颜色1着色； 
当不能用颜色1着色时，转用颜色2将未着色的顶点尽可能多的着色··· 
直到所有顶点都被着色停止。


另一种解法，我们对于无向图的边还不是很清楚，那么我们可以直接从集合出发（某一个集合可以归结于无向图中的不相邻的点的集合）去构造许多的集合。

理论1 将特征分割为最小数量的互斥特征群是NP难的。
证明：将图着色问题归约为此问题，而图着色是NP难的，所以此问题就是NP难的。

Proof: We will reduce the graph coloring problem[25] toour problem. Since graph coloring problem is NP-hard, we can then deduce our conclusion.



思考一：理论1说明多项式时间中求解这个NP难问题不可行。为了寻找好的近似算法，我们将最优捆绑问题归结为图着色问题，**给定图着色实例G=(V, E)。以G的关联矩阵的每一行为特征，得到我们问题的一个实例有|V|个特征。 很容易看到，在我们的问题中，一个独特的特征包与一组具有相同颜色的顶点相对应，反之亦然。这就是他们归结到图着色问题的一个说明**

Given any instance G = (V,E) of the graph coloring problem. We construct an instance of our problem as follows. Take each row of the incidence matrix of G as a feature, and get an instance of our problem with|V|features. It is easy to see that an exclusive bundle of features in our problem corresponds to a set of vertices with the same color, and vice versa.


如果两个特征之间不是相互排斥，那么我们用一个边将他们连接，然后用合理的贪婪算法用于图着色来做特征捆绑。

思考二：注意到通常有很多特征，尽管不是100％相互排斥的，也很少同时取非零值。 如果我们的算法可以允许一小部分的冲突，我们可以得到更少的特征包，进一步提高计算效率。

根据以上分析，他们提出了一下思路的算法

1. 建立一个图，每个点代表特征，每个边有权重，其权重和特征之间总体冲突相关。
2. 按照降序排列图中的度数来排序特征。
3. 检查排序之后的每个特征，对他进行特征绑定或者建立新的绑定使得操作之后的总体冲突最小。

<img src="/images/paper/lgb20.png"/>




### Q2:怎么进行merging features(特征合并) ###

如何合并同一个bundle的特征来降低训练时间复杂度。关键在于原始特征值可以从bundle中区分出来。鉴于直方图算法存储离散值而不是连续特征值，我们通过将互斥特征放在不同的箱中来构建bundle。这可以通过将偏移量添加到特征原始值中实现，例如，假设bundle中有两个特征，原始特征A取值[0, 10]，B取值[0, 20]。我们添加偏移量10到B中，因此B取值[10, 30]。通过这种做法，就可以安全地将A、B特征合并，使用一个取值[0, 30]的特征取代AB。算法见算法4，**这边没看懂，所以组会也没讲，等xgboost论文看懂后再来补坑**


<img src="/images/paper/lgb21.png"/>

## 实验部分 ##
数据集介绍
<img src="/images/paper/lgb22.png"/>
一次迭代时间对比
<img src="/images/paper/lgb23.png"/>
评判结果对比
<img src="/images/paper/lgb24.png"/>
训练速度对比，可以看到lightgbm速度很快，并且最后精度也很不错
<img src="/images/paper/lgb25.png"/>

## 未来工作 ##
1. 研究基于梯度的one-side采样中a和b的最佳选择
2. 继续提升EFB性能，来处理大量特征，无论特征是否稀疏

## 使用lightgbm的竞赛方案 ##
<img src="/images/paper/lgb26.png"/>

连接地址：https://github.com/Microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions

# 参考资料 #
[1]https://www.zhihu.com/question/63560633

[2]http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf

[3]《 The Elements of Statistical Learning》

[4]《统计学习方法》李航

[5]《机器学习》周志华








