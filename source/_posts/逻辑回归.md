---
title: 逻辑回归
categories:
  - 机器学习
date: 2018-12-12 10:46:34
tags:
---
# 原理与预测函数 #
我们使用sigmod函数作为“广义线性模型”的单调可微函数g（.），得到
$$ y=\frac {1}{1+e^{-(w^T x+b)}} \qquad (1)$$
上式可变换为
$$ ln \frac{y}{1-y}=w^Tx+b \qquad (2) $$
若将y视为样本x作为正例的可能性，则1-y是其反例的可能性。两者的比值\\( \frac{y}{1-y} \\)称为几率，反映了x作为正例的相对可能性，对几率去对数得到对数几率
$$ ln \frac {y}{1-y}    $$ 

由（2）式可知，实际上是用线性回归模型的预测结果去逼近真实标记的对数几率，其模型称为“**对数几率回归**”


我们来看如何求w和b，我们把（1）式的y视为类后验概率估计p(y=1|x)，在（2）式重写为

$$ ln \frac {p(y=1|x)}{p(y=0|x)} =w^Tx+b $$

显然有
$$ p(y=1|x)=\frac{e^{w^T x+b}}{1+e^{w^T x+b}} $$
$$ p(y=0|x)=\frac{1}{1+e^{w^T x+b}} $$
**预测函数**，拿我们讨论的最标准的二分类来说，分别计算p(y=1|x),p(y=0|x)哪个条件概率大就分到哪一类
# 损失函数推导 #

对于给定的数据集\\( {(x\_i,y\_i)}^N\_{i=1},y \in {0,1} \\),

设P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)

即$$ p(y|x)= \begin{cases} 
g(x), & \text {if $y$ is 1} \\
 1-g(x), & \text{if $y$ is 0} 
\end{cases} $$

可以合在一起写为
$$ p(y|x)= [g(x_i)]^{y_i} [1-g(x_i)]^{1-y_i}$$
两边取对数为
$$log\ p(y|x) = y_i log\ g(x_i)+(1-y_i)log\ (1- g(x_i))$$
p(y|x)的值需要最大化，但是我们损失函数需要最小化，所以前面加一个负号，就变成
$$L(w,b)=-\{ y_i log\ g(x_i)+(1-y_i)log\ (1- g(x_i))\}$$
就是交叉熵损失函数，其基本形式为
$$ H(p,q)=-\sum_x p(x)ln(q(x) $$
反映了两个概率分布之间的差异信息,其中p表示真实分布，q表示非真实分布,即反应我们推测的分布和真实分布的差异大小信息。
# 损失函数求解 #

似然函数为
$$ \prod^N_{i=1} [g(x_i)]^y_i [1-g(x_i)]^{1-y_i} $$
对数似然函数为
$$ L(w)=\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_ilog \frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_i (w \ast x_i)-log(1+\exp(w \ast x_i))] $$
对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决

如果采用梯度下降，更新公式为
<img src="/images/ML/004.jpg" width=640/>
# 正则化与模型评估 #
看这里https://sumenpuyuan.github.io/2018/12/05/%E6%94%B9%E5%96%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E7%AC%AC%E4%B8%80%E5%91%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/#zhengze
#类别不平衡问题 #
# sklearn参数 #
官方API：http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html
class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0,fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None,solver='liblinear', max_iter=100, multi_class='ovr', verbose=0,warm_start=False, n_jobs=1)
- penalty : str, ‘l1’or ‘l2’, default: ‘l2’
（1）LogisticRegression默认带了正则化项。penalty参数可选择的值为"l1"和"l2".分别对应L1的正则化和L2的正则化，默认是L2的正则化。
（2）在调参时如果我们主要的目的只是为了解决过拟合，一般penalty选择L2正则化就够了。但是如果选择L2正则化发现还是过拟合，即预测效果差的时候，就可以考虑L1正则化。另外，如果模型的特征非常多，我们希望一些不重要的特征系数归零，从而让模型系数稀疏化的话，也可以使用L1正则化。
（3）penalty参数的选择会影响我们损失函数优化算法的选择。即参数solver的选择，如果是L2正则化，那么4种可选的算法{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}都可以选择。但是如果penalty是L1正则化的话，就只能选择‘liblinear’了。这是因为L1正则化的损失函数不是连续可导的，而{‘newton-cg’, ‘lbfgs’,‘sag’}这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而‘liblinear’并没有这个依赖。
- dual : bool, default: False
（1）对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False。
- C : float, default: 1.0
（1）C为正则化系数λ的倒数，通常默认为1
- fit_intercept : bool, default: True
（1）是否存在截距，默认存在
- solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’}, default: ‘liblinear’优化算法选择参数
a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。
b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。
- multi_class : str, {‘ovr’, ‘multinomial’}, default:‘ovr’分类方式选择参数：
ovr即one-vs-rest(OvR)，而multinomial即many-vs-many(MvM)。如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。
- class_weight : dictor ‘balanced’, default: None（考虑误分类代价敏感、分类类型不平衡的问题）
（1）class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即不考虑权重，或者说所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重，或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。
（2）如果class_weight选择balanced，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低，样本量越少，则权重越高。当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))
参考：https://blog.csdn.net/CherDW/article/details/54891073