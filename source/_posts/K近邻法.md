---
title: K近邻法
categories:
  - 机器学习
date: 2018-11-01 17:04:17
tags:
---
# k近邻法 #
给定一个训练数据集，对新输入的输入实例，在训练数据集中找到与该实例最近的k个实例，这k个实例的多数属于某个类，就把该输入实例归入到这个类
## k值的选择 ##
如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测，‘学习’的近似误差会减小，只有与输入实例较近的训练实例才会对预测起作用。但缺点是‘学习’的估计误差会变大，预测结果对近邻的实例点非常敏感，如果紧邻点正好是噪声，预测就会出错。换句话说，k的减小意味着整体模型变复杂，容易发生过拟合

如果选择较大的k值，学习估计误差变小，学习近似误差增大，这时与输入实例较远的训练实例也会起预测作用。k值变大意味着模型变得简单
# k近邻法的实现：kd树 #
**常规的k-d tree的构建过程为：循环依序取数据点的各维度来作为切分维度，取数据点在该维度的中值作为切分超平面，将中值左侧的数据点挂在其左子树，将中值右侧的数据点挂在其右子树。递归处理其子树，直至所有数据点挂载完毕。**
<img src="/images/kd_example.png" width="640"/>
k-d树上的最邻近查找算法

　　在k-d树中进行数据的查找也是特征匹配的重要环节，其目的是检索在k-d树中与查询点距离最近的数据点。这里先以一个简单的实例来描述最邻近查找的基本思路。

　　星号表示要查询的点（2.1,3.1）。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点（2,3）。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行'回溯'操作：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从（7,2）点开始进行二叉查找，然后到达（5,4），最后到达（2,3），此时搜索路径中的节点为<（7,2），（5,4），（2,3）>，首先以（2,3）作为当前最近邻点，计算其到查询点（2.1,3.1）的距离为0.1414，然后回溯到其父节点（5,4），并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以（2.1,3.1）为圆心，以0.1414为半径画圆，如图4所示。发现该圆并不和超平面y = 4交割，因此不用进入（5,4）节点右子空间中去搜索。
<img src="/images/kd_1.png"/>
回溯到（7,2），以（2.1,3.1）为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入（7,2）右子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点（2,3），最近距离为0.1414。

　　一个复杂点了例子如查找点为（2，4.5）。同样先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径<（7,2），（5,4），（4,7）>，取（4,7）为当前最近邻点，计算其与目标查找点的距离为3.202。然后回溯到（5,4），计算其与查找点之间的距离为3.041。以（2，4.5）为圆心，以3.041为半径作圆，如图5所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找。此时需将（2,3）节点加入搜索路径中得<（7,2），（2,3）>。回溯至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5。回溯至（7,2），以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图6所示。至此，搜索路径回溯完。返回最近邻点（2,3），最近距离1.5。
<img src="/images/kd_2.png"/>

<img src="/images/kd_3.png"/>

<font color='red'>如果实例点是随机分布的，kd树搜索的平均计算复杂度是O（log N），这里N是训练实数，kd树更适用于训练实例数大于空间维数的k近邻搜索，当空间维数接近训练实例时，它的效率会迅速下降，几乎接近线性扫描</font>

**参考博客**：

https://leileiluoluo.com/posts/kdtree-algorithm-and-implementation.html
