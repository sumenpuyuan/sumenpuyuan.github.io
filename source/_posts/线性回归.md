title: 线性模型
categories:
  - 机器学习
tags:
  - 线性回归
  - 对数几率回归
date: 2018-10-11 20:10:07
---
# 机器学习常用概念 #
- 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。是定义在单个样本上的，算的是一个样本的误差。
- 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。
- 目标函数（Object Function）定义为：最终需要优化的函数。等于结构风险Cost Function + 正则化项。
- 正则化项，我们直接最小化经验风险，很容易产生过拟合现象，所以我们一般需要在经验风险上加上正则化项或损失，结构风险定义为
$$ R_{srm}(f)=1/N\sum^N_{i=1}L(y_i,f(x_i))+\lambda J(f)$$
其中J（f）为模型的复杂度。模型越复杂，复杂度越大，相反越简单，复杂度越低。复杂度表示了对复杂模型的惩罚。结构风险最小化，需要经验风险与模型复杂度同时最小。
- 过拟合现象，即训练误差很低，测试误差较大，即常说的高方差，一般因为模型复杂度太高导致。
- 泛华：在机器学习方法中，泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。在实际情况中，我们通常通过测试误差来评价学习方法的泛化能力。

# 线性模型 #
## 基本形式 ##
给定有d个属性描述的示例\\(x=(x_1:x_2……x_d)\\),其中Xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即
$$ f(x)=w_1x_1+w_2x_2+……+w_dx_d+b $$
一般向量形式写成
$$ f(x)=w^Tx+b $$
## 线性回归 ##
###  x只有一种属性 ###
我们先考一种最简单的情况，x只有一种属性，即假设函数为
$$ f(x_i)=wx_i+b $$
选择均方误差作为性能度量，试图使方差最下，即
$$ (w^*,b^*)=arg min(w,b)=\Sigma^m_{i=1}(f(x_i)-y_i)^2 $$
$$ =arg min(w,b)\Sigma^m_{i=1}(y_i-wx_i-b)^2 $$ 
基于均方误差进行模型求解的方法称为最小二乘法，我们将损失函数分别对w和b求导得

<img src="/images/p54.jpg" width=480 align=center/>
###  x有d个属性
此时我们试图学得
$$ f(x_i)=w^Tx_i+b  \qquad make \qquad f(x_i) \approx y_i $$
这称为“多元线性回归”

类似的，可以用最小二乘法对w和b进行估计，我们把w和b吸收入向量形式\\( \hat w=(w;b) \\),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，我们需要求如下式子
$$ \hat w^*=arg min(\hat w)(y-X \hat w)^T(y-X \hat w) $$
上式对w求导，得到
$$ \frac{ \partial E_w}{\partial w}=2X^T(Xw-y)$$
<img src="/images/ML/01.jpg" width=480 align=center/>
 
1. 当\\(X^TX\\)为满秩矩阵或正定矩阵时，上式为0可得
$$ w^*=(X^TX)^{-1}X^Ty $$
2. 如果不是，那么可能解出多个w，此时由学习算法的归纳偏好决定，常见的做法是引入正则化


#损失函数
MSE（mean square error）
$$ MSE(y,\hat y)-\frac{1}{n} \sum^n_{i=1}(y_i- \hat y_i)^2 $$
MAE(mean absolute error)
$$MAE(y,\hat y)=\frac{1}{n}\sum^n_{i=0}|y_i-\hat y_i|$$
skleran 调用
```python
from sklearn.metrics import mean_squared_error #均方误差
from sklearn.metrics import mean_absolute_error #平方绝对误差
from sklearn.metrics import r2_score#R square
#调用
mean_squared_error(y_test,y_predict)
mean_absolute_error(y_test,y_predict)
r2_score(y_test,y_predict)
```
#平方损失函数推导 #
用极大似然的思想推出平方损失函数
# 损失函数求解 #
## 梯度下降法 ##
假设函数为\\(f(x)=\theta x\\),损失函数为\\(J(\theta)\\),要最小化损失函数，则参数更新公式为
$$ \theta_i=\theta_{i-1}-\eta  \frac {\partial J(\theta)}{\partial \theta }$$
不断迭代，直到损失函数值达到一定要求
### 梯度下降相关理论 ###
<img src="/images/ML/03.jpg"/>
当\\(\theta 和 \theta_0 \\)相隔很近时，利用泰勒公式可知
$$ f(\theta)\approx f(\theta_0)+（\theta-\theta_0）*\nabla f(\theta_0) $$
其中\\(\theta - \theta_0\\)是矢量，大小就是梯度下降更新公式的\\(\eta\\),类似于下山过程中的步长，\\(\theta - \theta_0\\)的单位向量，我们用v表示，则\\(\theta - \theta_0\\)可表示为
$$\theta - \theta_0=\eta v  ——(1)$$
,**重点来了，我们希望每次theta更新，都能让f(theta)变小，就是说希望f(theta)<f(theta 0)，则有**
$$ f(\theta)-f(\theta_0)\approx \eta v*\nabla f(\theta_0)<0 $$
因为n是标量，一般取正数，可忽略，所以就是后半部分小于0，而v和梯度都是向量，两个向量相乘最小，即两者方向完全相反，即让v是负梯度方向，即
$$ v=- \frac{\nabla f(\theta_0)}{||\nabla f(\theta_0)||}$$
代入（1）式可得
$$ \theta=\theta_0 - \eta \frac{\nabla f(\theta_0)}{||\nabla f(\theta_0)||}$$
因为n是标量，所以简化为
$$\theta=\theta_0-\eta \nabla f(\theta_0) $$
## 牛顿法 ##
<img src="/images/ML/02.jpg" style='transform: rotate(90deg);'width=480 align=center/>
牛顿法也可用来求极值(用来求损失函数的最小值)，由于函数求极值的导数为0，故可用牛顿法求导函数的零点，迭代公式
$$ x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)} $$
## 拟牛顿法
没看懂，等最优化来补坑

# sklearn中的线性模型
## 基本形式
```python
>>> from sklearn.linear_model import LinearRegression
>>> lin_reg = LinearRegression()
>>> lin_reg.fit(X,y)
>>> lin_reg.intercept_, lin_reg.coef_
(array([4.21509616]),array([2.77011339]))
>>> lin_reg.predict(X_new)
array([[4.21509616],[9.75532293]])
```
## 岭回归（L2正则化）
对线性回归来说，对于岭回归，我们可以使用封闭方程去计算，也可以使用梯度下降去处理。
封闭方程求解
$$ \hat \theta=(X^T•X+\alpha A)^{-1}•X^T•y $$
```python
>>> from sklearn.linear_model import Ridge
>>> ridge_reg = Ridge(alpha=1, solver="cholesky")
>>> ridge_reg.fit(X, y)
>>> ridge_reg.predict([[1.5]])
array([[ 1.55071465]]
```
随机梯度法

```python
>>> sgd_reg = SGDRegressor(penalty="l2")
>>> sgd_reg.fit(X, y.ravel())
>>> sgd_reg.predict([[1.5]])
array([[ 1.13500145]])
```
##Lass回归（L1正则化）
```python
>>> from sklearn.linear_model import Lasso
>>> lasso_reg = Lasso(alpha=0.1)
>>> lasso_reg.fit(X, y)
>>> lasso_reg.predict([[1.5]])
array([ 1.53788174]
```
##弹性网络（ElasNet）
弹性网络介于 Ridge 回归和 Lasso 回归之间。它的正则项是 Ridge 回归和 Lasso 回归正则项的简单混合，同时你可以控制它们的混合率 r，当 r=0 时，弹性网络就是 Ridge 回归，当 r=1 时，其就是 Lasso 回归。
$$ J(\theta)=MSE(\theta)+r\alpha \sum^n_{i=1}|\theta_i|+\frac{1-r}{2}\alpha \sum^n_{i=1}\theta^2_i $$
那么我们该如何选择线性回归，岭回归，Lasso 回归，弹性网络呢？一般来说有一点正则项的表现更好，因此通常你应该避免使用简单的线性回归。岭回归是一个很好的首选项，但是如果你的特征仅有少数是真正有用的，你应该选择 Lasso 和弹性网络。就像我们讨论的那样，它两能够将无用特征的权重降为零。一般来说，弹性网络的表现要比 Lasso 好，因为当特征数量比样本的数量大的时候，或者特征之间有很强的相关性时，Lasso 可能会表现的不规律。
```python
>>> from sklearn.linear_model import ElasticNet
>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
>>> elastic_net.fit(X, y)
>>> elastic_net.predict([[1.5]])
array([ 1.54333232])
```
## 逻辑回归
```python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression()
log_reg.fit(X, y)
```
参考资料
1. https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ
2. https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/docs/4.%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md

