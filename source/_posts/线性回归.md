title: 线性模型
categories:
  - 机器学习
tags:
  - 线性回归
  - 对数几率回归
date: 2018-10-11 20:10:07
---
# 机器学习常用概念 #
- 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。是定义在单个样本上的，算的是一个样本的误差。
- 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。
- 目标函数（Object Function）定义为：最终需要优化的函数。等于结构风险Cost Function + 正则化项。
- 正则化项，我们直接最小化经验风险，很容易产生过拟合现象，所以我们一般需要在经验风险上加上正则化项或损失，结构风险定义为
$$ R_{srm}(f)=1/N\sum^N_{i=1}L(y_i,f(x_i))+\lambda J(f)$$
其中J（f）为模型的复杂度。模型越复杂，复杂度越大，相反越简单，复杂度越低。复杂度表示了对复杂模型的惩罚。结构风险最小化，需要经验风险与模型复杂度同时最小。
- 过拟合现象，即训练误差很低，测试误差较大，即常说的高方差，一般因为模型复杂度太高导致。
- 泛华：在机器学习方法中，泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。在实际情况中，我们通常通过测试误差来评价学习方法的泛化能力。

# 线性模型 #
## 基本形式 ##
给定有d个属性描述的示例\\(x=(x_1:x_2……x_d)\\),其中Xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即
$$ f(x)=w_1x_1+w_2x_2+……+w_dx_d+b $$
一般向量形式写成
$$ f(x)=w^Tx+b $$
## 线性回归 ##
###  x只有一种属性 ###
我们先考一种最简单的情况，x只有一种属性，即假设函数为
$$ f(x_i)=wx_i+b $$
选择均方误差作为性能度量，试图使方差最下，即
$$ (w^*,b^*)=arg min(w,b)=\Sigma^m_{i=1}(f(x_i)-y_i)^2 $$
$$ =arg min(w,b)\Sigma^m_{i=1}(y_i-wx_i-b)^2 $$ 
基于均方误差进行模型求解的方法称为最小二乘法，我们将损失函数分别对w和b求导得

<img src="/images/p54.jpg" width=480 align=center/>
###  x有d个属性
此时我们试图学得
$$ f(x_i)=w^Tx_i+b  \qquad make \qquad f(x_i) \approx y_i $$
这称为“多元线性回归”

类似的，可以用最小二乘法对w和b进行估计，我们把w和b吸收入向量形式\\( \hat w=(w;b) \\),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，我们需要求如下式子
$$ \hat w^*=arg min(\hat w)(y-X \hat w)^T(y-X \hat w) $$
上式对w求导，得到
$$ \frac{ \partial E_w}{\partial w}=2X^T(Xw-y)$$
<img src="/images/ML/01.jpg" width=480 align=center/>
 
1. 当\\(X^TX\\)为满秩矩阵或正定矩阵时，上式为0可得
$$ w^*=(X^TX)^{-1}X^Ty $$
2. 如果不是，那么可能解出多个w，此时由学习算法的归纳偏好决定，常见的做法是引入正则化

# 对数几率回归 #
我们使用sigmod函数作为“广义线性模型”的单调可微函数g（.），得到
$$ y=\frac {1}{1+e^{-(w^T x+b)}} \qquad (1)$$
上式可变换为
$$ ln \frac{y}{1-y}=w^Tx+b \qquad (2) $$
若将y视为样本x作为正例的可能性，则1-y是其反例的可能性。两者的比值\\( \frac{y}{1-y} \\)称为几率，反映了x作为正例的相对可能性，对几率去对数得到对数几率
$$ ln \frac {y}{1-y}    $$ 

由（2）式可知，实际上是用线性回归模型的预测结果去逼近真是标记的对数几率，其模型称为“**对数几率回归**”

我们来看如何求w和b，我们把（1）式的y视为类后验概率估计p(y=1|x)，在（2）式重写为

$$ ln \frac {p(y=1|x)}{p(y=0|x)} =w^Tx+b $$

显然有
$$ p(y=1|x)=\frac{e^{w^T x+b}}{1+e^{w^T x+b}} $$
$$ p(y=0|x)=\frac{1}{1+e^{w^T x+b}} $$
对于给定的数据集\\( {(x\_i,y\_i)}^N\_{i=1},y \in {0,1} \\),可以应用极大似然估计估计模型参数，从而得到逻辑回归模型
设P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)
似然函数为
$$ \prod^N_{i=1} [g(x_i)]^y_i [1-g(x_i)]^{1-y_i} $$
对数似然函数为
$$ L(w)=\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_ilog \frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))] $$
$$ =\Sigma^N_{i=1}[y_i (w*x_i)-log(1+\exp(w \ast x_i))] $$
对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决

# 损失函数求解 #
## 梯度下降法 ##
假设函数为\\(f(x)=\theta x\\),损失函数为\\(J(\theta)\\),要最小化损失函数，则参数更新公式为
$$ \theta_i=\theta_{i-1}-\eta  \frac {\partial J(\theta)}{\partial \theta }$$
不断迭代，直到损失函数值达到一定要求
#### 梯度下降相关理论
当\\(\theta 和 \theta_0 \\)相隔很近时，利用泰勒公式可知
$$ f(\theta)\approx f(\theta_0)*\nabla f(\theta_0) $$
其中\\(\theta - \theta_0\\)是矢量，大小就是梯度下降更新公式的\\(\eta\\),类似于下山过程中的步长，\\(\theta - \theta_0\\)的单位向量，我们用v表示，则\\(\theta - \theta_0\\)可表示为
$$\theta - \theta_0=\eta v  ——(1)$$
,**重点来了，我们希望每次theta更新，都能让f(theta)变小，就是说希望f(theta)<f(theta 0)，则有**
$$ f(\theta)-f(\theta_0)\approx \eta v*\nabla f(\theta_0)<0 $$
因为n是标量，一般取正数，可忽略，所以就是后半部分小于0，而v和梯度都是向量，两个向量相乘最小，即两者方向完全相反，即让v是负梯度方向，即
$$ v=- \frac{\nabla f(\theta_0)}{||\nabla f(\theta_0)||}$$
代入（1）式可得
$$ \theta=\theta_0 - \eta \frac{\nabla f(\theta_0)}{||\nabla f(\theta_0)||}$$
因为n是标量，所以简化为
$$\theta=\theta_0-\eta \nabla f(\theta_0) $$
## 牛顿法
<img src="/images/ML/02.jpg" style='transform: rotate(90deg);'width=480 align=center/>
牛顿法也可用来求极值(用来求损失函数的最小值)，由于函数求极值的导数为0，故可用牛顿法求导函数的零点，迭代公式
$$ x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)} $$





