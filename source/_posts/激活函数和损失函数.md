---
title: 激活函数和损失函数
categories:
  - 深度学习
date: 2018-12-02 21:52:05
tags:
---
# 激活函数 #
关于激活函数，首先要搞清楚的问题是，激活函数是什么，有什么用？不用激活函数可不可以？答案是不可以。激活函数的主要作用是提供网络的非线性建模能力。如果没有激活函数，那么该网络仅能够表达线性映射，此时即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。因此也可以认为，只有加入了激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。 那么激活函数应该具有什么样的性质呢？
+ 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。 
+ 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。 
+ 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著;当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate

从目前来看，常见的激活函数多是分段线性和具有指数形状的非线性函数

## sigmoid 函数##

$$ f(x)=\frac{1}{1+e^{-x}} $$

 ![img](images/DL/001.jpg)

sigmoid 是使用范围最广的一类激活函数，具有指数函数形状，它在物理意义上最为接近生物神经元。此外，(0, 1) 的输出还可以被表示作概率，或用于输入的归一化，代表性的如Sigmoid交叉熵损失函数。
缺点：容易产生梯度消失于爆炸问题
从下图sigmoid的导数函数可知，其最大值是0.25。反向链式求导网络过深时会出现问题。
 ![img](images/paper/alex7.png)
## ReLU函数 ##
