---
title: 决策树
date: 2018-10-15 10:59:23
categories:
  - 机器学习
tags:
---
#  基本流程 #

<img src="http://pgmz9e1an.bkt.clouddn.com/2018-10-17_221227.png" width="640"/>
三种情况导致递归返回
1. 当前节点包含的样本完全属于同一类别，无需划分
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
3. 当前节点包含样本集合为空，不能划分

第二种情况，把当前节点标记为叶节点，将其类别设定为该节点所含样本最多的类别

第三种情况，当前设为叶子节点，类别设为其父节点样本最多的类别
# 划分选择 #
难点在第八行，即选择最优划分属性，一般而言我们希望随着划分的进行，分支节点所包含的样本尽可能属于同一类，即节点纯度越来越高
## 信息增益 ##
**信息熵**
假定当前样本集合D中第k类样本所占比例是Pk（k=1,2……y）,则信息熵定义为
$$ Ent(D)=-\Sigma^y_{k=1} p_k log_2p_k $$
Ent（D）越小，D的纯度越高 

**信息增益**

假定离散属性a有V个可能的取值\\({a^1,a^2……a^V}\\)使用属性a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为\\(a^v\\)的样本，记为\\(D^v\\)

根据信息熵公式算出信息熵，再考虑到不同分支节点包含的样本数不同，给分支节点赋予权重\\(|D^v|/|D|\\)，即样本数越多的分支节点的影响越大，于是得到**信息增益公式**
$$ Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V \frac {|D^v|}{|D|}Ent(D^v) \qquad (1)$$

一般而言，信息增益对俄大，则意味着使用属性a来进行划分所获得“纯度提升”越大，所以使用信息增益来进行最优划分属性选择。**著名的ID3决策树算法就是以信息增益为准则来选择划分属性**
## 增益率 ##
<img src="http://pgmz9e1an.bkt.clouddn.com/2018-10-18_152827.png" width="640"/>
假设我们使用“编号”这一属性来作为一个候选划分，算出信息增益为0.998远大于其他候选属性划分，这很容易理解，“编号”属性产生17个分支，每个分支仅包含一个样本，纯度已达最高，然而这样的决策树不具有泛化能力，无法对新样本进行有效预测

**实际上，信息增益准则对取值数目较多的属性有所偏好**,为减少这种不利影响，**著名C4.5决策树算法使用增益率**选择最优划分属性，公式定义为
$$ Gain_ratio(D,a)=\frac {Gain(D,a)}{IV(a)} $$
其中
$$ IV(a)=-\sum^V_{v=1} \frac {|D^v|}{|D|} log_2 \frac{|D^v|}{|D|} $$
称为属性a的固有值，属性a的可能取值数目越多（即V越大）,则IV（a）的值通常会越大

需要注意的是，**增益率准则对于可取值数目较少的属性有所偏好**,所以C4.5算法并不是选择增益率最高的属性作为划分，而是采用一个启发式方法**先从候选划分找出信息增益高于平均水平的属性，然后再选择增益率最高的**
## 基尼指数 ##
CART决策树使用基尼指数来选择划分属性，数据集D的纯度用基尼值来度量，
$$ Gini(D)=\sum^{|y|}_{k=1}\sum_{k \neq k}p_kp_{k'} $$
$$ 1-\sum^{|y|}_{k=1}p^2_k $$
直观来说，基尼反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，**因此基尼越小，则数据集D的纯度越高**,按照（1）式格式，得a属性的基尼指数为
$$ Gini_index(D,a)=\sum^V_{v=1}\frac {|D^v|}{|D|}Gini(D^v) $$
# 剪枝处理 #
剪枝是决策树学习算法对付过拟合的主要手段
1. 预剪枝，在决策树生成过程中，对每个节点在划分前进行估计，若当前节点的划分不能导致决策树泛化能力提升，停止划分，并将当前节点标记为叶节点
2. 后剪枝则是先从训练街生成一颗完整的决策树，然后自底向上对非叶子节点进行考察，若将该节点对应的字数替换为叶节点导致泛华能力提升，则子树替换为叶节点

两者对比
1. 后剪枝通常比与预剪枝决策树保留更多的分支，所以后剪枝不容易欠拟合，泛化能力往往优于预剪枝决策树
2. 后剪枝是在生成完全决策树之后进行的，并且要自底向上的对书上的所有非叶子节点进行一一考察，所以开销要比预剪枝决策树大

# 连续与缺失值 #
## 连续值处理 ##
属性是连续值，**采用二分法对连续属性进行处理，这正是C4.5决策苏算法采用的机制**

假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合
$$ T_a=\{\frac {a^i+a^{i+1}}{2} | 1 \leq i \leq n-1\}$$

**与离散值不同的是，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性 **
## 缺失值处理 ##
需要解决两个问题
1. 如何在属性值缺失的情况下进行最优划分属性选择
2. 给定划分属性，若样本在该属性值上的值缺失，如何多样本进行划分？

**先来解决第一个问题**

给定训练集D和属性a，令\\(D^~\\)表示D中属性a没有缺失值的样本子集，对于问题1，显然我们仅可根据\\(D^~\\)判断属性a的优劣。假设属性a有V个可取值\\(a^1,a^2……a^V\\)

令\\(D^{~v}\\)表示\\(D^~\\)在属性a上取值为\\(a^v\\)的样本子集

\\(D^~_k\\)表示\\(D^~\\)中属于第k类（k=1,2……，|y|）的样本子集

假定我们为每一个样本x赋予一个权重\\(w_x\\)，并定义
$$ \rho = \frac {\Sigma_{x \in D^~ }w_x}{\Sigma_{x \in D}w_x} $$
$$ p^~_k=\frac {\Sigma_{x \in D^~_k}w_x}{\Sigma_{x \in D^~}w_x} \qquad (1 \leq k \leq |y|)$$
$$ $$