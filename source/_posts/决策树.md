---
title: 决策树
date: 2018-10-15 10:59:23
categories:
  - 机器学习
tags:
---
# 信息论基础 #
- 信息熵：是一个随机变量不确定性的度量，对于一个离散型随机变量 X~ p(x) ，其离散熵可以定义为：
$$ H(x)=-\sum_{x\in \chi} p(x)log(p(x)) $$
- 联合熵：分布为 p(x,y) 的一对随机变量  (X,Y) ,其联合熵定义为：
$$ H(X,Y)-=\sum_{x\in \chi}\sum_{y\in y} p(x,y)logp(x,y)=H(X)+H(Y|X) $$
表示输入为Ｘ，输出为Ｙ时，整个系统所具有的不确定程度
- 条件熵，表示表示已知编码输入集为X时，编码输出集为Y所剩余的不确定性。 
$$ H(Y|X)=-\sum_{x\in \chi}\sum_{y\in y}p(x,y)logp(y|x) $$
#  基本流程 #

<img src="/images/ML/010.jpg" width="640"/>
三种情况导致递归返回
1. 当前节点包含的样本完全属于同一类别，无需划分
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分
3. 当前节点包含样本集合为空，不能划分

第二种情况，把当前节点标记为叶节点，将其类别设定为该节点所含样本最多的类别

第三种情况，当前设为叶子节点，类别设为其父节点样本最多的类别
# 划分选择 #
难点在第八行，即选择最优划分属性，一般而言我们希望随着划分的进行，分支节点所包含的样本尽可能属于同一类，即节点纯度越来越高
## 信息增益 ##
**信息熵**
假定当前样本集合D中第k类样本所占比例是Pk（k=1,2……y）,则信息熵定义为
$$ Ent(D)=-\Sigma^y_{k=1} p_k log_2p_k $$
Ent（D）越小，D的纯度越高 

**信息增益**

假定离散属性a有V个可能的取值\\({a^1,a^2……a^V}\\)使用属性a对样本集D进行划分，则会产生V个分支节点，其中第v个分支节点包含了D中所有在属性a上取值为\\(a^v\\)的样本，记为\\(D^v\\)

根据信息熵公式算出信息熵，再考虑到不同分支节点包含的样本数不同，给分支节点赋予权重\\(|D^v|/|D|\\)，即样本数越多的分支节点的影响越大，于是得到**信息增益公式**
$$ Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V \frac {|D^v|}{|D|}Ent(D^v) \qquad (1)$$

一般而言，信息增益对俄大，则意味着使用属性a来进行划分所获得“纯度提升”越大，所以使用信息增益来进行最优划分属性选择。**著名的ID3决策树算法就是以信息增益为准则来选择划分属性**
## 增益率 ##
<img src="http://pgmz9e1an.bkt.clouddn.com/2018-10-18_152827.png" width="640"/>
假设我们使用“编号”这一属性来作为一个候选划分，算出信息增益为0.998远大于其他候选属性划分，这很容易理解，“编号”属性产生17个分支，每个分支仅包含一个样本，纯度已达最高，然而这样的决策树不具有泛化能力，无法对新样本进行有效预测

**实际上，信息增益准则对取值数目较多的属性有所偏好**,为减少这种不利影响，**著名C4.5决策树算法使用增益率**选择最优划分属性，公式定义为
$$ Gain\_ratio(D,a)=\frac {Gain(D,a)}{IV(a)} $$
其中
$$ IV(a)=-\sum^V_{v=1} \frac {|D^v|}{|D|} log_2 \frac{|D^v|}{|D|} $$
称为属性a的固有值，属性a的可能取值数目越多（即V越大）,则IV（a）的值通常会越大

需要注意的是，**增益率准则对于可取值数目较少的属性有所偏好**,所以C4.5算法并不是选择增益率最高的属性作为划分，而是采用一个启发式方法**先从候选划分找出信息增益高于平均水平的属性，然后再选择增益率最高的**
## 基尼指数 ##
CART决策树使用基尼指数来选择划分属性，数据集D的纯度用基尼值来度量，
$$ Gini(D)=\sum^{|y|}_{k=1}\sum_{k' \neq k}p_kp_{k'} $$
$$ =1-\sum^{|y|}_{k=1}p^2_k $$
直观来说，基尼反映了从数据集中随机抽取两个样本，其类别标记不一致的概率，**因此基尼越小，则数据集D的纯度越高**,按照（1）式格式，得a属性的基尼指数为
$$ Gini\_index(D,a)=\sum^V_{v=1}\frac {|D^v|}{|D|}Gini(D^v) $$


#ID3算法 #
## 流程 ##
<img src="/images/ML/006.jpg" width=640/>
<img src="/images/ML/007.jpg" width=640/>

ID3算法采用信息增益作为最优属性划分选择，另外有个阈值，如果当前最优划分属性信息增益小于该阈值，则直接返回。
**缺点**：ID3算法只有树的生成，所以容易过拟合。另外还有一个缺点，就是上面提到的实际上，信息增益准则对取值数目较多的属性有所偏好
# C4.5算法 #
基本流程与ID3算法一致，不过采用信息增益率作为划分最优属性标准。统计上书上仅仅只是说以信息增益率划分标准。西瓜书上说**增益率准则对于可取值数目较少的属性有所偏好**,所以C4.5算法并不是选择增益率最高的属性作为划分，而是采用一个启发式方法**先从候选划分找出信息增益高于平均水平的属性，然后再选择增益率最高的**
# CART分类与回归树  #
（classification and regression tree CART）分类与回归树，主要两步组成
1. 决策树生成：基于训练数据集生成决策树，生成决策树要尽量大
2. 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，用损失函数最小作为剪枝标准

**回归树使用平方误差最小化准则，分类用基尼指数最小化准则**
## 回归树生成流程 ##
看这里
https://sumenpuyuan.github.io/2018/11/14/LightGBM-A-Highly-Efficient-Gradient-Boosting-Decision-Tree%EF%BC%88NIPS-2017%EF%BC%89/#huigui
## 分类树 ##
**基尼指数**（感觉这里，统计比西瓜讲得要好）
定义：分类问题中，假设有K个类，样本点属于地k类的概率为pk，则概率分布的基尼指数定义为
$$ Gini(p)=\sum^K_{k=1}p_k(1-p_k)=1-\sum^K_{k=1}p_k^2 $$
对于二分类问题，假设样本点属于第一个类的概率是p，则概率分布的基尼指数为
$$ Gini(p)=2p(1-p) $$
对于给定的样本集合，其基尼指数为
$$ Gini(D)=1-\sum^K_{k=1}(\frac{|C_k|}{|D|})^2 $$
这里Ck是D中属于第k类的样本子集，K是类别个数，这个式子与上面基尼指数是一样的意思
**重要！！**
入股样本集合D根据特征A是否取某一可能值a被分割成D1和D2两部分，则在特征A的条件下，集合D的基尼指数定义为
$$ Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2) $$
**基本流程**
<img src="/images/ML/008.jpg" width=640/>
<img src="/images/ML/009.jpg" width=640/>
**与之前讨论树的不一样地方在于，之前都是划分属性，现在是根据属性中的某一个取值与否作为划分，即只能二叉分**
# 剪枝处理 #
剪枝是决策树学习算法对付过拟合的主要手段
1. 预剪枝，在决策树生成过程中，对每个节点在划分前进行估计，若当前节点的划分不能导致决策树泛化能力提升，停止划分，并将当前节点标记为叶节点
2. 后剪枝则是先从训练街生成一颗完整的决策树，然后自底向上对非叶子节点进行考察，若将该节点对应的字数替换为叶节点导致泛华能力提升，则子树替换为叶节点

两者对比
1. 后剪枝通常比与预剪枝决策树保留更多的分支，所以后剪枝不容易欠拟合，泛化能力往往优于预剪枝决策树
2. 后剪枝是在生成完全决策树之后进行的，并且要自底向上的对书上的所有非叶子节点进行一一考察，所以开销要比预剪枝决策树大

# 连续与缺失值 #
## 连续值处理 ##
属性是连续值，**采用二分法对连续属性进行处理，这正是C4.5决策苏算法采用的机制**

假定样本集D和连续属性a，假定a在D上出现了n个不同的取值，<font color='red'>将这些值从小到大排序</font>,记为\\({a^1,a^2……，a^n}\\)，基于划分点t将D分为两部分，一部分大于t一部分小于等于t。对于连续值属性，对相邻的属性取值ai与ai+1l来说，t在区间[ai,ai+1）的划分结果相同，所以产生n-1个划分点集合
$$ T_a=\{\frac {a^i+a^{i+1}}{2} | 1 \leq i \leq n-1\}$$

**与离散值不同的是，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性 **
## 缺失值处理 ##
需要解决两个问题
1. 如何在属性值缺失的情况下进行最优划分属性选择
2. 给定划分属性，若样本在该属性值上的值缺失，如何多样本进行划分？

**先来解决第一个问题**

给定训练集D和属性a，令\\(D^-\\)表示D中属性a没有缺失值的样本子集，对于问题1，显然我们仅可根据\\(D^-\\)判断属性a的优劣。假设属性a有V个可取值\\(a^1,a^2……a^V\\)

令\\(D^{-v}\\)表示\\(D^-\\)在属性a上取值为\\(a^v\\)的样本子集

\\(D^-_k\\)表示\\(D^-\\)中属于第k类（k=1,2……，|y|）的样本子集

假定我们为每一个样本x赋予一个权重\\(w_x\\)，并定义
$$ \rho = \frac {\Sigma_{x \in D^\- }w_x}{\Sigma_{x \in D}w_x} $$
$$ p^-_k=\frac {\Sigma_{x \in D^-_k}w_x}{\Sigma_{x \in D^-}w_x} \qquad (1 \leq k \leq |y|)$$
$$ r^-_v=\frac {\Sigma_{x \in D^-_k}w_x}{\Sigma_{x \in D^-}w_x} \qquad (1 \leq k \leq V)$$
基于上述定义，我们将信息增益的计算式推广为，
$$ Gain(D,a)=\rho *Gain(D^-,a) $$
$$ =\rho *(Ent(D^-)-\sum^V_{v=1}r^-_vEnt(D^-v))$$
$$ Ent(D^-)=-\sum^{|y|}_{k=1}p^-_klog_2p^-_k $$
**解决第二个问题**
划分属性是a，样本x在属性a的取值未知，则将x同时划入所有子节点，且样本权值在属性值a v对应的子节点调整为\\(r^-_v*w_x\\)，直观的看，就是让同一个样本以不同的概率划入到不同子节点中去

#sklearn参数 #
criterion:string类型，可选（默认为"gini"）
   衡量分类的质量。支持的标准有"gini"代表的是Gini impurity(不纯度)与"entropy"代表的是information gain（信息增益）。

splitter:string类型，可选（默认为"best"）
   一种用来在节点中选择分类的策略。支持的策略有"best"，选择最好的分类，"random"选择最好的随机分类。

max_features:int,float,string or None 可选（默认为None）
   在进行分类时需要考虑的特征数。

max_depth:int or None,可选（默认为"None"）
   表示树的最大深度。如果是"None",则节点会一直扩展直到所有的叶子都是纯的或者所有的叶子节点都包含少于min_samples_split个样本点。忽视max_leaf_nodes是不是为None。

min_samples_split:int,float,可选（默认为2）
   区分一个内部节点需要的最少的样本数。    
    1.如果是int，将其最为最小的样本数。
    2.如果是float，min_samples_split是一个百分率并且ceil(min_samples_split*n_samples)是每个分类需要的样本数。ceil是取大于或等于指定表达式的最小整数。

min_samples_leaf:int,float,可选（默认为1）
   一个叶节点所需要的最小样本数：
    1.如果是int，则其为最小样本数
    2.如果是float，则它是一个百分率并且ceil(min_samples_leaf*n_samples)是每个节点所需的样本数。

min_weight_fraction_leaf:float,可选（默认为0）
   一个叶节点的输入样本所需要的最小的加权分数。

max_leaf_nodes:int,None 可选（默认为None）
   在最优方法中使用max_leaf_nodes构建一个树。最好的节点是在杂质相对减少。如果是None则对叶节点的数目没有限制。如果不是None则不考虑max_depth.

class_weight:dict,list of dicts,"Banlanced" or None,可选（默认为None）
   表示在表{class_label:weight}中的类的关联权值。如果没有指定，所有类的权值都为1。对于多输出问题，一列字典的顺序可以与一列y的次序相同。
   "balanced"模型使用y的值去自动适应权值，并且是以输入数据中类的频率的反比例。如：n_samples/(n_classes*np.bincount(y))。
   对于多输出，每列y的权值都会想乘。
   如果sample_weight已经指定了，这些权值将于samples以合适的方法相乘。

random_state:int,RandomState instance or None
   如果是int,random_state 是随机数字发生器的种子；如果是RandomState，random_state是随机数字发生器，如果是None，随机数字发生器是np.random使用的RandomState instance.

persort:bool,可选（默认为False）
   是否预分类数据以加速训练时最好分类的查找。在有大数据集的决策树中，如果设为true可能会减慢训练的过程。当使用一个小数据集或者一个深度受限的决策树中，可以减速训练的过程。