<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="线性回归,对数几率回归," />










<meta name="description" content="机器学习常用概念 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。是定义在单个样本上的，算的是一个样本的误差。 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。 目标函数（Object Function">
<meta name="keywords" content="线性回归,对数几率回归">
<meta property="og:type" content="article">
<meta property="og:title" content="线性模型">
<meta property="og:url" content="http://yoursite.com/2018/10/11/线性回归/index.html">
<meta property="og:site_name" content="苏门蒲沅">
<meta property="og:description" content="机器学习常用概念 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。是定义在单个样本上的，算的是一个样本的误差。 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。 目标函数（Object Function">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/p54.jpg">
<meta property="og:image" content="http://yoursite.com/images/ML/01.jpg">
<meta property="og:image" content="http://yoursite.com/images/ML/03.jpg">
<meta property="og:image" content="http://yoursite.com/images/ML/02.jpg">
<meta property="og:updated_time" content="2018-12-09T03:10:04.218Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="线性模型">
<meta name="twitter:description" content="机器学习常用概念 损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。是定义在单个样本上的，算的是一个样本的误差。 代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。 目标函数（Object Function">
<meta name="twitter:image" content="http://yoursite.com/images/p54.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/10/11/线性回归/"/>





  <title>线性模型 | 苏门蒲沅</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">苏门蒲沅</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/10/11/线性回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="苏门蒲沅">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">线性模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-10-11T20:10:07+08:00">
                2018-10-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="机器学习常用概念"><a href="#机器学习常用概念" class="headerlink" title="机器学习常用概念"></a>机器学习常用概念</h1><ul>
<li>损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。是定义在单个样本上的，算的是一个样本的误差。</li>
<li>代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。</li>
<li>目标函数（Object Function）定义为：最终需要优化的函数。等于结构风险Cost Function + 正则化项。</li>
<li>正则化项，我们直接最小化经验风险，很容易产生过拟合现象，所以我们一般需要在经验风险上加上正则化项或损失，结构风险定义为<script type="math/tex; mode=display">R_{srm}(f)=1/N\sum^N_{i=1}L(y_i,f(x_i))+\lambda J(f)</script>其中J（f）为模型的复杂度。模型越复杂，复杂度越大，相反越简单，复杂度越低。复杂度表示了对复杂模型的惩罚。结构风险最小化，需要经验风险与模型复杂度同时最小。</li>
<li>过拟合现象，即训练误差很低，测试误差较大，即常说的高方差，一般因为模型复杂度太高导致。</li>
<li>泛华：在机器学习方法中，泛化能力通俗来讲就是指学习到的模型对未知数据的预测能力。在实际情况中，我们通常通过测试误差来评价学习方法的泛化能力。</li>
</ul>
<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><p>给定有d个属性描述的示例\(x=(x_1:x_2……x_d)\),其中Xi是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即</p>
<script type="math/tex; mode=display">f(x)=w_1x_1+w_2x_2+……+w_dx_d+b</script><p>一般向量形式写成</p>
<script type="math/tex; mode=display">f(x)=w^Tx+b</script><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="x只有一种属性"><a href="#x只有一种属性" class="headerlink" title="x只有一种属性"></a>x只有一种属性</h3><p>我们先考一种最简单的情况，x只有一种属性，即假设函数为</p>
<script type="math/tex; mode=display">f(x_i)=wx_i+b</script><p>选择均方误差作为性能度量，试图使方差最下，即</p>
<script type="math/tex; mode=display">(w^*,b^*)=arg min(w,b)=\Sigma^m_{i=1}(f(x_i)-y_i)^2</script><script type="math/tex; mode=display">=arg min(w,b)\Sigma^m_{i=1}(y_i-wx_i-b)^2</script><p>基于均方误差进行模型求解的方法称为最小二乘法，我们将损失函数分别对w和b求导得</p>
<p><img src="/images/p54.jpg" width="480" align="center/"></p>
<h3 id="x有d个属性"><a href="#x有d个属性" class="headerlink" title="x有d个属性"></a>x有d个属性</h3><p>此时我们试图学得</p>
<script type="math/tex; mode=display">f(x_i)=w^Tx_i+b  \qquad make \qquad f(x_i) \approx y_i</script><p>这称为“多元线性回归”</p>
<p>类似的，可以用最小二乘法对w和b进行估计，我们把w和b吸收入向量形式\( \hat w=(w;b) \),相应的把数据集D表示为一个m*(d+1)大小的矩阵X，我们需要求如下式子</p>
<script type="math/tex; mode=display">\hat w^*=arg min(\hat w)(y-X \hat w)^T(y-X \hat w)</script><p>上式对w求导，得到</p>
<script type="math/tex; mode=display">\frac{ \partial E_w}{\partial w}=2X^T(Xw-y)</script><p><img src="/images/ML/01.jpg" width="480" align="center/"></p>
<ol>
<li>当\(X^TX\)为满秩矩阵或正定矩阵时，上式为0可得<script type="math/tex; mode=display">w^*=(X^TX)^{-1}X^Ty</script></li>
<li>如果不是，那么可能解出多个w，此时由学习算法的归纳偏好决定，常见的做法是引入正则化</li>
</ol>
<h1 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h1><p>我们使用sigmod函数作为“广义线性模型”的单调可微函数g（.），得到</p>
<script type="math/tex; mode=display">y=\frac {1}{1+e^{-(w^T x+b)}} \qquad (1)</script><p>上式可变换为</p>
<script type="math/tex; mode=display">ln \frac{y}{1-y}=w^Tx+b \qquad (2)</script><p>若将y视为样本x作为正例的可能性，则1-y是其反例的可能性。两者的比值\( \frac{y}{1-y} \)称为几率，反映了x作为正例的相对可能性，对几率去对数得到对数几率</p>
<script type="math/tex; mode=display">ln \frac {y}{1-y}</script><p>由（2）式可知，实际上是用线性回归模型的预测结果去逼近真是标记的对数几率，其模型称为“<strong>对数几率回归</strong>”</p>
<p>我们来看如何求w和b，我们把（1）式的y视为类后验概率估计p(y=1|x)，在（2）式重写为</p>
<script type="math/tex; mode=display">ln \frac {p(y=1|x)}{p(y=0|x)} =w^Tx+b</script><p>显然有</p>
<script type="math/tex; mode=display">p(y=1|x)=\frac{e^{w^T x+b}}{1+e^{w^T x+b}}</script><script type="math/tex; mode=display">p(y=0|x)=\frac{1}{1+e^{w^T x+b}}</script><p>对于给定的数据集\( {(x_i,y_i)}^N_{i=1},y \in {0,1} \),可以应用极大似然估计估计模型参数，从而得到逻辑回归模型<br>设P(Y=1|x)=g(x),P(Y=0|x)=1-g(x)<br>似然函数为</p>
<script type="math/tex; mode=display">\prod^N_{i=1} [g(x_i)]^y_i [1-g(x_i)]^{1-y_i}</script><p>对数似然函数为</p>
<script type="math/tex; mode=display">L(w)=\Sigma ^N_{i=1} [y_ilog(x_i)+(1-y_i)log(1-g(x_i))]</script><script type="math/tex; mode=display">=\Sigma^N_{i=1}[y_ilog \frac {g(x_i)}{1-g(x_i)} + log(1-g(x_i))]</script><script type="math/tex; mode=display">=\Sigma^N_{i=1}[y_i (w \ast x_i)-log(1+\exp(w \ast x_i))]</script><p>对L(w)求最大值，得到w的估计值,常用梯度下降和牛顿法解决</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>MSE（mean square error）</p>
<script type="math/tex; mode=display">MSE(y,\hat y)-\frac{1}{n} \sum^n_{i=1}(y_i- \hat y_i)^2</script><p>MAE(mean absolute error)</p>
<script type="math/tex; mode=display">MAE(y,\hat y)=\frac{1}{n}\sum^n_{i=0}|y_i-\hat y_i|</script><p>skleran 调用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error <span class="comment">#均方误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error <span class="comment">#平方绝对误差</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score<span class="comment">#R square</span></span><br><span class="line"><span class="comment">#调用</span></span><br><span class="line">mean_squared_error(y_test,y_predict)</span><br><span class="line">mean_absolute_error(y_test,y_predict)</span><br><span class="line">r2_score(y_test,y_predict)</span><br></pre></td></tr></table></figure></p>
<h1 id="损失函数求解"><a href="#损失函数求解" class="headerlink" title="损失函数求解"></a>损失函数求解</h1><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>假设函数为\(f(x)=\theta x\),损失函数为\(J(\theta)\),要最小化损失函数，则参数更新公式为</p>
<script type="math/tex; mode=display">\theta_i=\theta_{i-1}-\eta  \frac {\partial J(\theta)}{\partial \theta }</script><p>不断迭代，直到损失函数值达到一定要求</p>
<h3 id="梯度下降相关理论"><a href="#梯度下降相关理论" class="headerlink" title="梯度下降相关理论"></a>梯度下降相关理论</h3><p><img src="/images/ML/03.jpg"><br>当\(\theta 和 \theta_0 \)相隔很近时，利用泰勒公式可知</p>
<script type="math/tex; mode=display">f(\theta)\approx f(\theta_0)+（\theta-\theta_0）*\nabla f(\theta_0)</script><p>其中\(\theta - \theta_0\)是矢量，大小就是梯度下降更新公式的\(\eta\),类似于下山过程中的步长，\(\theta - \theta_0\)的单位向量，我们用v表示，则\(\theta - \theta_0\)可表示为</p>
<script type="math/tex; mode=display">\theta - \theta_0=\eta v  ——(1)</script><p>,<strong>重点来了，我们希望每次theta更新，都能让f(theta)变小，就是说希望f(theta)&lt;f(theta 0)，则有</strong></p>
<script type="math/tex; mode=display">f(\theta)-f(\theta_0)\approx \eta v*\nabla f(\theta_0)<0</script><p>因为n是标量，一般取正数，可忽略，所以就是后半部分小于0，而v和梯度都是向量，两个向量相乘最小，即两者方向完全相反，即让v是负梯度方向，即</p>
<script type="math/tex; mode=display">v=- \frac{\nabla f(\theta_0)}{||\nabla f(\theta_0)||}</script><p>代入（1）式可得</p>
<script type="math/tex; mode=display">\theta=\theta_0 - \eta \frac{\nabla f(\theta_0)}{||\nabla f(\theta_0)||}</script><p>因为n是标量，所以简化为</p>
<script type="math/tex; mode=display">\theta=\theta_0-\eta \nabla f(\theta_0)</script><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p><img src="/images/ML/02.jpg" style="transform: rotate(90deg);" width="480" align="center/"><br>牛顿法也可用来求极值(用来求损失函数的最小值)，由于函数求极值的导数为0，故可用牛顿法求导函数的零点，迭代公式</p>
<script type="math/tex; mode=display">x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)}</script><h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>没看懂，等最优化来补坑</p>
<h1 id="sklearn中的线性模型"><a href="#sklearn中的线性模型" class="headerlink" title="sklearn中的线性模型"></a>sklearn中的线性模型</h1><h2 id="基本形式-1"><a href="#基本形式-1" class="headerlink" title="基本形式"></a>基本形式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_reg = LinearRegression()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_reg.fit(X,y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_reg.intercept_, lin_reg.coef_</span><br><span class="line">(array([<span class="number">4.21509616</span>]),array([<span class="number">2.77011339</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lin_reg.predict(X_new)</span><br><span class="line">array([[<span class="number">4.21509616</span>],[<span class="number">9.75532293</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="岭回归（L2正则化）"><a href="#岭回归（L2正则化）" class="headerlink" title="岭回归（L2正则化）"></a>岭回归（L2正则化）</h2><p>对线性回归来说，对于岭回归，我们可以使用封闭方程去计算，也可以使用梯度下降去处理。<br>封闭方程求解</p>
<script type="math/tex; mode=display">\hat \theta=(X^T•X+\alpha A)^{-1}•X^T•y</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ridge_reg = Ridge(alpha=<span class="number">1</span>, solver=<span class="string">"cholesky"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ridge_reg.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ridge_reg.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([[ <span class="number">1.55071465</span>]]</span><br></pre></td></tr></table></figure>
<p>随机梯度法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_reg = SGDRegressor(penalty=<span class="string">"l2"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_reg.fit(X, y.ravel())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sgd_reg.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([[ <span class="number">1.13500145</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="Lass回归（L1正则化）"><a href="#Lass回归（L1正则化）" class="headerlink" title="Lass回归（L1正则化）"></a>Lass回归（L1正则化）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lasso_reg = Lasso(alpha=<span class="number">0.1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lasso_reg.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lasso_reg.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([ <span class="number">1.53788174</span>]</span><br></pre></td></tr></table></figure>
<h2 id="弹性网络（ElasNet）"><a href="#弹性网络（ElasNet）" class="headerlink" title="弹性网络（ElasNet）"></a>弹性网络（ElasNet）</h2><p>弹性网络介于 Ridge 回归和 Lasso 回归之间。它的正则项是 Ridge 回归和 Lasso 回归正则项的简单混合，同时你可以控制它们的混合率 r，当 r=0 时，弹性网络就是 Ridge 回归，当 r=1 时，其就是 Lasso 回归。</p>
<script type="math/tex; mode=display">J(\theta)=MSE(\theta)+r\alpha \sum^n_{i=1}|\theta_i|+\frac{1-r}{2}\alpha \sum^n_{i=1}\theta^2_i</script><p>那么我们该如何选择线性回归，岭回归，Lasso 回归，弹性网络呢？一般来说有一点正则项的表现更好，因此通常你应该避免使用简单的线性回归。岭回归是一个很好的首选项，但是如果你的特征仅有少数是真正有用的，你应该选择 Lasso 和弹性网络。就像我们讨论的那样，它两能够将无用特征的权重降为零。一般来说，弹性网络的表现要比 Lasso 好，因为当特征数量比样本的数量大的时候，或者特征之间有很强的相关性时，Lasso 可能会表现的不规律。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>elastic_net = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>elastic_net.fit(X, y)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>elastic_net.predict([[<span class="number">1.5</span>]])</span><br><span class="line">array([ <span class="number">1.54333232</span>])</span><br></pre></td></tr></table></figure></p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<p>参考资料</p>
<ol>
<li><a href="https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/k26Fm0GL3fdVA9VbQIVAuQ</a></li>
<li><a href="https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/docs/4.%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md" target="_blank" rel="noopener">https://github.com/apachecn/hands_on_Ml_with_Sklearn_and_TF/blob/dev/docs/4.%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B.md</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/线性回归/" rel="tag"># 线性回归</a>
          
            <a href="/tags/对数几率回归/" rel="tag"># 对数几率回归</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/15/决策树/" rel="prev" title="决策树">
                决策树 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习常用概念"><span class="nav-number">1.</span> <span class="nav-text">机器学习常用概念</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性模型"><span class="nav-number">2.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本形式"><span class="nav-number">2.1.</span> <span class="nav-text">基本形式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归"><span class="nav-number">2.2.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#x只有一种属性"><span class="nav-number">2.2.1.</span> <span class="nav-text">x只有一种属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#x有d个属性"><span class="nav-number">2.2.2.</span> <span class="nav-text">x有d个属性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对数几率回归"><span class="nav-number">3.</span> <span class="nav-text">对数几率回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#损失函数"><span class="nav-number">4.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#损失函数求解"><span class="nav-number">5.</span> <span class="nav-text">损失函数求解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降法"><span class="nav-number">5.1.</span> <span class="nav-text">梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降相关理论"><span class="nav-number">5.1.1.</span> <span class="nav-text">梯度下降相关理论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#牛顿法"><span class="nav-number">5.2.</span> <span class="nav-text">牛顿法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#拟牛顿法"><span class="nav-number">5.3.</span> <span class="nav-text">拟牛顿法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sklearn中的线性模型"><span class="nav-number">6.</span> <span class="nav-text">sklearn中的线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本形式-1"><span class="nav-number">6.1.</span> <span class="nav-text">基本形式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#岭回归（L2正则化）"><span class="nav-number">6.2.</span> <span class="nav-text">岭回归（L2正则化）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lass回归（L1正则化）"><span class="nav-number">6.3.</span> <span class="nav-text">Lass回归（L1正则化）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#弹性网络（ElasNet）"><span class="nav-number">6.4.</span> <span class="nav-text">弹性网络（ElasNet）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑回归"><span class="nav-number">6.5.</span> <span class="nav-text">逻辑回归</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>







        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
